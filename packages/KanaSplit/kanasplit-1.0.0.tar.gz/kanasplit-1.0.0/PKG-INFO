Metadata-Version: 2.2
Name: KanaSplit
Version: 1.0.0
Summary: A Japanese text tokenizer with POS tagging and Jisho.org integration.
Home-page: https://github.com/byteMe394/KanaSplit
Author: JosÃ© Trujillo
Author-email: joseantonio_tf@outlook.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Natural Language :: Japanese
Classifier: Topic :: Text Processing :: Linguistic
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: ratelimit
Requires-Dist: MeCab
Requires-Dist: requests
Requires-Dist: PyQt5
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# KanaSplit - Japanese Text Tokenizer

![KanaSplit Logo](assets/dancing_shigure.gif)  
*A powerful and efficient Japanese text tokenizer with POS tagging and Jisho.org integration.*

## ğŸ“Œ Overview
KanaSplit is a Japanese text tokenizer designed to break down Japanese sentences into meaningful tokens while providing part-of-speech (POS) tagging. It integrates with [Jisho.org](https://jisho.org) to fetch additional lexical data for individual words. The tool is built using **MeCab**, a popular morphological analyzer for the Japanese language.

## ğŸš€ Features
- ğŸ”¹ **Tokenization**: Splits Japanese sentences into words and morphemes.
- ğŸ”¹ **POS Tagging**: Provides grammatical category for each token.
- ğŸ”¹ **Furigana Support**: Extracts readings for kanji words.
- ğŸ”¹ **Jisho.org API Integration**: Retrieves word meanings and definitions.
- ğŸ”¹ **Command-Line Interface (CLI)**: Allows easy text tokenization from the terminal.
- ğŸ”¹ **Error Logging**: Captures and logs API failures for debugging.

## ğŸ”§ Installation
KanaSplit requires **Python 3.6+** and **MeCab** to be installed.

### **1ï¸âƒ£ Install Dependencies**
```sh
pip install -r requirements.txt
```

### **2ï¸âƒ£ Install MeCab (if not installed)**
#### **For Windows**:
```sh
choco install mecab mecab-ipadic
```
#### **For macOS**:
```sh
brew install mecab mecab-ipadic
```
#### **For Linux (Ubuntu/Debian)**:
```sh
sudo apt install mecab mecab-ipadic
```

### **3ï¸âƒ£ Install KanaSplit**
```sh
pip install .
```

## ğŸ“– Usage
### **1ï¸âƒ£ Tokenize Japanese Text**
```sh
kanasplit-cli "ç§ã¯ãŠå¯¿å¸ã‚’é£Ÿã¹ãŸã„ã§ã™ã€‚"
```
**Output:**
```
- ç§ (åè©)
- ã¯ (åŠ©è©)
- ãŠ (æ¥é ­è©)
- å¯¿å¸ (åè©)
- ã‚’ (åŠ©è©)
- é£Ÿã¹ (å‹•è©)
- ãŸã„ (åŠ©å‹•è©)
- ã§ã™ (åŠ©å‹•è©)
- ã€‚ (è¨˜å·)
```

### **2ï¸âƒ£ Fetch Word Definitions from Jisho.org**
```sh
kanasplit-cli -w "å¯¿å¸"
```
**Output:**
```
Word: å¯¿å¸
Reading: ã™ã—
Meanings: ['sushi', 'range of dishes made with vinegared rice combined with fish, vegetables, egg, etc.']
```

## ğŸ›  API Usage
You can also use KanaSplit in your Python scripts:
```python
from tokenizer import tokenize_text_with_pos, fetch_word_from_jisho

text = "ãƒã‚µãƒŸã‚’è²·ã„ãŸã„ã‚“ã§ã™ãŒã€æ–‡æˆ¿å…·å£²ã‚Šå ´ã¯ä½•å›ã§ã™ã‹ï¼Ÿ"
tokens = tokenize_text_with_pos(text)
print(tokens)

word_data = fetch_word_from_jisho("å¯¿å¸")
print(word_data)
```

## ğŸ“ Configuration
KanaSplit logs errors in `errors.log`. You can configure logging settings in `tokenizer.py`.

## ğŸ—ï¸ Development & Contribution
Want to contribute? Feel free to fork this repository and submit a pull request!
```sh
git clone https://github.com/byteMe394/KanaSplit.git
cd KanaSplit
```

## ğŸ“œ License
This project is licensed under the MIT License.

## ğŸ‘¨â€ğŸ’» Author
**JosÃ© Trujillo**  
[GitHub](https://github.com/byteMe394) | [Email](mailto:joseantonio_tf@outlook.com)
