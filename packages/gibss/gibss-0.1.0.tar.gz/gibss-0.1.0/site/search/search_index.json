{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Generalized iterative Bayesian stepwises selection This is the documentation for the gibss python package, which implements generalized iterative Bayesian stepwise selection (GIBSS). GIBSS provides a recipe for building a sparse additive model when given a large number of predictors. GIBSS was motivated by the iterative Bayesian stepwise selection (IBSS) procedure used to fit the sum of single effects regression (SuSiE). GIBSS corresponds to IBSS when the base model is a univariate regression with a Gaussian prior on the effect. In this setting IBSS is coordinate ascent of a variational objective in a particular variational family. Although GIBSS was inspired by IBSS, GIBSS does not (to our knowledge) correspond to optimization of a particular objective. The algorithm is heuristic.","title":"Home"},{"location":"#generalized-iterative-bayesian-stepwises-selection","text":"This is the documentation for the gibss python package, which implements generalized iterative Bayesian stepwise selection (GIBSS). GIBSS provides a recipe for building a sparse additive model when given a large number of predictors. GIBSS was motivated by the iterative Bayesian stepwise selection (IBSS) procedure used to fit the sum of single effects regression (SuSiE). GIBSS corresponds to IBSS when the base model is a univariate regression with a Gaussian prior on the effect. In this setting IBSS is coordinate ascent of a variational objective in a particular variational family. Although GIBSS was inspired by IBSS, GIBSS does not (to our knowledge) correspond to optimization of a particular objective. The algorithm is heuristic.","title":"Generalized iterative Bayesian stepwises selection"},{"location":"additive/","text":"Additive models At it's core GIBSS is fitting an additive model. Each component of the additive model produces a prediction \\(\\psi_l\\) which contributes to the total predictions \\(\\psi = \\sum \\psi_l\\) . We can estimate each component of the additive model in an iterative fashion, estimating one \\(\\psi_l\\) while holding the other \\(\\psi_j\\) \\(j \\neq l\\) fixed. When we fit SuSiE via GIBSS, each component corresponds to an SER, and \\(\\psi_l\\) are the posterior means of the linear predictor \\(\\mathbb E [X b]\\) . However, there is no requirement that the additive model consist of homogenous components. For example, we might include an additive component for the intercept and any other covariates that we want to include in the model. Additive model interface fit_additive(fitfuns: List[Callable], tol: float=1e-3, maxiter: int=10, keep_intermediate=False) -> (List[Any], AdditiveState) A minimal version of this function could be implemented as fit_additive pseudocode def fit_additive_core ( fitfuns : List [ Callable ]) -> ( List [ AdditiveComponent ], AdditiveState ): psi = 0. components = [] for fun in enumerate ( fitfuns , i ): component = fun ( psi , None ) psi = psi + component . psi components . append ( component ) while { not converged } for fun in enumerate ( fitfuns , i ): psi = psi - components [ i ] . psi components [ i ] = fun ( psi , components [ i ]) psi = psi + components [ i ] . psi return components , state fitfuns is a list of functions with signature fitfun(psi: Array, old_fit: Union[AdditiveComponent, None]) -> AdditiveComponent . All of the work goes into designing the additive components. fitfun knows what data it should use, how to initialize itself either with an old fit or from scratch. fit_additive is meant just to handle the basic logic of iteratively fitting an additive model. There are a few features we add. - When the argument keep_intermediate = True the function will save all the intermediate states of components after each run of the loop. - We also add arguments for controlling how convergence is monitored. Using fit_additive to fit new SuSiE models The functions that we can pass to fit_additive need to have a specific type signature def fun ( psi : Array , fit : Union [ Fit , None ]) -> Fit Where Fit type represents the output of the fit function. Importantly, it must be able to handle the case where fit = None , as is the case in the first iteration. The function must know what data, parameters, etc should be used. To facilitate the development of new additive models, we provide a helper function for building functions that are compatible with additive_model def make_fitfun ( X : np . ndarray , y : np . ndarray , fun : Callable , initfun : Callable , kwargs : dict ) -> Callable : \"\"\" Produces a function for fitting an additive component using the provided data and settings. The returned function has a signature compatible with the additive_model interface. Args: X (np.ndarray): A p x n matrix where p is the number of variables and n is the number of observations. y (np.ndarray): An n-dimensional vector of observations corresponding to the rows of X. fun (Callable): A function that takes `coef_init`, `X`, `y`, `psi`, and possibly other arguments specified in `kwargs`. initfun (Callable): A function with the signature `(X, y, psi, fit) -> Array` used to initialize `coef_init`. It must be able to handle `fit` as `None`. kwargs (dict): A dictionary of additional keyword arguments to pass to `fun`. Returns: Callable: A function `fitfun(psi: Array, fit: Union[None, Fit]) -> Fit`, used to fit the SER model. \"\"\" @jax . jit def fitfun ( psi , fit ): coef_init = initfun ( X , y , psi , fit ) return fun ( coef_init = coef_init , X = X , y = y , offset = psi , ** kwargs ) return fitfun make_fitfun assumes that the function fun has positional arguments coef_init , X , y``, and psi . Other arguments can be specified in the dictionary kwargs which will be passed as keyword arguments to fun`. For example we could construct a function for fitting the logistic SER with from gibss.logistic import logistic_ser_hermite initfun = lambda X , y , psi , fit : jnp . zeros (( X . shape [ 0 ], 1 )) fitfun = make_fitfun ( X , y , logistic_ser_hermite , initfun ) fitfun ( 0. , None ) And fit an additive model (logistic SuSiE) with fitfuns = [ fitfun for _ in range ( 5 )] fit , state = additive_model ( fitfuns ) This framework makes it simple to implement new variations of SuSiE. Want to do SuSiE with a new likelihood? Implement the SER for that new likelihood. We have implementations for logistic regression and the Cox proportional hazards model. Want to estimate the prior variance? Implement a version of the SER that does that. Want to include fixed effects in the model? Implement a separate additive component that handles estimation of the fixed effect. This is how we handle the intercept in gibss.logistic.fit_logistic_susie .","title":"Additive"},{"location":"additive/#additive-models","text":"At it's core GIBSS is fitting an additive model. Each component of the additive model produces a prediction \\(\\psi_l\\) which contributes to the total predictions \\(\\psi = \\sum \\psi_l\\) . We can estimate each component of the additive model in an iterative fashion, estimating one \\(\\psi_l\\) while holding the other \\(\\psi_j\\) \\(j \\neq l\\) fixed. When we fit SuSiE via GIBSS, each component corresponds to an SER, and \\(\\psi_l\\) are the posterior means of the linear predictor \\(\\mathbb E [X b]\\) . However, there is no requirement that the additive model consist of homogenous components. For example, we might include an additive component for the intercept and any other covariates that we want to include in the model.","title":"Additive models"},{"location":"additive/#additive-model-interface","text":"fit_additive(fitfuns: List[Callable], tol: float=1e-3, maxiter: int=10, keep_intermediate=False) -> (List[Any], AdditiveState) A minimal version of this function could be implemented as fit_additive pseudocode def fit_additive_core ( fitfuns : List [ Callable ]) -> ( List [ AdditiveComponent ], AdditiveState ): psi = 0. components = [] for fun in enumerate ( fitfuns , i ): component = fun ( psi , None ) psi = psi + component . psi components . append ( component ) while { not converged } for fun in enumerate ( fitfuns , i ): psi = psi - components [ i ] . psi components [ i ] = fun ( psi , components [ i ]) psi = psi + components [ i ] . psi return components , state fitfuns is a list of functions with signature fitfun(psi: Array, old_fit: Union[AdditiveComponent, None]) -> AdditiveComponent . All of the work goes into designing the additive components. fitfun knows what data it should use, how to initialize itself either with an old fit or from scratch. fit_additive is meant just to handle the basic logic of iteratively fitting an additive model. There are a few features we add. - When the argument keep_intermediate = True the function will save all the intermediate states of components after each run of the loop. - We also add arguments for controlling how convergence is monitored.","title":"Additive model interface"},{"location":"additive/#using-fit_additive-to-fit-new-susie-models","text":"The functions that we can pass to fit_additive need to have a specific type signature def fun ( psi : Array , fit : Union [ Fit , None ]) -> Fit Where Fit type represents the output of the fit function. Importantly, it must be able to handle the case where fit = None , as is the case in the first iteration. The function must know what data, parameters, etc should be used. To facilitate the development of new additive models, we provide a helper function for building functions that are compatible with additive_model def make_fitfun ( X : np . ndarray , y : np . ndarray , fun : Callable , initfun : Callable , kwargs : dict ) -> Callable : \"\"\" Produces a function for fitting an additive component using the provided data and settings. The returned function has a signature compatible with the additive_model interface. Args: X (np.ndarray): A p x n matrix where p is the number of variables and n is the number of observations. y (np.ndarray): An n-dimensional vector of observations corresponding to the rows of X. fun (Callable): A function that takes `coef_init`, `X`, `y`, `psi`, and possibly other arguments specified in `kwargs`. initfun (Callable): A function with the signature `(X, y, psi, fit) -> Array` used to initialize `coef_init`. It must be able to handle `fit` as `None`. kwargs (dict): A dictionary of additional keyword arguments to pass to `fun`. Returns: Callable: A function `fitfun(psi: Array, fit: Union[None, Fit]) -> Fit`, used to fit the SER model. \"\"\" @jax . jit def fitfun ( psi , fit ): coef_init = initfun ( X , y , psi , fit ) return fun ( coef_init = coef_init , X = X , y = y , offset = psi , ** kwargs ) return fitfun make_fitfun assumes that the function fun has positional arguments coef_init , X , y``, and psi . Other arguments can be specified in the dictionary kwargs which will be passed as keyword arguments to fun`. For example we could construct a function for fitting the logistic SER with from gibss.logistic import logistic_ser_hermite initfun = lambda X , y , psi , fit : jnp . zeros (( X . shape [ 0 ], 1 )) fitfun = make_fitfun ( X , y , logistic_ser_hermite , initfun ) fitfun ( 0. , None ) And fit an additive model (logistic SuSiE) with fitfuns = [ fitfun for _ in range ( 5 )] fit , state = additive_model ( fitfuns ) This framework makes it simple to implement new variations of SuSiE. Want to do SuSiE with a new likelihood? Implement the SER for that new likelihood. We have implementations for logistic regression and the Cox proportional hazards model. Want to estimate the prior variance? Implement a version of the SER that does that. Want to include fixed effects in the model? Implement a separate additive component that handles estimation of the fixed effect. This is how we handle the intercept in gibss.logistic.fit_logistic_susie .","title":"Using fit_additive to fit new SuSiE models"},{"location":"logistic_susie/","text":"Logistic SuSiE We have implemented two versions of logistic SuSiE. For the moment we recommend using the version in gibss.logistic module. The most important design decision made in this module is to handle the intercept and other fixed covariates in their own component of the additive model. This dramatically simplifies implementation of the SER, where each sub-problem corresponds to fitting a simple univariate logistic regression. It is much easier to develop a fast, stable optimization scheme for this 1d problem. Typically, we would also want to estimate an intercept, or the effect of other covariates that we would want to include in the model (e.g. genotype PCs for GWAS or eQTL studies). The univariate approach has the advantage that we can flexibly specify how to handle these effects independently from the SER. For example, we could change the regularization/priors on the fixed effect without needing to implement a new SER. Fitting a single effect regression (SER) model The single effect regression is a simple model where exactly one of \\(p\\) variables has a non-zero effect. Compared to more general variable selection problems inference in the SER is tractable because we can enumerate all possible configurations of non-zero effects. Inference in the SER can be carried out as follows. For each variable \\(j = 1 \\dots p\\) Compute the MAP estimate for the effect \\(b_j\\) , \\(\\hat b_j\\) . Approximate the Bayes Factor \\(\\text{BF}_j\\) , \\(\\widehat{\\text{BF}}_j\\) Compute the (approximate) posterior inclusion probabilities \\(\\alpha_j \\propto \\widehat{\\text {BF}}_j\\) For each observation \\(i = 1, \\dots, n\\) , compute the (approximate) posterior mean predictions \\(\\psi_i = \\sum \\alpha_j \\hat b_j x_{ij}\\) . The logistic SER When you call logistic.gibss.fit_logistic_susie with method='hermite' , you fit a logistic SER where the posterior mean and Bayes factor are approximated using adaptive Gauss-Hermite quadrature. When the quadrature rule uses \\(m=1\\) points, this corresponds to the usual Laplace approximation. Estimating the prior variance We offer the option to estimate the prior variance. A simple way to estimate the prior variance is to evaluate the SER along a fixed grid of prior variance settings, and then selecting the SER with the highest marginal likelihood. We think that this approach is sufficient, compared to more precise maximization of the prior variance. It is implemented in gibss.logistic.logistic_ser_hermite_grid , which takes an argument prior_variance_grid . Rationale for default optimization hyperparameters We use the following defaults in gibss.logistic. when method = 'hermite' defaultserkwargs = dict ( prior_variance = float ( 10. ), newtonkwargs = dict ( tol = 1e-2 , maxiter = 5 , alpha = 0.2 , gamma =- 0.1 ) ) Prior variance We take set the pior_variance = 10. . The appropriate choice of prior variance is of course dependent on the scale of the covariates. If we assume that the user are providing covariates where a unit increase in the covariate is non-negligble (e.g. increased dosage of the derived allele, inclusion in a gene set), then a prior variance of \\(10\\) corresponds to pretty weak regularization of the effect. I think setting the default prior variance to a rather large value is safe in the sense that we will get smaller Bayes factors which will cause us to be conservative when evaluating evidence for the presence of a non-zero effect in the SER. Number of iteration We estimate the MAP using Newton's method with backtracking line search. We use just \\(5\\) iterations of Newton's method for each SER, but use the estimates from this iteration to initialize the next iteration. That is, we initialize component \\(l\\) at iteration \\(t+1\\) with the estimates from component \\(l\\) at iteration \\(t\\) . Across several iterations of the outer loop, as psi stabilizes, the optimization problem in the inner loop remains unchanged. Heuristically, we save computational effort by not optimizing very precisely the intermediate objectives that are liable to change drastically iteration to iteration, and by leveraging the previous approximate optima when the problems are similar. Settings for backtracking line search For convex problems, Newton exhibits fast (quadratic) convergence within a neighborhood of the optima with stepsize \\(1\\) .Away from the optimum, the Newton update is guarunteed to be a descent direction but the step size may need tuning. We start with a stepsize of one and decay geometrically until the objective improves (or at least, does not increase too much). Since we are allowing just \\(5\\) evaluations of the objective, and we would like to ensure that our effect gets updated at each stage, we set the step size scaling factor to \\(0.2\\) , which gives a minimum stepsize of \\(0.2^5\\) . In practice this minimum step size is small enough that we will improve the objective at each iteration. Sufficient decrease parameter . The sufficient decrease parameter gamma says to accept a move if \\(f(x_{t+1}) < f(x_t) + \\gamma ||g||_2^2\\) Where \\(g\\) is the gradient. We actually allow slight decrease in the objective function by setting \\(\\gamma = -0.1\\) . The optimization is implemented in JAX, and we find that with 32bit floats, and optimization by the compiler it is not uncommon to dramatic decrease in the sub-optimality of a solution while actually seeing very slight increases in the objective. Therefore to avoid the optimization procedure getting stuck we allow for slight increases in the objective. It would be good to better understand the cause of these numerical issues. Adjusting the optimization hyperparameters The choice of optimization hyper-parameters can have a dramatic effect on performance. Newton's method with backtracking line search is guaranteed to converge, but because we are repeatedly computing MAP estimates for many sub-problems, it pays to be mindful of the balance between accuracy and computation.","title":"Logistic SuSiE"},{"location":"logistic_susie/#logistic-susie","text":"We have implemented two versions of logistic SuSiE. For the moment we recommend using the version in gibss.logistic module. The most important design decision made in this module is to handle the intercept and other fixed covariates in their own component of the additive model. This dramatically simplifies implementation of the SER, where each sub-problem corresponds to fitting a simple univariate logistic regression. It is much easier to develop a fast, stable optimization scheme for this 1d problem. Typically, we would also want to estimate an intercept, or the effect of other covariates that we would want to include in the model (e.g. genotype PCs for GWAS or eQTL studies). The univariate approach has the advantage that we can flexibly specify how to handle these effects independently from the SER. For example, we could change the regularization/priors on the fixed effect without needing to implement a new SER.","title":"Logistic SuSiE"},{"location":"logistic_susie/#fitting-a-single-effect-regression-ser-model","text":"The single effect regression is a simple model where exactly one of \\(p\\) variables has a non-zero effect. Compared to more general variable selection problems inference in the SER is tractable because we can enumerate all possible configurations of non-zero effects. Inference in the SER can be carried out as follows. For each variable \\(j = 1 \\dots p\\) Compute the MAP estimate for the effect \\(b_j\\) , \\(\\hat b_j\\) . Approximate the Bayes Factor \\(\\text{BF}_j\\) , \\(\\widehat{\\text{BF}}_j\\) Compute the (approximate) posterior inclusion probabilities \\(\\alpha_j \\propto \\widehat{\\text {BF}}_j\\) For each observation \\(i = 1, \\dots, n\\) , compute the (approximate) posterior mean predictions \\(\\psi_i = \\sum \\alpha_j \\hat b_j x_{ij}\\) .","title":"Fitting a single effect regression (SER) model"},{"location":"logistic_susie/#the-logistic-ser","text":"When you call logistic.gibss.fit_logistic_susie with method='hermite' , you fit a logistic SER where the posterior mean and Bayes factor are approximated using adaptive Gauss-Hermite quadrature. When the quadrature rule uses \\(m=1\\) points, this corresponds to the usual Laplace approximation.","title":"The logistic SER"},{"location":"logistic_susie/#estimating-the-prior-variance","text":"We offer the option to estimate the prior variance. A simple way to estimate the prior variance is to evaluate the SER along a fixed grid of prior variance settings, and then selecting the SER with the highest marginal likelihood. We think that this approach is sufficient, compared to more precise maximization of the prior variance. It is implemented in gibss.logistic.logistic_ser_hermite_grid , which takes an argument prior_variance_grid .","title":"Estimating the prior variance"},{"location":"logistic_susie/#rationale-for-default-optimization-hyperparameters","text":"We use the following defaults in gibss.logistic. when method = 'hermite' defaultserkwargs = dict ( prior_variance = float ( 10. ), newtonkwargs = dict ( tol = 1e-2 , maxiter = 5 , alpha = 0.2 , gamma =- 0.1 ) ) Prior variance We take set the pior_variance = 10. . The appropriate choice of prior variance is of course dependent on the scale of the covariates. If we assume that the user are providing covariates where a unit increase in the covariate is non-negligble (e.g. increased dosage of the derived allele, inclusion in a gene set), then a prior variance of \\(10\\) corresponds to pretty weak regularization of the effect. I think setting the default prior variance to a rather large value is safe in the sense that we will get smaller Bayes factors which will cause us to be conservative when evaluating evidence for the presence of a non-zero effect in the SER. Number of iteration We estimate the MAP using Newton's method with backtracking line search. We use just \\(5\\) iterations of Newton's method for each SER, but use the estimates from this iteration to initialize the next iteration. That is, we initialize component \\(l\\) at iteration \\(t+1\\) with the estimates from component \\(l\\) at iteration \\(t\\) . Across several iterations of the outer loop, as psi stabilizes, the optimization problem in the inner loop remains unchanged. Heuristically, we save computational effort by not optimizing very precisely the intermediate objectives that are liable to change drastically iteration to iteration, and by leveraging the previous approximate optima when the problems are similar. Settings for backtracking line search For convex problems, Newton exhibits fast (quadratic) convergence within a neighborhood of the optima with stepsize \\(1\\) .Away from the optimum, the Newton update is guarunteed to be a descent direction but the step size may need tuning. We start with a stepsize of one and decay geometrically until the objective improves (or at least, does not increase too much). Since we are allowing just \\(5\\) evaluations of the objective, and we would like to ensure that our effect gets updated at each stage, we set the step size scaling factor to \\(0.2\\) , which gives a minimum stepsize of \\(0.2^5\\) . In practice this minimum step size is small enough that we will improve the objective at each iteration. Sufficient decrease parameter . The sufficient decrease parameter gamma says to accept a move if \\(f(x_{t+1}) < f(x_t) + \\gamma ||g||_2^2\\) Where \\(g\\) is the gradient. We actually allow slight decrease in the objective function by setting \\(\\gamma = -0.1\\) . The optimization is implemented in JAX, and we find that with 32bit floats, and optimization by the compiler it is not uncommon to dramatic decrease in the sub-optimality of a solution while actually seeing very slight increases in the objective. Therefore to avoid the optimization procedure getting stuck we allow for slight increases in the objective. It would be good to better understand the cause of these numerical issues.","title":"Rationale for default optimization hyperparameters"},{"location":"logistic_susie/#adjusting-the-optimization-hyperparameters","text":"The choice of optimization hyper-parameters can have a dramatic effect on performance. Newton's method with backtracking line search is guaranteed to converge, but because we are repeatedly computing MAP estimates for many sub-problems, it pays to be mindful of the balance between accuracy and computation.","title":"Adjusting the optimization hyperparameters"},{"location":"newton/","text":"Newton's method in JAX Motivation In order to apply GIBSS you need to provide a univariate regression function that returns, at a minimum, a Bayes factor and a point estimate of the predictions for each observation. We use GIBSS to do Bayesian variable selection in logistic regression or Cox regression. In both these examples (and GLMs more generally) the univariate regression problems are convex. We make the process of implementing new models in GIBSS simple. We implement a Newton's method with backgracking line search in JAX. By providing a an implementation of the objective function in JAX, and utilizing JAXs automatic differentiation and vectorization features, GIBSS automates the implementation of of the single effect regression by first computing the MAP estimate for each variable, and then approximating the Bayes factor via Gauss-Hermite quadrature. The Newton optimizer. We use JAX to implement a simple, light weight newton optimizer. At a high level, the optimizer takes a function to be minimzed, uses JAX to compute the gradient and hessian, and performs newton steps. For convex objectives, the newton direction is guarunteed to be a descent direction, meaning that there is a stepsize such that \\(x_{t+1} = x_t - \\beta H_t^{-1}g_t\\) will decrease the objective function. We start with a step size of \\(1\\) and decay geometrically by a factor \\(\\alpha \\in (0, 1)\\) until the function achieved a sufficient decrease . That means we require \\(f(x_{t+1}) < f(x_t) - \\gamma ||g||_2^2\\) . The two parameters controlling the behavior of the optimizer are alpha , which control the decay of the stepsize in the backtracking line search, and gamma which controls the decrease criteria of accepting a proposed move. Selecting \\(\\gamma = -\\inf\\) corresponds with always accepting the move. When a convex optimization problem is initialized sufficiently close to its optimum it can be shown that a step size of \\(\\beta = 1\\) is always decreasing. However, when initialzed far from the optimum there is no guaruntee. Small, negative values of \\(\\gamma\\) . In practice we find that due to numerical stablity issues can cause updates to slightly increase the objective, while substantially decreasing Newton's decrement, which provides an upper bound on the suboptimality of the solution. We monitor convergence by Newton's decrement, so we find it useful to set gamma to a small negative value, e.g. \\(0.1\\) . In practice, we find this prevents us from divergent behavior at poor initialization, without causing the algorithm to stall. A comment on numerical issues JAX JIT compiler is known to change the numeric output of comptutations. That is, the JIT compiled version of a function might produce slightly different output compared to the original function. This is due to the fact that floating point airthmatic is only an approximation of regular arithmetic, and clever optimizations made by the compiler might change the sequence of operations the machine carries out. In practice, we find that even extremely small steps in the descent direction can apprently increase the objective by a small amount, while dramatically decreasing an upper bound/approximation to the sub-optimality.","title":"Newton"},{"location":"newton/#newtons-method-in-jax","text":"","title":"Newton's method in JAX"},{"location":"newton/#motivation","text":"In order to apply GIBSS you need to provide a univariate regression function that returns, at a minimum, a Bayes factor and a point estimate of the predictions for each observation. We use GIBSS to do Bayesian variable selection in logistic regression or Cox regression. In both these examples (and GLMs more generally) the univariate regression problems are convex. We make the process of implementing new models in GIBSS simple. We implement a Newton's method with backgracking line search in JAX. By providing a an implementation of the objective function in JAX, and utilizing JAXs automatic differentiation and vectorization features, GIBSS automates the implementation of of the single effect regression by first computing the MAP estimate for each variable, and then approximating the Bayes factor via Gauss-Hermite quadrature.","title":"Motivation"},{"location":"newton/#the-newton-optimizer","text":"We use JAX to implement a simple, light weight newton optimizer. At a high level, the optimizer takes a function to be minimzed, uses JAX to compute the gradient and hessian, and performs newton steps. For convex objectives, the newton direction is guarunteed to be a descent direction, meaning that there is a stepsize such that \\(x_{t+1} = x_t - \\beta H_t^{-1}g_t\\) will decrease the objective function. We start with a step size of \\(1\\) and decay geometrically by a factor \\(\\alpha \\in (0, 1)\\) until the function achieved a sufficient decrease . That means we require \\(f(x_{t+1}) < f(x_t) - \\gamma ||g||_2^2\\) . The two parameters controlling the behavior of the optimizer are alpha , which control the decay of the stepsize in the backtracking line search, and gamma which controls the decrease criteria of accepting a proposed move. Selecting \\(\\gamma = -\\inf\\) corresponds with always accepting the move. When a convex optimization problem is initialized sufficiently close to its optimum it can be shown that a step size of \\(\\beta = 1\\) is always decreasing. However, when initialzed far from the optimum there is no guaruntee.","title":"The Newton optimizer."},{"location":"newton/#small-negative-values-of-gamma","text":"In practice we find that due to numerical stablity issues can cause updates to slightly increase the objective, while substantially decreasing Newton's decrement, which provides an upper bound on the suboptimality of the solution. We monitor convergence by Newton's decrement, so we find it useful to set gamma to a small negative value, e.g. \\(0.1\\) . In practice, we find this prevents us from divergent behavior at poor initialization, without causing the algorithm to stall.","title":"Small, negative values of \\(\\gamma\\)."},{"location":"newton/#a-comment-on-numerical-issues","text":"JAX JIT compiler is known to change the numeric output of comptutations. That is, the JIT compiled version of a function might produce slightly different output compared to the original function. This is due to the fact that floating point airthmatic is only an approximation of regular arithmetic, and clever optimizations made by the compiler might change the sequence of operations the machine carries out. In practice, we find that even extremely small steps in the descent direction can apprently increase the objective by a small amount, while dramatically decreasing an upper bound/approximation to the sub-optimality.","title":"A comment on numerical issues"}]}