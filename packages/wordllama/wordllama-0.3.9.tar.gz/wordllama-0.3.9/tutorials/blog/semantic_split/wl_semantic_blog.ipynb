{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21deba6-d7dc-4152-9e4a-67a72e02820f",
   "metadata": {},
   "source": [
    "# Semantic Splitting with WordLlama\n",
    "2024-10-03\n",
    "\n",
    "## TLDR\n",
    "\n",
    "Split text like \"The Lord of the Rings\" in ~750ms, incorporating semantic similarity.\n",
    "\n",
    "## Why spend so much effort on splitting?\n",
    "\n",
    "When splitting/chunking text for Retrieval-Augmented Generation (RAG) applications, the goal is to avoid breaking apart complete ideas or topics. Here's a progression of standard splitting methods:\n",
    "\n",
    "1. Fixed character count: Splits on a set number of characters, even breaking words.\n",
    "2. Word-aware splitting: Forms splits of approximate character counts without breaking words.\n",
    "3. Sentence-level splitting: Breaks only at sentence boundaries, sacrificing some consistency in chunk sizes.\n",
    "4. Paragraph-level splitting: Maintains paragraph integrity but may result in highly variable chunk sizes.\n",
    "\n",
    "These methods don't require language modeling, but they lack semantic awareness. More advanced techniques using language models can provide better semantic coherence but typically at a significant computational cost and latency. And, although semantic splitting is conceptually simple, it still involves multiple steps to refine a quality algorithm.\n",
    "\n",
    "WordLlama is a good platform for accomplishing this, since it can incorporate basic semantic information into the chunking process, without adding significant computational requirements. Here, we develop a recipe for semantic splitting with WordLlama using an intuitive process. \n",
    "\n",
    "## Target texts\n",
    "\n",
    "We focus on chunking for information retrieval and RAG applications. For these use cases, text chunks typically consist of 256-2048 tokens. Input texts can vary widely, including:\n",
    "\n",
    "- Long-form articles or blog posts\n",
    "- Academic papers or research reports\n",
    "- Books or book chapters\n",
    "- Legal documents or contracts\n",
    "- Technical documentation or manuals\n",
    "- Transcripts from interviews, speeches, or conversations\n",
    "\n",
    "The challenge lies in maintaining semantic coherence across diverse text types and lengths while producing consistently sized chunks.\n",
    "\n",
    "## Method Overview\n",
    "\n",
    "Our process involves three main steps:\n",
    "\n",
    "1. `split`: Divide the original text into small chunks.\n",
    "2. `embed`: Generate embeddings for each chunk.\n",
    "3. `reconstruct`: Combine chunks based on similarity information up to target sizes\n",
    "\n",
    "The algorithm aims to:\n",
    "\n",
    "1. Maximize information continuity, keeping related concepts and ideas together\n",
    "2. Produce consistent chunk sizes\n",
    "3. Maintain high performance with low computational requirements\n",
    "\n",
    "### Split\n",
    "\n",
    "The initial split divides the text into smaller units, typically at the paragraph level and at the sentence level when paragraphs exceed the target sizes. This step ensures that the basic grammatical structures remain intact, which is a simple way to preserve semantic information without language models.\n",
    "\n",
    "### Embed\n",
    "\n",
    "Each chunk is embedded using WordLlama. It offers a good balance between semantic representation and computational efficiency.\n",
    "\n",
    "### Reconstruct\n",
    "\n",
    "The reconstruction step checks the similarity between adjacent chunks using their embeddings. It uses this information to make better decisions about where to place chunk boundaries:\n",
    "\n",
    "- Keep semantically similar content together\n",
    "- Avoid splitting in the middle of ideas\n",
    "- Form chunks of consistent sizes\n",
    "\n",
    "Using WordLlama embedding with efficient algorithms, this process can handle large texts quickly, making it suitable for various applications and computational environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ae4d8-9529-400e-b4f2-557e501f7f9b",
   "metadata": {},
   "source": [
    "### Load The Lord of the Rings Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d4105-d9d3-42a8-99d9-a915d838f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "filename = \"lotr_fellowship.txt\"\n",
    "with open(filename, \"rb\") as file:\n",
    "    raw_data = file.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    encoding = result['encoding']\n",
    "\n",
    "with open(filename, \"r\", encoding=encoding) as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10ba04-316b-4804-84eb-83121b93cd62",
   "metadata": {},
   "source": [
    "## Step 1: Split\n",
    "\n",
    "First let's see what we get from a simple `splitlines()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5487c8-473d-46a0-a4f1-4ed826fa60af",
   "metadata": {},
   "source": [
    "### Plot Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c676f-cb8c-4785-a75b-9227aa5c8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_chars(chars_per_line):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    \n",
    "    # First plot: full range\n",
    "    sns.histplot(chars_per_line, bins=200, ax=axes[0], kde=False)\n",
    "    axes[0].set_title(\"Characters per Line - Full Range\")\n",
    "    axes[0].set_xlabel(\"# Characters\")\n",
    "    axes[0].set_ylabel(\"$log($Counts$)$\")\n",
    "    axes[0].semilogy(True)\n",
    "    \n",
    "    # Second plot: zoomed-in range\n",
    "    sns.histplot(chars_per_line, bins=1000, ax=axes[1], kde=False)\n",
    "    axes[1].set_title(\"Characters per Line - Zoomed In (0 to 100)\")\n",
    "    axes[1].set_xlabel(\"# Characters\")\n",
    "    axes[1].set_ylabel(\"Counts\")\n",
    "    axes[1].set_xlim((0, 200))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55540391-e9fc-480a-975e-c65a8a3b3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into lines\n",
    "lines = text.splitlines()\n",
    "\n",
    "# Calculate the number of characters per line\n",
    "chars_per_line = list(map(len, lines))\n",
    "\n",
    "plot_chars(chars_per_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa943d6-9f6d-42f4-91b8-5b40c52daa2b",
   "metadata": {},
   "source": [
    "Here we can see a bunch of small fragments with close to zero size. Additionally, there are some smaller segments below 50 characters. While most of the chunks are fewer than 1k characters, there are a few larger ones as well. The chunks that are a few characters or less are not likely to carry much semantic information and are disproportionate compared to most of the other segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc6d8a-03fa-45ca-8b4c-9914a58eaf20",
   "metadata": {},
   "source": [
    "### Constrained Coalesce\n",
    "\n",
    "I started with `constrained_batches()` from `more-itertools`, which is a nice batching algorithm for gathering texts. However, it is a \"greedy\" algorithm that combines from left to right until a batch would exceed a given constraint by batching with the next item. This can leave the last batch as a small fragment compared to the constraint size, and is very likely to sandwich the fragments between larger sections.\n",
    "\n",
    "To address this, I added a similar idea, but with \"coalesce\" style combination. This does recursive neighbor to neighbor batching up to the constraint size, so that more consistent batch sizes are produced. The result is that the combined segments do not typically hit the max size, but also the smallest fragments are not as small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe3fa6-d58a-42cf-937f-d2e22b42b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from wordllama.algorithms.splitter import constrained_coalesce, constrained_batches, reverse_merge\n",
    "\n",
    "\n",
    "letters = list(string.ascii_lowercase)\n",
    "\n",
    "# using constrained coalesce\n",
    "constrained_coalesce(letters, max_size=5, separator=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c9a90-444c-4e36-8af1-5cbf28b26950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using constrained batches\n",
    "list(map(\"\".join, constrained_batches(letters, max_size=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76891851-1fab-4b6e-a8c0-77f344302d6e",
   "metadata": {},
   "source": [
    "In the coalesce algorithm, we prioritize consistent chunk sizes, which is more beneficial for embedding comparisons. We also add a `reverse_merge` operation which more forcibly merges anything below a certain number of characters `n`\n",
    "with the previous string in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d2baf-37f2-4f63-a4d6-c9dde0922fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into lines\n",
    "lines = text.splitlines()\n",
    "lines = constrained_coalesce(lines, max_size=96, separator=\"\\n\")\n",
    "lines = reverse_merge(lines, n=32, separator=\"\\n\")\n",
    "\n",
    "# Calculate the number of characters per line\n",
    "chars_per_line = list(map(len, lines))\n",
    "\n",
    "plot_chars(chars_per_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d645a-de88-4ac2-9121-1f0380d5371c",
   "metadata": {},
   "source": [
    "This is better. Let's take care of the larger segments.\n",
    "\n",
    "We need to import a function to do sentence splitting. First we'll split the large paragraphs and then recombine them back to a smaller size. If we have to split a paragraph, it's probably best to keep localized sections of it together with other sentences in the split.\n",
    "\n",
    "We'll need to have a target size. 1536 chars is a good number for 512 token width models. We also reverse merge to clean up before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84082725-a378-4693-8df2-b83e3b7c80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import List\n",
    "from wordllama.algorithms.splitter import split_sentences\n",
    "\n",
    "def flatten(nested_list: List[List]) -> List:\n",
    "    return list(chain.from_iterable(nested_list))\n",
    "\n",
    "def constrained_split(\n",
    "    text: str,\n",
    "    target_size: int,\n",
    "    separator: str = \" \",\n",
    ") -> List[str]:\n",
    "    sentences = split_sentences(text)\n",
    "    sentences = constrained_coalesce(sentences, target_size, separator=separator)\n",
    "    sentences = reverse_merge(sentences, n=32, separator=\" \")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a417d1-8ad3-411c-8958-4aff4b589da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = 1536\n",
    "MAX_CHUNK = 512\n",
    "\n",
    "# Split the text into lines\n",
    "lines = text.splitlines()\n",
    "lines = constrained_coalesce(lines, max_size=96, separator=\"\\n\")\n",
    "lines = reverse_merge(lines, n=32, separator=\"\\n\")\n",
    "\n",
    "results = []\n",
    "# break chunks above target size into\n",
    "# sentences and combine into medium sized chunks\n",
    "for line in lines:\n",
    "    if len(line) > TARGET_SIZE:\n",
    "        sentence_chunks = constrained_split(line, MAX_CHUNK, separator=\" \")\n",
    "    else:\n",
    "        sentence_chunks = [line]\n",
    "    results.extend(sentence_chunks)\n",
    "lines = results\n",
    "\n",
    "\n",
    "# Calculate the number of characters per line\n",
    "chars_per_line = list(map(len, lines))\n",
    "\n",
    "plot_chars(chars_per_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b40817-5d93-4b55-bc9e-4c03975cfb79",
   "metadata": {},
   "source": [
    "Now we have a more reasonable starting point for doing semantic splitting. Let's use wordllama to embed the segments into vectors, and compute similarity for all the segments.\n",
    "\n",
    "## Step 2: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306b3c7-ca1b-42b6-823d-ba9bb995363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordllama import WordLlama\n",
    "\n",
    "wl = WordLlama.load()\n",
    "\n",
    "# calculate the cross-similarity\n",
    "embeddings = wl.embed(lines, norm=True)\n",
    "xsim = wl.vector_similarity(embeddings, embeddings)\n",
    "\n",
    "plt.imshow(xsim)\n",
    "plt.grid(False)\n",
    "plt.colorbar()\n",
    "plt.title(\"Cross-similarity of lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24f2aa-94d9-4129-9fac-33e86117b28c",
   "metadata": {},
   "source": [
    "Here's where we can see how wordllama can help. As we traverse the diagonal, we can identify blocks of similar texts. The very small block in the upper left corner is the table of contents.\n",
    "\n",
    "A windowed average should help with finding these blocks. The average should be high when the window spans a region of high similarity, and low between between similarity blocks. Instead of computing the full cross similarity, it makes more sense to compare localized text segments using a sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887251e3-b23d-4a7f-a222-2d84c423dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordllama.algorithms.find_local_minima import windowed_cross_similarity\n",
    "\n",
    "xsim = windowed_cross_similarity(embeddings, window_size=5)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "im1 = ax1.plot(xsim)\n",
    "ax1.set_title('Full: Windowed Cross-similarity')\n",
    "ax1.set_ylabel(\"Cross-similarity\")\n",
    "\n",
    "ax2.plot(range(250, 400), xsim[250:400], 'b-')\n",
    "ax2.set_title('Zoomed in: Windowed Cross-similarity')\n",
    "ax2.set_xlabel('Line #')\n",
    "ax2.axvline(308, ls=\"--\", color=\"r\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feefc633-e0d6-4e7f-8bf8-25336d4a1599",
   "metadata": {},
   "source": [
    "With the size of our segments, even 10-20 segments is a decent chunk size. Here we can zoom in on the minimum around the **dashed red line index (308)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84717667-2a16-417c-8b89-51d0017fe920",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join([lines[i] if i != 308 else f\">>>>>>>>>>>>>{lines[i]}<<<<<<<<<<<<<\" for i in range(305, 310)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e860ca7-7d68-4ea6-96dc-3f7c6160cbac",
   "metadata": {},
   "source": [
    "### Avast, a chapter break!\n",
    "\n",
    "Savitzky-Golay, time do your thing. It's a smoothing filter that fits an Nth-degree polynomial over a small window of points. Therefore, it does not have phase shift, and it provides easily calculable derivatives with low sensitivity to noise. This filter is the basis of our `find_local_minima` algorithm, which looks for the roots of the first derivative (mins and maxes), and checks the sign of the second derivative to determine minima. It then interpolates between points to determine which index to split at.\n",
    "\n",
    "The process goes like this:\n",
    "1. Apply the Savitzky-Golay filter to calculate the first and second derivatives\n",
    "2. Identify roots of the first derivative to find mins/maxes\n",
    "3. Use the sign of the second derivative to distinguish minima\n",
    "4. Interpolate  and round to find the best index for splitting\n",
    "\n",
    "Last, we screen off everything above a percentile (e.g., 0.4), so we're just keeping minima at globally low similarity points. This ensures that we're only splitting at the most significant semantic boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415dbbd3-665b-4102-925f-9aa649837e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wordllama.algorithms.find_local_minima import find_local_minima\n",
    "\n",
    "a,b = (250, 400)\n",
    "\n",
    "results = find_local_minima(xsim, poly_order=2, window_size=3)\n",
    "\n",
    "# filter below median\n",
    "idx = np.where(results[1] < np.quantile(xsim, 0.4))\n",
    "\n",
    "x = results[0][idx]\n",
    "y = results[1][idx]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(xsim)\n",
    "ax1.plot(x, y, \"r.\")\n",
    "ax1.set_title('Full: Windowed Cross-similarity')\n",
    "ax1.set_ylabel(\"Cross-similarity\")\n",
    "\n",
    "ax2.plot(range(a,b), xsim[a:b], 'b-')\n",
    "zoom, = np.where((x < b) & (x >= a))\n",
    "ax2.plot(x[zoom], y[zoom], \"r*\")\n",
    "\n",
    "ax2.set_title('Zoomed in: Windowed Cross-similarity')\n",
    "ax2.set_xlabel('Line #')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75955c5b-529c-4a9f-8028-ff11bd484a0e",
   "metadata": {},
   "source": [
    "Well that was fun. Now all that's left is to bring the sections back up to our target size.\n",
    "\n",
    "Here, we can use batching functions we discussed previously, and take regions in between our semantic split points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c68e025-9ac7-4fcc-9748-d5f482819ca7",
   "metadata": {},
   "source": [
    "## Step 3: Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288166a-fd07-486f-8efa-eb046fc70a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct using the minima as boundaries for coalesce\n",
    "# this ensures that any semantic boundaries are respected\n",
    "chunks = []\n",
    "start = 0\n",
    "for end in x + [len(lines)]:\n",
    "    chunk = constrained_coalesce(lines[start:end], TARGET_SIZE)\n",
    "    chunks.extend(chunk)\n",
    "    start = end\n",
    "\n",
    "lines = list(map(\"\".join, constrained_batches(lines, max_size=TARGET_SIZE, strict=False)))\n",
    "\n",
    "# Calculate the number of characters per line\n",
    "chars_per_line = list(map(len, lines))\n",
    "\n",
    "# plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "sns.histplot(chars_per_line, bins=200, ax=ax, kde=False)\n",
    "ax.set_title(\"Characters per Line - Full Range\")\n",
    "ax.set_xlabel(\"# Characters\")\n",
    "ax.set_ylabel(\"$log($Counts$)$\")\n",
    "ax.semilogy(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb7bae-ec7e-4940-8ee8-29f3c5de2e5b",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314bd35a-fb36-4903-8024-4477fbcb584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_strings(string_list, offset=0):\n",
    "    \"\"\"\n",
    "    Convert a list of strings into a markdown table and display it in a Jupyter notebook.\n",
    "    \n",
    "    Parameters:\n",
    "    - string_list (list): The list of strings to display\n",
    "    \n",
    "    Returns:\n",
    "    - None (displays the table in the notebook)\n",
    "    \"\"\"\n",
    "    # Create the table header\n",
    "    table = \"| Index | Text |\\n|-------|------|\\n\"\n",
    "    \n",
    "    # Add each string to the table\n",
    "    for i, text in enumerate(string_list):\n",
    "        row = f\"| {i + offset} | {text[:600]}{'...' if len(text) > 600 else ''} |\\n\"\n",
    "        table += row\n",
    "    \n",
    "    # Display the table\n",
    "    display(Markdown(table))\n",
    "\n",
    "display_strings(lines[400:406], offset=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b25389-63e8-4426-8fb7-c85037369168",
   "metadata": {},
   "source": [
    "# wl.split() the algorithm\n",
    "\n",
    "Put all of that into an algorithm, and we have built a splitter that works efficiently.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e75af3-00c3-42de-b1fe-0cbfc6eab975",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = wl.split(\n",
    "        text,\n",
    "        target_size=1536,\n",
    "        window_size = 3,\n",
    "        poly_order = 2,\n",
    "        savgol_window = 3,\n",
    ")\n",
    "\n",
    "print(f\"Length of text: {len(text):.2e} chars\\n# of chunks: {len(results)}\\n\\nProcessing time:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f343aa-b367-4dc5-944f-2a13e8469f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
