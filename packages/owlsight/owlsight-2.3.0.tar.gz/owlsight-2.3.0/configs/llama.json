{
    "main": {
        "max_retries_on_error": 3,
        "prompt_retry_on_error": true,
        "prompt_code_execution": true,
        "track_model_usage": false,
        "extra_index_url": "",
        "sequence_on_loading": [
            "U",
            "U",
            "U",
            "ENTER",
            "import inspect",
            "ENTER",
            "function_description = inspect.getdoc(sum_two_numbers)",
            "ENTER",
            "exit()",
            "ENTER",
            "U",
            "U",
            "From a scale of 1 to 10, how would you rate the following function {{function_description}}?",
            "ENTER"
        ]
    },
    "model": {
        "model_id": "lmstudio-community/Llama-3.2-3B-Instruct-GGUF",
        "apply_chat_history": false,
        "system_prompt": "",
        "transformers__device": null,
        "transformers__quantization_bits": null,
        "transformers__stream": true,
        "transformers__model_kwargs": {},
        "gguf__filename": "Llama-3.2-3B-Instruct-Q8_0.gguf",
        "gguf__verbose": false,
        "gguf__n_ctx": 16384,
        "gguf__n_gpu_layers": 0,
        "gguf__n_batch": 8,
        "gguf__n_cpu_threads": 8,
        "onnx__model_dir": "",
        "onnx__verbose": false,
        "onnx__n_cpu_threads": 8
    },
    "generate": {
        "stopwords": [],
        "max_new_tokens": 512,
        "temperature": 0.0,
        "generation_kwargs": {}
    },
    "rag": {
        "active": false,
        "target_library": "",
        "top_k": 3,
        "search": ""
    },
    "huggingface": {
        "search": "gguf 1b llama",
        "top_k": 49,
        "select_model": "lmstudio-community/Llama-3.2-1B-Instruct-GGUF",
        "task": null
    }
}