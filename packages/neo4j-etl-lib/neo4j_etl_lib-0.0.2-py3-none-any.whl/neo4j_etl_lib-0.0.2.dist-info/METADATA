Metadata-Version: 2.3
Name: neo4j-etl-lib
Version: 0.0.2
Summary: Building blocks for ETL pipelines.
Keywords: etl,graph,database
Author-email: Bert Radke <bert.radke@pm.me>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Programming Language :: Python :: 3
Classifier: Topic :: Database
Classifier: Development Status :: 4 - Beta
Requires-Dist: pydantic>=2.10.5; python_version >= '3.8'
Requires-Dist: neo4j>=5.27.0; python_version >= '3.7'
Requires-Dist: python-dotenv>=1.0.1; python_version >= '3.8'
Requires-Dist: tabulate>=0.9.0; python_version >= '3.7'
Requires-Dist: click>=8.1.8; python_version >= '3.7'
Requires-Dist: pytest>=8.3.0 ; extra == "dev" and ( python_version >= '3.8')
Requires-Dist: testcontainers[neo4j]==4.9.0 ; extra == "dev" and ( python_version >= '3.9' and python_version < '4.0')
Requires-Dist: pytest-cov ; extra == "dev"
Requires-Dist: bumpver ; extra == "dev"
Requires-Dist: isort ; extra == "dev"
Requires-Dist: pip-tools ; extra == "dev"
Requires-Dist: sphinx ; extra == "dev"
Requires-Dist: sphinx-rtd-theme ; extra == "dev"
Requires-Dist: pydata-sphinx-theme ; extra == "dev"
Requires-Dist: sphinx-autodoc-typehints ; extra == "dev"
Requires-Dist: sphinxcontrib-napoleon ; extra == "dev"
Requires-Dist: graphdatascience>=1.13 ; extra == "gds" and ( python_version >= '3.9')
Project-URL: Documentation, https://neo-technology-field.github.io/python-etl-lib/index.html
Project-URL: Home, https://github.com/neo-technology-field/python-etl-lib
Provides-Extra: dev
Provides-Extra: gds

# Python ETL Toolbox

Complete documentation can be found on https://neo-technology-field.github.io/python-etl-lib/index.html

A library of building blocks to assemble etl pipelines.

So, instead of providing yet another etl tool, the aim is to provide quality building blocks for the usual etl task. These building blocks should (do) meet the following functional requirements:

* logging (of tasks performed including times, errors, and statistics)
* error handling 
* validation of data (currently via Pydantic)
* batching and streaming
* optionally record the information about performed tasks and provide means (NeoDash, console) to review past etl runs.

While this library currently focuses on Neo4j databases, it can be extended to other sources and sinks as needed. 

It does not provide a CLI out of the box, but contains a set of functions to list and manage past runs (if they are stored in a database). In addition, the provided example illustrates how to assemble a etl pipeline and run it from a CLI.

## Quick guide

### Installation

Package is available on PyPi and can be installed (for development) via:

```bash
python3 -m venv venv
source venv/bin/activate
python -m pip install pip-tools
pip-compile --extra dev pyproject.toml
pip-sync
```

### Usage

The below shows a minimalistic etl pipeline to a single CSV file (look at the GTFS example to see more details)

```python

class LoadAgenciesTask(CSVLoad2Neo4jTasks):
    
    class Agency(BaseModel):
        """ Define the Pydantic model for data validation. """
        id: str = Field(alias="agency_id", default="generic")
        name: str = Field(alias="agency_name")
        url: str = Field(alias="agency_url")
        timezone: str = Field(alias="agency_timezone")
        lang: str = Field(alias="agency_lang")

    def __init__(self, context: ETLContext, file:Path):
        super().__init__(context, LoadAgenciesTask.Agency, file)

    def task_name(self) -> str:
        return f"{self.__class__.__name__}('{self.file}')"

    def _query(self):
        """Load the data into Neo4j."""
        return """ UNWIND $batch AS row
        MERGE (a:Agency {id: row.id})
            SET a.name= row.name, 
            a.url= row.url, 
            a.timezone= row.timezone, 
            a.lang= row.lang
        """

    @classmethod
    def file_name(cls):
        return "agency.txt"

context = ETLContext(env_vars=dict(os.environ))

schema = SchemaTask(context=context)
init_group = TaskGroup(context=context, tasks=[schema], name="schema-init")

tasks = [
    LoadAgenciesTask(context=context, file=input_directory / LoadAgenciesTask.file_name()),
]
csv_group = TaskGroup(context=context, tasks=tasks, name="csv-loading")

all_group = TaskGroup(context=context, tasks=[init_group, csv_group], name="main")

context.reporter.register_tasks(all_group)

all_group.execute()

```
See the provided [example](examples/gtfs/README.md) for a more realistic pipeline and how the logging and reporting would look like.

With the above, all lines in the input file `agency.txt` that do not fit the Pydantic model, would be sent to an json file, containing the error data and a description of why it could not be loaded.

