{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde8a9dc",
   "metadata": {},
   "source": [
    "# LangWatch Batch Evaluation Cookbook\n",
    "\n",
    "## Step 1: Define our LLM pipeline\n",
    "\n",
    "Let's create a simple RAG pipeline using LangChain, guaranteeing that we can get the output and the retrieved documents used during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61c2b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/var/folders/rp/9_s_f3kd1ssb089myww_p9zw0000gn/T/ipykernel_48253/189265682.py:37: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_documents = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "retrieved_documents: ['Introduction - LangWatchLangWatch home pageSearch...SupportDashboardlangwatch/langwatchlangwatch/langwatchSearch...NavigationIntroductionDocumentationLangEvalsOpen DashboardGitHub RepoIntroductionSelf-HostingOptimization StudioOverviewLLM NodesDatasetsEvaluatingOptimizingMonitoring IntegrationOverviewPythonTypeScriptOpenTelemetryLangflow IntegrationFlowise IntegrationREST API IntegrationRAG Context TrackingConceptsCookbooksEvaluationsEvaluationsCustom Evaluator IntegrationGuardrailsOverviewSetting Up GuardrailsUser EventsOverviewEventsDSPy VisualizationQuickstartCustom Optimizer TrackingRAG VisualizationMore FeaturesTriggersAnnotationsDatasetsEmbedded AnalyticsAPI EndpointsTracesAnnotationsTroubleshooting and SupportIntroductionWelcome to LangWatch, the all-in-one open-source LLMops platform.\\nLangWatch allows you to track, monitor, guardrail and evaluate your LLMs apps for measuring quality and alert on issues.', 'LangWatch allows you to track, monitor, guardrail and evaluate your LLMs apps for measuring quality and alert on issues.\\nFor domain experts, it allows you to easily sift through conversations, see topics being discussed and annotate and score messages\\nfor improvement in a collaborative manner with the development team.\\nFor developers, it allows you to debug, build datasets, prompt engineer on the playground and\\nrun batch evaluations or DSPy experiments to continuously improve the product.\\nFinally, for the business, it allows you to track conversation metrics and give full user and quality analytics, cost tracking, build\\ncustom dashboards and even integrate it back on your own platform for reporting to your customers.\\nYou can sign up and already start the integration on our free tier by following the guides bellow:\\nPython Integration GuideTypeScript Integration GuideREST API\\nYou can also open the demo project check out a video on our platform.\\n\\u200bGet in touch', 'Python Integration GuideTypeScript Integration GuideREST API\\nYou can also open the demo project check out a video on our platform.\\n\\u200bGet in touch\\nFeel free to reach out to us directly at [email\\xa0protected]. You can also open a GitHub issue\\nto report bugs and request features, or join our Discord channel and ask questions directly for the community and the core team.Self-HostinggithublinkedinPowered by MintlifyOn this pageGet in touch']\n",
      "output: LangWatch is an open-source LLMops platform for tracking, monitoring, and evaluating LLM applications. It helps measure quality, debug, and provides analytics for developers and businesses. More info: [LangWatch Docs](https://docs.langwatch.ai).\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.langwatch.ai\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "\n",
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "retrieved_documents = []\n",
    "\n",
    "# Wrap the FAISS retriever so that we can capture which documents were used to generate the response\n",
    "@tool\n",
    "def langwatch_search(\n",
    "    query: str\n",
    ") -> list[Document]:\n",
    "    \"\"\"\"Search for information about LangWatch. For any questions about LangWatch, use this tool if you didn't already\"\"\"\n",
    "\n",
    "    global retrieved_documents\n",
    "    retrieved_documents = retriever.get_relevant_documents(query)\n",
    "    return retrieved_documents\n",
    "\n",
    "tools = [langwatch_search]\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that only reply in short tweet-like responses, use tools only once.\\n\\n{agent_scratchpad}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "agent = create_tool_calling_agent(model, tools, prompt)\n",
    "executor = AgentExecutor(agent=agent, tools=tools, verbose=False)  # type: ignore\n",
    "\n",
    "output = executor.invoke({\"question\": \"What is LangWatch?\"})[\"output\"]\n",
    "\n",
    "print(\"\")\n",
    "print(\"retrieved_documents:\", [ d.page_content for d in retrieved_documents])\n",
    "print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac02587",
   "metadata": {},
   "source": [
    "## Step 2: Run the Batch Evaluation Experiment\n",
    "\n",
    "Now we can use the dataset we have from LangWatch to run a batch evaluation experiment through our LLM pipeline, to see the results and tweak it for optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f4041d-b469-42d9-affc-f7ccbc03c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch evaluation...\n",
      "Follow the results at: http://localhost:5560/inbox-narrator/experiments/langwatch-rag-experiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "langwatch.batch_evaluation.DatasetEntry() got multiple values for keyword argument 'contexts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m BatchEvaluation(\n\u001b[1;32m     15\u001b[0m     experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangWatch RAG Experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangwatch-rag\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     evaluations\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-moderation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m     callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m results\u001b[38;5;241m.\u001b[39mdf\n",
      "File \u001b[0;32m~/Projects/langwatch-saas/langwatch/python-sdk/langwatch/batch_evaluation.py:170\u001b[0m, in \u001b[0;36mBatchEvaluation.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     futures: List[Future] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    167\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_record, record) \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m dataset\n\u001b[1;32m    168\u001b[0m     ]\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(as_completed(futures), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal):\n\u001b[0;32m--> 170\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    172\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_completion)\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch evaluation done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Projects/langwatch-saas/langwatch/python-sdk/langwatch/batch_evaluation.py:221\u001b[0m, in \u001b[0;36mBatchEvaluation.run_record\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    216\u001b[0m     entry_with_output \u001b[38;5;241m=\u001b[39m DatasetEntry(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mentry\u001b[38;5;241m.\u001b[39mmodel_dump(),\n\u001b[1;32m    218\u001b[0m         output\u001b[38;5;241m=\u001b[39mcallbackResponse,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     entry_with_output \u001b[38;5;241m=\u001b[39m DatasetEntry(\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mentry\u001b[38;5;241m.\u001b[39mmodel_dump(),\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(callbackResponse \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    224\u001b[0m     )\n\u001b[1;32m    226\u001b[0m coroutines: \u001b[38;5;28mlist\u001b[39m[Coroutine[Tuple[\u001b[38;5;28mstr\u001b[39m, SingleEvaluationResult], Any, Any]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m evaluation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluations:\n",
      "\u001b[0;31mTypeError\u001b[0m: langwatch.batch_evaluation.DatasetEntry() got multiple values for keyword argument 'contexts'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langwatch.batch_evaluation import BatchEvaluation, DatasetEntry\n",
    "\n",
    "\n",
    "def callback(entry: DatasetEntry):\n",
    "    output = executor.invoke({\"question\": entry[\"input\"]})[\"output\"]\n",
    "\n",
    "    return {\"output\": output, \"contexts\": [d.page_content for d in retrieved_documents]}\n",
    "\n",
    "# Instantiate the BatchEvaluation object\n",
    "evaluation = BatchEvaluation(\n",
    "    experiment=\"LangWatch RAG Experiment\",\n",
    "    dataset=\"langwatch-rag\",\n",
    "    evaluations=[\"openai-moderation\",\"faithfulness\"],\n",
    "    callback=callback,\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "results = evaluation.run()\n",
    "results.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa8b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
