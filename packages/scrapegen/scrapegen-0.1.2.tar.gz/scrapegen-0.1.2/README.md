# ScrapeGen

<img src="https://github.com/user-attachments/assets/2f458a05-66f9-47a4-bc40-6069e3c9e849" alt="Logo" width="80" height="80">

ScrapeGen ğŸš€ is a powerful Python library that combines web scraping with AI-driven data extraction to collect and structure information from websites efficiently. It leverages Google's Gemini models for intelligent data processing and provides a flexible, configurable framework for web scraping operations.

## âœ¨ Features

- **ğŸ¤– AI-Powered Data Extraction**: Utilizes Google's Gemini models for intelligent parsing
- **âš™ï¸ Configurable Web Scraping**: Supports depth control and flexible extraction rules
- **ğŸ“Š Structured Data Modeling**: Uses Pydantic for well-defined data structures
- **ğŸ›¡ï¸ Robust Error Handling**: Implements retry mechanisms and detailed error reporting
- **ğŸ”§ Customizable Scraping Configurations**: Adjust settings dynamically based on needs
- **ğŸŒ Comprehensive URL Handling**: Supports both relative and absolute URLs
- **ğŸ“¦ Modular Architecture**: Ensures clear separation of concerns for maintainability

## ğŸ“¥ Installation

```bash
pip install scrapegen
```

## ğŸ“Œ Requirements

- Python 3.7+
- Google API Key (for Gemini models)
- Required Python packages:
  - requests
  - beautifulsoup4
  - langchain
  - langchain-google-genai
  - pydantic

## ğŸš€ Quick Start with Custom Prompts

```python
from scrapegen import ScrapeGen, CompanyInfo, CompaniesInfo

# Initialize ScrapeGen with your Google API key
scraper = ScrapeGen(api_key="your-google-api-key", model="gemini-1.5-pro")

# Define target URL and custom prompt
url = "https://example.com"
custom_prompt = """
Analyze the website content and extract:
- Company name
- Core technologies
- Industry focus areas
- Key product features
"""

# Scrape with custom prompt and model
companies_data = scraper.scrape(
    url=url,
    prompt=custom_prompt,
    base_model=CompaniesInfo
)

# Display extracted data
for company in companies_data.companies:
    print(f"ğŸ¢ {company.company_name}")
    print(f"ğŸ”§ Technologies: {', '.join(company.core_technologies)}")
    print(f"ğŸ“ˆ Focus Areas: {', '.join(company.industry_focus)}")
```

## âš™ï¸ Configuration

### ğŸ”¹ ScrapeConfig Options

```python
from scrapegen import ScrapeConfig

config = ScrapeConfig(
    max_pages=20,      # Max pages to scrape per depth level
    max_subpages=2,    # Max subpages to scrape per page
    max_depth=1,       # Max depth to follow links
    timeout=30,        # Request timeout in seconds
    retries=3,         # Number of retry attempts
    user_agent="Mozilla/5.0 (compatible; ScrapeGen/1.0)",
    headers=None       # Additional HTTP headers
)
```

### ğŸ”„ Updating Configuration

```python
scraper = ScrapeGen(api_key="your-api-key", model="gemini-1.5-pro", config=config)

# Dynamically update configuration
scraper.update_config(max_pages=30, timeout=45)
```

## ğŸ“Œ Custom Data Models

Define Pydantic models to structure extracted data:

```python
from pydantic import BaseModel
from typing import Optional, List

class CustomDataModel(BaseModel):
    title: str
    description: Optional[str]
    date: str
    tags: List[str]

class CustomDataCollection(BaseModel):
    items: List[CustomDataModel]

# Scrape using the custom model
data = scraper.scrape(url, CustomDataCollection)
```

## ğŸ¤– Supported Gemini Models

- gemini-1.5-flash-8b
- gemini-1.5-pro
- gemini-2.0-flash-exp
- gemini-1.5-flash

## ğŸ†• Custom Prompt Engineering Guide

### 1ï¸âƒ£ Basic Prompt Structure

```python
basic_prompt = """
Extract the following details from the content:
- Company name
- Founding year
- Headquarters location
- Main products/services
"""
```

### 2ï¸âƒ£ Tech-Focused Extraction

```python
tech_prompt = """
Identify and categorize technologies mentioned in the content:
1. AI/ML Technologies
2. Cloud Infrastructure
3. Data Analytics Tools
4. Cybersecurity Solutions
Include version numbers and implementation details where available.
"""
```

### 3ï¸âƒ£ Multi-Level Extraction

```python
multi_level_prompt = """
Perform hierarchical extraction:
1. Company Overview:
   - Name
   - Mission statement
   - Key executives
2. Technical Capabilities:
   - Core technologies
   - Development stack
   - Infrastructure
3. Market Position:
   - Competitors
   - Market share
   - Growth metrics
"""
```

## ğŸ“Œ Specialized Prompt Examples

### ğŸ” Competitive Analysis Prompt

```python
competitor_prompt = """
Identify and compare key competitors:
- Competitor names
- Feature differentiators
- Pricing models
- Market positioning
Output as a comparison table with relative strengths.
"""
```

### ğŸŒ± Sustainability Focused Prompt

```python
green_prompt = """
Extract environmental sustainability information:
1. Green initiatives
2. Carbon reduction targets
3. Eco-friendly technologies
4. Sustainability certifications
5. Renewable energy usage
Prioritize quantitative metrics and timelines.
"""
```

### ğŸ’¡ Innovation Tracking Prompt

```python
innovation_prompt = """
Analyze R&D activities and innovations:
- Recent patents filed
- Research partnerships
- New product launches (last 24 months)
- Technology breakthroughs
- Investment in R&D (% of revenue)
"""
```

## ğŸ› ï¸ Prompt Optimization Tips

1. **Be Specific**: Clearly define required fields and formats

   ```python
   "Format output as JSON with 'company_name', 'employees', 'revenue' keys"
   ```
2. **Add Context**:

   ```python
   "Analyze content from CEO interviews for strategic priorities"
   ```
3. **Define Output Structure**:

   ```python
   "Categorize findings under 'basic_info', 'tech_stack', 'growth_metrics'"
   ```
4. **Set Priorities**:

   ```python
   "Focus on technical specifications over marketing content"
   ```

## âš ï¸ Error Handling

ScrapeGen provides specific exception classes for detailed error handling:

- **â— ScrapeGenError**: Base exception class
- **âš™ï¸ ConfigurationError**: Errors related to scraper configuration
- **ğŸ•·ï¸ ScrapingError**: Issues encountered during web scraping
- **ğŸ” ExtractionError**: Problems with AI-driven data extraction

Example usage:

```python
try:
    data = scraper.scrape(
        url=url,
        prompt=complex_prompt,
        base_model=MarketAnalysis
    )
except ExtractionError as e:
    print(f"ğŸ” Extraction failed with custom prompt: {e}")
    print(f"ğŸ§  Prompt used: {complex_prompt}")
except ScrapingError as e:
    print(f"ğŸŒ Scraping error: {str(e)}")
```

## ğŸ—ï¸ Architecture

ScrapeGen follows a modular design for scalability and maintainability:

1. **ğŸ•·ï¸ WebsiteScraper**: Handles core web scraping logic
2. **ğŸ“‘ InfoExtractorAi**: Performs AI-driven content extraction
3. **ğŸ¤– LlmManager**: Manages interactions with language models
4. **ğŸ”— UrlParser**: Parses and normalizes URLs
5. **ğŸ“¥ ContentExtractor**: Extracts structured data from HTML elements

## âœ… Best Practices

### 1ï¸âƒ£ Rate Limiting

- â³ Use delays between requests
- ğŸ“œ Respect robots.txt guidelines
- âš–ï¸ Configure max_pages and max_depth responsibly

### 2ï¸âƒ£ Error Handling

- ğŸ”„ Wrap scraping operations in try-except blocks
- ğŸ“‹ Implement proper logging for debugging
- ğŸ” Handle network timeouts and retries effectively

### 3ï¸âƒ£ Resource Management

- ğŸ–¥ï¸ Monitor memory usage for large-scale operations
- ğŸ“š Implement pagination for large datasets
- â±ï¸ Adjust timeout settings based on expected response times

## ğŸ¤ Contributing

Contributions are welcome! ğŸ‰ Feel free to submit a Pull Request to improve ScrapeGen.

## âœ¨ Features

- **ğŸ¤– AI-Powered Data Extraction**: Utilizes Google's Gemini models for intelligent parsing.
- **âš™ï¸ Configurable Web Scraping**: Supports depth control and flexible extraction rules.
- **ğŸ“Š Structured Data Modeling**: Uses Pydantic for well-defined data structures.
- **ğŸ›¡ï¸ Robust Error Handling**: Implements retry mechanisms and detailed error reporting.
- **ğŸ”§ Customizable Scraping Configurations**: Adjust settings dynamically based on needs.
- **ğŸŒ Comprehensive URL Handling**: Supports both relative and absolute URLs.
- **ğŸ“¦ Modular Architecture**: Ensures clear separation of concerns for maintainability.

## ğŸ“¥ Installation

```bash
pip install scrapegen  # Package name may vary
```

## ğŸ“Œ Requirements

- Python 3.7+
- Google API Key (for Gemini models)
- Required Python packages:
  - requests
  - beautifulsoup4
  - langchain
  - langchain-google-genai
  - pydantic

## ğŸš€ Quick Start

```python
from scrapegen import ScrapeGen, CompanyInfo, CompaniesInfo

# Initialize ScrapeGen with your Google API key
scraper = ScrapeGen(api_key="your-google-api-key", model="gemini-1.5-pro")

# Define the target URL
url = "https://example.com"

# Scrape and extract company information
companies_data = scraper.scrape(url, CompaniesInfo)

# Display extracted data
for company in companies_data.companies:
    print(f"ğŸ¢ Company Name: {company.company_name}")
    print(f"ğŸ“„ Description: {company.company_description}")
```

## âš™ï¸ Configuration

### ğŸ”¹ ScrapeConfig Options

```python
from scrapegen import ScrapeConfig

config = ScrapeConfig(
    max_pages=20,      # Max pages to scrape per depth level
    max_subpages=2,    # Max subpages to scrape per page
    max_depth=1,       # Max depth to follow links
    timeout=30,        # Request timeout in seconds
    retries=3,         # Number of retry attempts
    user_agent="Mozilla/5.0 (compatible; ScrapeGen/1.0)",
    headers=None       # Additional HTTP headers
)
```

### ğŸ”„ Updating Configuration

```python
scraper = ScrapeGen(api_key="your-api-key", model="gemini-1.5-pro", config=config)

# Dynamically update configuration
scraper.update_config(max_pages=30, timeout=45)
```

## ğŸ“Œ Custom Data Models

Define Pydantic models to structure extracted data:

```python
from pydantic import BaseModel
from typing import Optional, List

class CustomDataModel(BaseModel):
    title: str
    description: Optional[str]
    date: str
    tags: List[str]

class CustomDataCollection(BaseModel):
    items: List[CustomDataModel]

# Scrape using the custom model
data = scraper.scrape(url, CustomDataCollection)
```

## ğŸ¤– Supported Gemini Models

- gemini-1.5-flash-8b
- gemini-1.5-pro
- gemini-2.0-flash-exp
- gemini-1.5-flash

## âš ï¸ Error Handling

ScrapeGen provides specific exception classes for detailed error handling:

- **â— ScrapeGenError**: Base exception class.
- **âš™ï¸ ConfigurationError**: Errors related to scraper configuration.
- **ğŸ•·ï¸ ScrapingError**: Issues encountered during web scraping.
- **ğŸ” ExtractionError**: Problems with AI-driven data extraction.

Example usage:

```python
try:
    data = scraper.scrape(url, CustomDataCollection)
except ConfigurationError as e:
    print(f"âš™ï¸ Configuration error: {e}")
except ScrapingError as e:
    print(f"ğŸ•·ï¸ Scraping error: {e}")
except ExtractionError as e:
    print(f"ğŸ” Extraction error: {e}")
```

## ğŸ—ï¸ Architecture

ScrapeGen follows a modular design for scalability and maintainability:

1. **ğŸ•·ï¸ WebsiteScraper**: Handles core web scraping logic.
2. **ğŸ“‘ InfoExtractorAi**: Performs AI-driven content extraction.
3. **ğŸ¤– LlmManager**: Manages interactions with language models.
4. **ğŸ”— UrlParser**: Parses and normalizes URLs.
5. **ğŸ“¥ ContentExtractor**: Extracts structured data from HTML elements.

## âœ… Best Practices

### 1ï¸âƒ£ Rate Limiting

- â³ Use delays between requests.
- ğŸ“œ Respect robots.txt guidelines.
- âš–ï¸ Configure max_pages and max_depth responsibly.

### 2ï¸âƒ£ Error Handling

- ğŸ”„ Wrap scraping operations in try-except blocks.
- ğŸ“‹ Implement proper logging for debugging.
- ğŸ” Handle network timeouts and retries effectively.

### 3ï¸âƒ£ Resource Management

- ğŸ–¥ï¸ Monitor memory usage for large-scale operations.
- ğŸ“š Implement pagination for large datasets.
- â±ï¸ Adjust timeout settings based on expected response times.

## ğŸ¤ Contributing

Contributions are welcome! ğŸ‰ Feel free to submit a Pull Request to improve ScrapeGen.
