# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.project import Project
from ..core.pydantic_utilities import parse_obj_as
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from ..core.jsonable_encoder import jsonable_encoder
from ..types.interval_usage_and_plan import IntervalUsageAndPlan
from ..types.eval_dataset import EvalDataset
from ..types.local_eval import LocalEval
from ..types.local_eval_results import LocalEvalResults
from ..core.serialization import convert_and_respect_annotation_metadata
from ..types.local_eval_sets import LocalEvalSets
from ..types.prompt_mixin_prompts import PromptMixinPrompts
from ..types.prompt_spec import PromptSpec
from ..core.client_wrapper import AsyncClientWrapper

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class ProjectsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list_projects(
        self,
        *,
        organization_id: typing.Optional[str] = None,
        project_name: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[Project]:
        """
        List projects or get one by name

        Parameters
        ----------
        organization_id : typing.Optional[str]

        project_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[Project]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.list_projects()
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/projects",
            method="GET",
            params={
                "organization_id": organization_id,
                "project_name": project_name,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[Project],
                    parse_obj_as(
                        type_=typing.List[Project],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_project(
        self,
        *,
        name: str,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Project:
        """
        Create a new project.

        Parameters
        ----------
        name : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Project
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.create_project(
            name="name",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/projects",
            method="POST",
            params={
                "organization_id": organization_id,
            },
            json={
                "name": name,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Project,
                    parse_obj_as(
                        type_=Project,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert_project(
        self,
        *,
        name: str,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Project:
        """
        Upsert a project.
        Updates if a project with the same name already exists. Otherwise, creates a new project.

        Parameters
        ----------
        name : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Project
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.upsert_project(
            name="name",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/projects",
            method="PUT",
            params={
                "organization_id": organization_id,
            },
            json={
                "name": name,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Project,
                    parse_obj_as(
                        type_=Project,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Project:
        """
        Get a project by ID.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Project
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.get_project()
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Project,
                    parse_obj_as(
                        type_=Project,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_existing_project(
        self,
        project_id: typing.Optional[str],
        *,
        name: str,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Project:
        """
        Update an existing project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        name : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Project
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.update_existing_project(
            name="name",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}",
            method="PUT",
            params={
                "organization_id": organization_id,
            },
            json={
                "name": name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Project,
                    parse_obj_as(
                        type_=Project,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Delete a project by ID.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.delete_project()
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}",
            method="DELETE",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_project_usage(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> IntervalUsageAndPlan:
        """
        Get usage for a project

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        IntervalUsageAndPlan
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.get_project_usage()
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/usage",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    IntervalUsageAndPlan,
                    parse_obj_as(
                        type_=IntervalUsageAndPlan,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_datasets_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[EvalDataset]:
        """
        List eval datasets for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[EvalDataset]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.list_datasets_for_project()
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/eval/dataset",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[EvalDataset],
                    parse_obj_as(
                        type_=typing.List[EvalDataset],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_eval_dataset_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        name: str,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvalDataset:
        """
        Create a new eval dataset for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        name : str
            The name of the EvalDataset.

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvalDataset
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.create_eval_dataset_for_project(
            name="name",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/eval/dataset",
            method="POST",
            params={
                "organization_id": organization_id,
            },
            json={
                "name": name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvalDataset,
                    parse_obj_as(
                        type_=EvalDataset,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_local_eval_set_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        app_name: str,
        results: typing.Dict[str, typing.Sequence[LocalEval]],
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[LocalEvalResults]:
        """
        Create a new local eval set.

        Parameters
        ----------
        project_id : typing.Optional[str]

        app_name : str
            The name of the app.

        results : typing.Dict[str, typing.Sequence[LocalEval]]
            The eval results.

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[LocalEvalResults]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud, LocalEval

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.create_local_eval_set_for_project(
            app_name="app_name",
            results={"key": [LocalEval()]},
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/localevalset",
            method="POST",
            params={
                "organization_id": organization_id,
            },
            json={
                "app_name": app_name,
                "results": convert_and_respect_annotation_metadata(
                    object_=results, annotation=typing.Dict[str, typing.Sequence[LocalEval]], direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[LocalEvalResults],
                    parse_obj_as(
                        type_=typing.List[LocalEvalResults],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_local_evals_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[LocalEvalResults]:
        """
        List local eval results for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[LocalEvalResults]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.list_local_evals_for_project()
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/localeval",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[LocalEvalResults],
                    parse_obj_as(
                        type_=typing.List[LocalEvalResults],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_local_eval_sets_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[LocalEvalSets]:
        """
        List local eval sets for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[LocalEvalSets]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.list_local_eval_sets_for_project()
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/localevalsets",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[LocalEvalSets],
                    parse_obj_as(
                        type_=typing.List[LocalEvalSets],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_local_eval_set(
        self,
        project_id: typing.Optional[str],
        local_eval_set_id: str,
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Delete a local eval set.

        Parameters
        ----------
        project_id : typing.Optional[str]

        local_eval_set_id : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.delete_local_eval_set(
            local_eval_set_id="local_eval_set_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/localevalset/{jsonable_encoder(local_eval_set_id)}",
            method="DELETE",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Optional[typing.Any],
                    parse_obj_as(
                        type_=typing.Optional[typing.Any],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_promptmixin_prompts(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PromptMixinPrompts]:
        """
        List PromptMixin prompt sets for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PromptMixinPrompts]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.list_promptmixin_prompts()
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/prompts",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PromptMixinPrompts],
                    parse_obj_as(
                        type_=typing.List[PromptMixinPrompts],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_prompt_mixin_prompts(
        self,
        project_id_: typing.Optional[str],
        *,
        project_id: str,
        name: str,
        prompts: typing.Sequence[PromptSpec],
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptMixinPrompts:
        """
        Create a new PromptMixin prompt set.

        Parameters
        ----------
        project_id_ : typing.Optional[str]

        project_id : str
            The ID of the project.

        name : str
            The name of the prompt set.

        prompts : typing.Sequence[PromptSpec]
            The prompts.

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptMixinPrompts
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud, PromptSpec

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.create_prompt_mixin_prompts(
            project_id="project_id",
            name="name",
            prompts=[
                PromptSpec(
                    prompt_key="prompt_key",
                    prompt_class="prompt_class",
                    prompt_type="prompt_type",
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id_)}/prompts",
            method="POST",
            params={
                "organization_id": organization_id,
            },
            json={
                "project_id": project_id,
                "name": name,
                "prompts": convert_and_respect_annotation_metadata(
                    object_=prompts, annotation=typing.Sequence[PromptSpec], direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptMixinPrompts,
                    parse_obj_as(
                        type_=PromptMixinPrompts,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_promptmixin_prompts(
        self,
        project_id_: typing.Optional[str],
        prompt_set_id: str,
        *,
        project_id: str,
        name: str,
        prompts: typing.Sequence[PromptSpec],
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptMixinPrompts:
        """
        Update a PromptMixin prompt set.

        Parameters
        ----------
        project_id_ : typing.Optional[str]

        prompt_set_id : str

        project_id : str
            The ID of the project.

        name : str
            The name of the prompt set.

        prompts : typing.Sequence[PromptSpec]
            The prompts.

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptMixinPrompts
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud, PromptSpec

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.update_promptmixin_prompts(
            prompt_set_id="prompt_set_id",
            project_id="project_id",
            name="name",
            prompts=[
                PromptSpec(
                    prompt_key="prompt_key",
                    prompt_class="prompt_class",
                    prompt_type="prompt_type",
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id_)}/prompts/{jsonable_encoder(prompt_set_id)}",
            method="PUT",
            params={
                "organization_id": organization_id,
            },
            json={
                "project_id": project_id,
                "name": name,
                "prompts": convert_and_respect_annotation_metadata(
                    object_=prompts, annotation=typing.Sequence[PromptSpec], direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptMixinPrompts,
                    parse_obj_as(
                        type_=PromptMixinPrompts,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_prompt_mixin_prompts(
        self,
        project_id: typing.Optional[str],
        prompt_set_id: str,
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Delete a PromptMixin prompt set.

        Parameters
        ----------
        project_id : typing.Optional[str]

        prompt_set_id : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.projects.delete_prompt_mixin_prompts(
            prompt_set_id="prompt_set_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/prompts/{jsonable_encoder(prompt_set_id)}",
            method="DELETE",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Optional[typing.Any],
                    parse_obj_as(
                        type_=typing.Optional[typing.Any],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncProjectsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list_projects(
        self,
        *,
        organization_id: typing.Optional[str] = None,
        project_name: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[Project]:
        """
        List projects or get one by name

        Parameters
        ----------
        organization_id : typing.Optional[str]

        project_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[Project]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.list_projects()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/projects",
            method="GET",
            params={
                "organization_id": organization_id,
                "project_name": project_name,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[Project],
                    parse_obj_as(
                        type_=typing.List[Project],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_project(
        self,
        *,
        name: str,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Project:
        """
        Create a new project.

        Parameters
        ----------
        name : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Project
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.create_project(
                name="name",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/projects",
            method="POST",
            params={
                "organization_id": organization_id,
            },
            json={
                "name": name,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Project,
                    parse_obj_as(
                        type_=Project,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert_project(
        self,
        *,
        name: str,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Project:
        """
        Upsert a project.
        Updates if a project with the same name already exists. Otherwise, creates a new project.

        Parameters
        ----------
        name : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Project
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.upsert_project(
                name="name",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/projects",
            method="PUT",
            params={
                "organization_id": organization_id,
            },
            json={
                "name": name,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Project,
                    parse_obj_as(
                        type_=Project,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Project:
        """
        Get a project by ID.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Project
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.get_project()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Project,
                    parse_obj_as(
                        type_=Project,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_existing_project(
        self,
        project_id: typing.Optional[str],
        *,
        name: str,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Project:
        """
        Update an existing project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        name : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Project
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.update_existing_project(
                name="name",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}",
            method="PUT",
            params={
                "organization_id": organization_id,
            },
            json={
                "name": name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Project,
                    parse_obj_as(
                        type_=Project,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Delete a project by ID.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.delete_project()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}",
            method="DELETE",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_project_usage(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> IntervalUsageAndPlan:
        """
        Get usage for a project

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        IntervalUsageAndPlan
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.get_project_usage()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/usage",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    IntervalUsageAndPlan,
                    parse_obj_as(
                        type_=IntervalUsageAndPlan,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_datasets_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[EvalDataset]:
        """
        List eval datasets for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[EvalDataset]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.list_datasets_for_project()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/eval/dataset",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[EvalDataset],
                    parse_obj_as(
                        type_=typing.List[EvalDataset],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_eval_dataset_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        name: str,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvalDataset:
        """
        Create a new eval dataset for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        name : str
            The name of the EvalDataset.

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvalDataset
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.create_eval_dataset_for_project(
                name="name",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/eval/dataset",
            method="POST",
            params={
                "organization_id": organization_id,
            },
            json={
                "name": name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvalDataset,
                    parse_obj_as(
                        type_=EvalDataset,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_local_eval_set_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        app_name: str,
        results: typing.Dict[str, typing.Sequence[LocalEval]],
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[LocalEvalResults]:
        """
        Create a new local eval set.

        Parameters
        ----------
        project_id : typing.Optional[str]

        app_name : str
            The name of the app.

        results : typing.Dict[str, typing.Sequence[LocalEval]]
            The eval results.

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[LocalEvalResults]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, LocalEval

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.create_local_eval_set_for_project(
                app_name="app_name",
                results={"key": [LocalEval()]},
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/localevalset",
            method="POST",
            params={
                "organization_id": organization_id,
            },
            json={
                "app_name": app_name,
                "results": convert_and_respect_annotation_metadata(
                    object_=results, annotation=typing.Dict[str, typing.Sequence[LocalEval]], direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[LocalEvalResults],
                    parse_obj_as(
                        type_=typing.List[LocalEvalResults],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_local_evals_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[LocalEvalResults]:
        """
        List local eval results for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[LocalEvalResults]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.list_local_evals_for_project()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/localeval",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[LocalEvalResults],
                    parse_obj_as(
                        type_=typing.List[LocalEvalResults],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_local_eval_sets_for_project(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[LocalEvalSets]:
        """
        List local eval sets for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[LocalEvalSets]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.list_local_eval_sets_for_project()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/localevalsets",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[LocalEvalSets],
                    parse_obj_as(
                        type_=typing.List[LocalEvalSets],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_local_eval_set(
        self,
        project_id: typing.Optional[str],
        local_eval_set_id: str,
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Delete a local eval set.

        Parameters
        ----------
        project_id : typing.Optional[str]

        local_eval_set_id : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.delete_local_eval_set(
                local_eval_set_id="local_eval_set_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/localevalset/{jsonable_encoder(local_eval_set_id)}",
            method="DELETE",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Optional[typing.Any],
                    parse_obj_as(
                        type_=typing.Optional[typing.Any],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_promptmixin_prompts(
        self,
        project_id: typing.Optional[str],
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PromptMixinPrompts]:
        """
        List PromptMixin prompt sets for a project.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PromptMixinPrompts]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.list_promptmixin_prompts()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/prompts",
            method="GET",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PromptMixinPrompts],
                    parse_obj_as(
                        type_=typing.List[PromptMixinPrompts],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_prompt_mixin_prompts(
        self,
        project_id_: typing.Optional[str],
        *,
        project_id: str,
        name: str,
        prompts: typing.Sequence[PromptSpec],
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptMixinPrompts:
        """
        Create a new PromptMixin prompt set.

        Parameters
        ----------
        project_id_ : typing.Optional[str]

        project_id : str
            The ID of the project.

        name : str
            The name of the prompt set.

        prompts : typing.Sequence[PromptSpec]
            The prompts.

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptMixinPrompts
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, PromptSpec

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.create_prompt_mixin_prompts(
                project_id="project_id",
                name="name",
                prompts=[
                    PromptSpec(
                        prompt_key="prompt_key",
                        prompt_class="prompt_class",
                        prompt_type="prompt_type",
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id_)}/prompts",
            method="POST",
            params={
                "organization_id": organization_id,
            },
            json={
                "project_id": project_id,
                "name": name,
                "prompts": convert_and_respect_annotation_metadata(
                    object_=prompts, annotation=typing.Sequence[PromptSpec], direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptMixinPrompts,
                    parse_obj_as(
                        type_=PromptMixinPrompts,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_promptmixin_prompts(
        self,
        project_id_: typing.Optional[str],
        prompt_set_id: str,
        *,
        project_id: str,
        name: str,
        prompts: typing.Sequence[PromptSpec],
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PromptMixinPrompts:
        """
        Update a PromptMixin prompt set.

        Parameters
        ----------
        project_id_ : typing.Optional[str]

        prompt_set_id : str

        project_id : str
            The ID of the project.

        name : str
            The name of the prompt set.

        prompts : typing.Sequence[PromptSpec]
            The prompts.

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PromptMixinPrompts
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, PromptSpec

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.update_promptmixin_prompts(
                prompt_set_id="prompt_set_id",
                project_id="project_id",
                name="name",
                prompts=[
                    PromptSpec(
                        prompt_key="prompt_key",
                        prompt_class="prompt_class",
                        prompt_type="prompt_type",
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id_)}/prompts/{jsonable_encoder(prompt_set_id)}",
            method="PUT",
            params={
                "organization_id": organization_id,
            },
            json={
                "project_id": project_id,
                "name": name,
                "prompts": convert_and_respect_annotation_metadata(
                    object_=prompts, annotation=typing.Sequence[PromptSpec], direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PromptMixinPrompts,
                    parse_obj_as(
                        type_=PromptMixinPrompts,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_prompt_mixin_prompts(
        self,
        project_id: typing.Optional[str],
        prompt_set_id: str,
        *,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Delete a PromptMixin prompt set.

        Parameters
        ----------
        project_id : typing.Optional[str]

        prompt_set_id : str

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.projects.delete_prompt_mixin_prompts(
                prompt_set_id="prompt_set_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/projects/{jsonable_encoder(project_id)}/prompts/{jsonable_encoder(prompt_set_id)}",
            method="DELETE",
            params={
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Optional[typing.Any],
                    parse_obj_as(
                        type_=typing.Optional[typing.Any],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
