# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.extract_agent import ExtractAgent
from ..core.pydantic_utilities import parse_obj_as
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from .types.extract_agent_create_data_schema import ExtractAgentCreateDataSchema
from ..types.extract_config import ExtractConfig
from ..core.serialization import convert_and_respect_annotation_metadata
from .types.extract_schema_validate_request_data_schema import ExtractSchemaValidateRequestDataSchema
from ..types.extract_schema_validate_response import ExtractSchemaValidateResponse
from ..core.jsonable_encoder import jsonable_encoder
from .types.extract_agent_update_data_schema import ExtractAgentUpdateDataSchema
from ..types.extract_job import ExtractJob
from ..types.extract_job_create_data_schema_override import ExtractJobCreateDataSchemaOverride
from ..types.extract_job_create import ExtractJobCreate
from ..types.llama_extract_settings import LlamaExtractSettings
from ..types.extract_resultset import ExtractResultset
from ..types.extract_run import ExtractRun
from ..core.client_wrapper import AsyncClientWrapper

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class LlamaExtractClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list_extraction_agents(
        self, *, project_id: typing.Optional[str] = None, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[ExtractAgent]:
        """
        Parameters
        ----------
        project_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractAgent]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.list_extraction_agents()
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/extraction-agents",
            method="GET",
            params={
                "project_id": project_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[ExtractAgent],
                    parse_obj_as(
                        type_=typing.List[ExtractAgent],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_extraction_agent(
        self,
        *,
        name: str,
        data_schema: ExtractAgentCreateDataSchema,
        config: ExtractConfig,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        name : str
            The name of the extraction schema

        data_schema : ExtractAgentCreateDataSchema
            The schema of the data.

        config : ExtractConfig
            The configuration parameters for the extraction agent.

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        from llama_index import ExtractConfig, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.create_extraction_agent(
            name="name",
            data_schema={},
            config=ExtractConfig(),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/extraction-agents",
            method="POST",
            params={
                "project_id": project_id,
                "organization_id": organization_id,
            },
            json={
                "name": name,
                "data_schema": convert_and_respect_annotation_metadata(
                    object_=data_schema, annotation=ExtractAgentCreateDataSchema, direction="write"
                ),
                "config": convert_and_respect_annotation_metadata(
                    object_=config, annotation=ExtractConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractAgent,
                    parse_obj_as(
                        type_=ExtractAgent,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def validate_extraction_schema(
        self,
        *,
        data_schema: ExtractSchemaValidateRequestDataSchema,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractSchemaValidateResponse:
        """
        Validates an extraction agent's schema definition.
        Returns the normalized and validated schema if valid, otherwise raises an HTTP 400.

        Parameters
        ----------
        data_schema : ExtractSchemaValidateRequestDataSchema

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractSchemaValidateResponse
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.validate_extraction_schema(
            data_schema={},
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/extraction-agents/schema/validation",
            method="POST",
            json={
                "data_schema": convert_and_respect_annotation_metadata(
                    object_=data_schema, annotation=ExtractSchemaValidateRequestDataSchema, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractSchemaValidateResponse,
                    parse_obj_as(
                        type_=ExtractSchemaValidateResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_extraction_agent_by_name(
        self,
        name: str,
        *,
        project_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.get_extraction_agent_by_name(
            name="name",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/extraction-agents/by-name/{jsonable_encoder(name)}",
            method="GET",
            params={
                "project_id": project_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractAgent,
                    parse_obj_as(
                        type_=ExtractAgent,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_extraction_agent(
        self, extraction_agent_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.get_extraction_agent(
            extraction_agent_id="extraction_agent_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/extraction-agents/{jsonable_encoder(extraction_agent_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractAgent,
                    parse_obj_as(
                        type_=ExtractAgent,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_extraction_agent(
        self,
        extraction_agent_id: str,
        *,
        data_schema: ExtractAgentUpdateDataSchema,
        config: ExtractConfig,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        extraction_agent_id : str

        data_schema : ExtractAgentUpdateDataSchema
            The schema of the data

        config : ExtractConfig
            The configuration parameters for the extraction agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        from llama_index import ExtractConfig, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.update_extraction_agent(
            extraction_agent_id="extraction_agent_id",
            data_schema={},
            config=ExtractConfig(),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/extraction-agents/{jsonable_encoder(extraction_agent_id)}",
            method="PUT",
            json={
                "data_schema": convert_and_respect_annotation_metadata(
                    object_=data_schema, annotation=ExtractAgentUpdateDataSchema, direction="write"
                ),
                "config": convert_and_respect_annotation_metadata(
                    object_=config, annotation=ExtractConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractAgent,
                    parse_obj_as(
                        type_=ExtractAgent,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_extraction_agent(
        self, extraction_agent_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[typing.Any]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.delete_extraction_agent(
            extraction_agent_id="extraction_agent_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/extraction-agents/{jsonable_encoder(extraction_agent_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Optional[typing.Any],
                    parse_obj_as(
                        type_=typing.Optional[typing.Any],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_jobs(
        self, *, extraction_agent_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[ExtractJob]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractJob]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.list_jobs(
            extraction_agent_id="extraction_agent_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs",
            method="GET",
            params={
                "extraction_agent_id": extraction_agent_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[ExtractJob],
                    parse_obj_as(
                        type_=typing.List[ExtractJob],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def run_job(
        self,
        *,
        extraction_agent_id: str,
        file_id: str,
        data_schema_override: typing.Optional[ExtractJobCreateDataSchemaOverride] = OMIT,
        config_override: typing.Optional[ExtractConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file_id : str
            The id of the file

        data_schema_override : typing.Optional[ExtractJobCreateDataSchemaOverride]
            The data schema to override the extraction agent's data schema with

        config_override : typing.Optional[ExtractConfig]
            The config to override the extraction agent's config with

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.run_job(
            extraction_agent_id="extraction_agent_id",
            file_id="file_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs",
            method="POST",
            json={
                "extraction_agent_id": extraction_agent_id,
                "file_id": file_id,
                "data_schema_override": convert_and_respect_annotation_metadata(
                    object_=data_schema_override, annotation=ExtractJobCreateDataSchemaOverride, direction="write"
                ),
                "config_override": convert_and_respect_annotation_metadata(
                    object_=config_override, annotation=ExtractConfig, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractJob,
                    parse_obj_as(
                        type_=ExtractJob,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_job(self, job_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> ExtractJob:
        """
        Parameters
        ----------
        job_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.get_job(
            job_id="job_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/jobs/{jsonable_encoder(job_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractJob,
                    parse_obj_as(
                        type_=ExtractJob,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def run_job_with_parsed_file_test(
        self,
        *,
        job_create: ExtractJobCreate,
        extract_settings: typing.Optional[LlamaExtractSettings] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractResultset:
        """
        Parameters
        ----------
        job_create : ExtractJobCreate

        extract_settings : typing.Optional[LlamaExtractSettings]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractResultset
            Successful Response

        Examples
        --------
        from llama_index import ExtractJobCreate, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.run_job_with_parsed_file_test(
            job_create=ExtractJobCreate(
                extraction_agent_id="extraction_agent_id",
                file_id="file_id",
            ),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs/parsed/test",
            method="POST",
            json={
                "job_create": convert_and_respect_annotation_metadata(
                    object_=job_create, annotation=ExtractJobCreate, direction="write"
                ),
                "extract_settings": convert_and_respect_annotation_metadata(
                    object_=extract_settings, annotation=LlamaExtractSettings, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractResultset,
                    parse_obj_as(
                        type_=ExtractResultset,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def run_job_with_parsed_file(
        self,
        *,
        extraction_agent_id: str,
        file_id: str,
        data_schema_override: typing.Optional[ExtractJobCreateDataSchemaOverride] = OMIT,
        config_override: typing.Optional[ExtractConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractResultset:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file_id : str
            The id of the file

        data_schema_override : typing.Optional[ExtractJobCreateDataSchemaOverride]
            The data schema to override the extraction agent's data schema with

        config_override : typing.Optional[ExtractConfig]
            The config to override the extraction agent's config with

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractResultset
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.run_job_with_parsed_file(
            extraction_agent_id="extraction_agent_id",
            file_id="file_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs/parsed",
            method="POST",
            json={
                "extraction_agent_id": extraction_agent_id,
                "file_id": file_id,
                "data_schema_override": convert_and_respect_annotation_metadata(
                    object_=data_schema_override, annotation=ExtractJobCreateDataSchemaOverride, direction="write"
                ),
                "config_override": convert_and_respect_annotation_metadata(
                    object_=config_override, annotation=ExtractConfig, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractResultset,
                    parse_obj_as(
                        type_=ExtractResultset,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def run_job_test_user(
        self,
        *,
        job_create: ExtractJobCreate,
        extract_settings: typing.Optional[LlamaExtractSettings] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Parameters
        ----------
        job_create : ExtractJobCreate

        extract_settings : typing.Optional[LlamaExtractSettings]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        from llama_index import ExtractJobCreate, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.run_job_test_user(
            job_create=ExtractJobCreate(
                extraction_agent_id="extraction_agent_id",
                file_id="file_id",
            ),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs/test",
            method="POST",
            json={
                "job_create": convert_and_respect_annotation_metadata(
                    object_=job_create, annotation=ExtractJobCreate, direction="write"
                ),
                "extract_settings": convert_and_respect_annotation_metadata(
                    object_=extract_settings, annotation=LlamaExtractSettings, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractJob,
                    parse_obj_as(
                        type_=ExtractJob,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_job_result(
        self, job_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ExtractResultset:
        """
        Parameters
        ----------
        job_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractResultset
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.get_job_result(
            job_id="job_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/jobs/{jsonable_encoder(job_id)}/result",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractResultset,
                    parse_obj_as(
                        type_=ExtractResultset,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_extract_runs(
        self, *, extraction_agent_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[ExtractRun]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractRun]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.list_extract_runs(
            extraction_agent_id="extraction_agent_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/runs",
            method="GET",
            params={
                "extraction_agent_id": extraction_agent_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[ExtractRun],
                    parse_obj_as(
                        type_=typing.List[ExtractRun],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_run_by_job_id(self, job_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> ExtractRun:
        """
        Parameters
        ----------
        job_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractRun
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.get_run_by_job_id(
            job_id="job_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/runs/by-job/{jsonable_encoder(job_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractRun,
                    parse_obj_as(
                        type_=ExtractRun,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_run(self, run_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> ExtractRun:
        """
        Parameters
        ----------
        run_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractRun
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.llama_extract.get_run(
            run_id="run_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/runs/{jsonable_encoder(run_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractRun,
                    parse_obj_as(
                        type_=ExtractRun,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncLlamaExtractClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list_extraction_agents(
        self, *, project_id: typing.Optional[str] = None, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[ExtractAgent]:
        """
        Parameters
        ----------
        project_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractAgent]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.list_extraction_agents()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/extraction-agents",
            method="GET",
            params={
                "project_id": project_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[ExtractAgent],
                    parse_obj_as(
                        type_=typing.List[ExtractAgent],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_extraction_agent(
        self,
        *,
        name: str,
        data_schema: ExtractAgentCreateDataSchema,
        config: ExtractConfig,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        name : str
            The name of the extraction schema

        data_schema : ExtractAgentCreateDataSchema
            The schema of the data.

        config : ExtractConfig
            The configuration parameters for the extraction agent.

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, ExtractConfig

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.create_extraction_agent(
                name="name",
                data_schema={},
                config=ExtractConfig(),
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/extraction-agents",
            method="POST",
            params={
                "project_id": project_id,
                "organization_id": organization_id,
            },
            json={
                "name": name,
                "data_schema": convert_and_respect_annotation_metadata(
                    object_=data_schema, annotation=ExtractAgentCreateDataSchema, direction="write"
                ),
                "config": convert_and_respect_annotation_metadata(
                    object_=config, annotation=ExtractConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractAgent,
                    parse_obj_as(
                        type_=ExtractAgent,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def validate_extraction_schema(
        self,
        *,
        data_schema: ExtractSchemaValidateRequestDataSchema,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractSchemaValidateResponse:
        """
        Validates an extraction agent's schema definition.
        Returns the normalized and validated schema if valid, otherwise raises an HTTP 400.

        Parameters
        ----------
        data_schema : ExtractSchemaValidateRequestDataSchema

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractSchemaValidateResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.validate_extraction_schema(
                data_schema={},
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/extraction-agents/schema/validation",
            method="POST",
            json={
                "data_schema": convert_and_respect_annotation_metadata(
                    object_=data_schema, annotation=ExtractSchemaValidateRequestDataSchema, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractSchemaValidateResponse,
                    parse_obj_as(
                        type_=ExtractSchemaValidateResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_extraction_agent_by_name(
        self,
        name: str,
        *,
        project_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.get_extraction_agent_by_name(
                name="name",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/extraction-agents/by-name/{jsonable_encoder(name)}",
            method="GET",
            params={
                "project_id": project_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractAgent,
                    parse_obj_as(
                        type_=ExtractAgent,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_extraction_agent(
        self, extraction_agent_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.get_extraction_agent(
                extraction_agent_id="extraction_agent_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/extraction-agents/{jsonable_encoder(extraction_agent_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractAgent,
                    parse_obj_as(
                        type_=ExtractAgent,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_extraction_agent(
        self,
        extraction_agent_id: str,
        *,
        data_schema: ExtractAgentUpdateDataSchema,
        config: ExtractConfig,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        extraction_agent_id : str

        data_schema : ExtractAgentUpdateDataSchema
            The schema of the data

        config : ExtractConfig
            The configuration parameters for the extraction agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, ExtractConfig

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.update_extraction_agent(
                extraction_agent_id="extraction_agent_id",
                data_schema={},
                config=ExtractConfig(),
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/extraction-agents/{jsonable_encoder(extraction_agent_id)}",
            method="PUT",
            json={
                "data_schema": convert_and_respect_annotation_metadata(
                    object_=data_schema, annotation=ExtractAgentUpdateDataSchema, direction="write"
                ),
                "config": convert_and_respect_annotation_metadata(
                    object_=config, annotation=ExtractConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractAgent,
                    parse_obj_as(
                        type_=ExtractAgent,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_extraction_agent(
        self, extraction_agent_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[typing.Any]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.delete_extraction_agent(
                extraction_agent_id="extraction_agent_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/extraction-agents/{jsonable_encoder(extraction_agent_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Optional[typing.Any],
                    parse_obj_as(
                        type_=typing.Optional[typing.Any],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_jobs(
        self, *, extraction_agent_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[ExtractJob]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractJob]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.list_jobs(
                extraction_agent_id="extraction_agent_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs",
            method="GET",
            params={
                "extraction_agent_id": extraction_agent_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[ExtractJob],
                    parse_obj_as(
                        type_=typing.List[ExtractJob],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def run_job(
        self,
        *,
        extraction_agent_id: str,
        file_id: str,
        data_schema_override: typing.Optional[ExtractJobCreateDataSchemaOverride] = OMIT,
        config_override: typing.Optional[ExtractConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file_id : str
            The id of the file

        data_schema_override : typing.Optional[ExtractJobCreateDataSchemaOverride]
            The data schema to override the extraction agent's data schema with

        config_override : typing.Optional[ExtractConfig]
            The config to override the extraction agent's config with

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.run_job(
                extraction_agent_id="extraction_agent_id",
                file_id="file_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs",
            method="POST",
            json={
                "extraction_agent_id": extraction_agent_id,
                "file_id": file_id,
                "data_schema_override": convert_and_respect_annotation_metadata(
                    object_=data_schema_override, annotation=ExtractJobCreateDataSchemaOverride, direction="write"
                ),
                "config_override": convert_and_respect_annotation_metadata(
                    object_=config_override, annotation=ExtractConfig, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractJob,
                    parse_obj_as(
                        type_=ExtractJob,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_job(self, job_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> ExtractJob:
        """
        Parameters
        ----------
        job_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.get_job(
                job_id="job_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/jobs/{jsonable_encoder(job_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractJob,
                    parse_obj_as(
                        type_=ExtractJob,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def run_job_with_parsed_file_test(
        self,
        *,
        job_create: ExtractJobCreate,
        extract_settings: typing.Optional[LlamaExtractSettings] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractResultset:
        """
        Parameters
        ----------
        job_create : ExtractJobCreate

        extract_settings : typing.Optional[LlamaExtractSettings]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractResultset
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, ExtractJobCreate

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.run_job_with_parsed_file_test(
                job_create=ExtractJobCreate(
                    extraction_agent_id="extraction_agent_id",
                    file_id="file_id",
                ),
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs/parsed/test",
            method="POST",
            json={
                "job_create": convert_and_respect_annotation_metadata(
                    object_=job_create, annotation=ExtractJobCreate, direction="write"
                ),
                "extract_settings": convert_and_respect_annotation_metadata(
                    object_=extract_settings, annotation=LlamaExtractSettings, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractResultset,
                    parse_obj_as(
                        type_=ExtractResultset,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def run_job_with_parsed_file(
        self,
        *,
        extraction_agent_id: str,
        file_id: str,
        data_schema_override: typing.Optional[ExtractJobCreateDataSchemaOverride] = OMIT,
        config_override: typing.Optional[ExtractConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractResultset:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file_id : str
            The id of the file

        data_schema_override : typing.Optional[ExtractJobCreateDataSchemaOverride]
            The data schema to override the extraction agent's data schema with

        config_override : typing.Optional[ExtractConfig]
            The config to override the extraction agent's config with

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractResultset
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.run_job_with_parsed_file(
                extraction_agent_id="extraction_agent_id",
                file_id="file_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs/parsed",
            method="POST",
            json={
                "extraction_agent_id": extraction_agent_id,
                "file_id": file_id,
                "data_schema_override": convert_and_respect_annotation_metadata(
                    object_=data_schema_override, annotation=ExtractJobCreateDataSchemaOverride, direction="write"
                ),
                "config_override": convert_and_respect_annotation_metadata(
                    object_=config_override, annotation=ExtractConfig, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractResultset,
                    parse_obj_as(
                        type_=ExtractResultset,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def run_job_test_user(
        self,
        *,
        job_create: ExtractJobCreate,
        extract_settings: typing.Optional[LlamaExtractSettings] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Parameters
        ----------
        job_create : ExtractJobCreate

        extract_settings : typing.Optional[LlamaExtractSettings]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, ExtractJobCreate

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.run_job_test_user(
                job_create=ExtractJobCreate(
                    extraction_agent_id="extraction_agent_id",
                    file_id="file_id",
                ),
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/jobs/test",
            method="POST",
            json={
                "job_create": convert_and_respect_annotation_metadata(
                    object_=job_create, annotation=ExtractJobCreate, direction="write"
                ),
                "extract_settings": convert_and_respect_annotation_metadata(
                    object_=extract_settings, annotation=LlamaExtractSettings, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractJob,
                    parse_obj_as(
                        type_=ExtractJob,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_job_result(
        self, job_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ExtractResultset:
        """
        Parameters
        ----------
        job_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractResultset
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.get_job_result(
                job_id="job_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/jobs/{jsonable_encoder(job_id)}/result",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractResultset,
                    parse_obj_as(
                        type_=ExtractResultset,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_extract_runs(
        self, *, extraction_agent_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[ExtractRun]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractRun]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.list_extract_runs(
                extraction_agent_id="extraction_agent_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/extractionv2/runs",
            method="GET",
            params={
                "extraction_agent_id": extraction_agent_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[ExtractRun],
                    parse_obj_as(
                        type_=typing.List[ExtractRun],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_run_by_job_id(
        self, job_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ExtractRun:
        """
        Parameters
        ----------
        job_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractRun
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.get_run_by_job_id(
                job_id="job_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/runs/by-job/{jsonable_encoder(job_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractRun,
                    parse_obj_as(
                        type_=ExtractRun,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_run(self, run_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> ExtractRun:
        """
        Parameters
        ----------
        run_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractRun
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.llama_extract.get_run(
                run_id="run_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/extractionv2/runs/{jsonable_encoder(run_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ExtractRun,
                    parse_obj_as(
                        type_=ExtractRun,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
