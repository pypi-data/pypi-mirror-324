# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from ..types.pipeline_type import PipelineType
from ..core.request_options import RequestOptions
from ..types.pipeline import Pipeline
from ..core.pydantic_utilities import parse_obj_as
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from ..types.pipeline_create_embedding_config import PipelineCreateEmbeddingConfig
from ..types.pipeline_create_transform_config import PipelineCreateTransformConfig
from ..types.configured_transformation_item import ConfiguredTransformationItem
from ..types.data_sink_create import DataSinkCreate
from ..types.preset_retrieval_params import PresetRetrievalParams
from ..types.eval_execution_params import EvalExecutionParams
from ..types.llama_parse_parameters import LlamaParseParameters
from ..core.serialization import convert_and_respect_annotation_metadata
from ..core.jsonable_encoder import jsonable_encoder
from .types.pipeline_update_embedding_config import PipelineUpdateEmbeddingConfig
from .types.pipeline_update_transform_config import PipelineUpdateTransformConfig
from ..types.managed_ingestion_status_response import ManagedIngestionStatusResponse
from ..types.eval_dataset_job_record import EvalDatasetJobRecord
from ..types.eval_execution_params_override import EvalExecutionParamsOverride
from ..types.eval_question_result import EvalQuestionResult
from ..types.pipeline_file import PipelineFile
from ..types.pipeline_file_create import PipelineFileCreate
from ..types.paginated_list_pipeline_files_response import PaginatedListPipelineFilesResponse
from .types.pipeline_file_update_custom_metadata_value import PipelineFileUpdateCustomMetadataValue
from .. import core
from ..types.pipeline_data_source import PipelineDataSource
from ..types.pipeline_data_source_create import PipelineDataSourceCreate
from ..types.metadata_filters import MetadataFilters
from ..types.retrieval_mode import RetrievalMode
from ..types.retrieve_results import RetrieveResults
from ..types.pipeline_deployment import PipelineDeployment
from ..types.playground_session import PlaygroundSession
from ..types.input_message import InputMessage
from ..types.chat_data import ChatData
from ..types.cloud_document import CloudDocument
from ..types.cloud_document_create import CloudDocumentCreate
from ..types.text_node import TextNode
from ..core.client_wrapper import AsyncClientWrapper

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class PipelinesClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def search_pipelines(
        self,
        *,
        project_id: typing.Optional[str] = None,
        project_name: typing.Optional[str] = None,
        pipeline_name: typing.Optional[str] = None,
        pipeline_type: typing.Optional[PipelineType] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[Pipeline]:
        """
        Search for pipelines by various parameters.

        Parameters
        ----------
        project_id : typing.Optional[str]

        project_name : typing.Optional[str]

        pipeline_name : typing.Optional[str]

        pipeline_type : typing.Optional[PipelineType]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[Pipeline]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.search_pipelines()
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/pipelines",
            method="GET",
            params={
                "project_id": project_id,
                "project_name": project_name,
                "pipeline_name": pipeline_name,
                "pipeline_type": pipeline_type,
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[Pipeline],
                    parse_obj_as(
                        type_=typing.List[Pipeline],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_pipeline(
        self,
        *,
        name: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        embedding_config: typing.Optional[PipelineCreateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineCreateTransformConfig] = OMIT,
        configured_transformations: typing.Optional[typing.Sequence[ConfiguredTransformationItem]] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        eval_parameters: typing.Optional[EvalExecutionParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        pipeline_type: typing.Optional[PipelineType] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Create a new pipeline for a project.

        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        embedding_config : typing.Optional[PipelineCreateEmbeddingConfig]

        transform_config : typing.Optional[PipelineCreateTransformConfig]
            Configuration for the transformation.

        configured_transformations : typing.Optional[typing.Sequence[ConfiguredTransformationItem]]
            Deprecated, use embedding_config or transform_config instead. configured transformations for the pipeline.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        eval_parameters : typing.Optional[EvalExecutionParams]
            Eval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]
            Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

        pipeline_type : typing.Optional[PipelineType]
            Type of pipeline. Either PLAYGROUND or MANAGED.

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.create_pipeline(
            name="name",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/pipelines",
            method="POST",
            params={
                "project_id": project_id,
                "organization_id": organization_id,
            },
            json={
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=PipelineCreateEmbeddingConfig, direction="write"
                ),
                "transform_config": convert_and_respect_annotation_metadata(
                    object_=transform_config, annotation=PipelineCreateTransformConfig, direction="write"
                ),
                "configured_transformations": convert_and_respect_annotation_metadata(
                    object_=configured_transformations,
                    annotation=typing.Sequence[ConfiguredTransformationItem],
                    direction="write",
                ),
                "data_sink_id": data_sink_id,
                "embedding_model_config_id": embedding_model_config_id,
                "data_sink": convert_and_respect_annotation_metadata(
                    object_=data_sink, annotation=DataSinkCreate, direction="write"
                ),
                "preset_retrieval_parameters": convert_and_respect_annotation_metadata(
                    object_=preset_retrieval_parameters, annotation=PresetRetrievalParams, direction="write"
                ),
                "eval_parameters": convert_and_respect_annotation_metadata(
                    object_=eval_parameters, annotation=EvalExecutionParams, direction="write"
                ),
                "llama_parse_parameters": convert_and_respect_annotation_metadata(
                    object_=llama_parse_parameters, annotation=LlamaParseParameters, direction="write"
                ),
                "name": name,
                "pipeline_type": pipeline_type,
                "managed_pipeline_id": managed_pipeline_id,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert_pipeline(
        self,
        *,
        name: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        embedding_config: typing.Optional[PipelineCreateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineCreateTransformConfig] = OMIT,
        configured_transformations: typing.Optional[typing.Sequence[ConfiguredTransformationItem]] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        eval_parameters: typing.Optional[EvalExecutionParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        pipeline_type: typing.Optional[PipelineType] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Upsert a pipeline for a project.
        Updates if a pipeline with the same name and project_id already exists. Otherwise, creates a new pipeline.

        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        embedding_config : typing.Optional[PipelineCreateEmbeddingConfig]

        transform_config : typing.Optional[PipelineCreateTransformConfig]
            Configuration for the transformation.

        configured_transformations : typing.Optional[typing.Sequence[ConfiguredTransformationItem]]
            Deprecated, use embedding_config or transform_config instead. configured transformations for the pipeline.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        eval_parameters : typing.Optional[EvalExecutionParams]
            Eval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]
            Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

        pipeline_type : typing.Optional[PipelineType]
            Type of pipeline. Either PLAYGROUND or MANAGED.

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.upsert_pipeline(
            name="name",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/pipelines",
            method="PUT",
            params={
                "project_id": project_id,
                "organization_id": organization_id,
            },
            json={
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=PipelineCreateEmbeddingConfig, direction="write"
                ),
                "transform_config": convert_and_respect_annotation_metadata(
                    object_=transform_config, annotation=PipelineCreateTransformConfig, direction="write"
                ),
                "configured_transformations": convert_and_respect_annotation_metadata(
                    object_=configured_transformations,
                    annotation=typing.Sequence[ConfiguredTransformationItem],
                    direction="write",
                ),
                "data_sink_id": data_sink_id,
                "embedding_model_config_id": embedding_model_config_id,
                "data_sink": convert_and_respect_annotation_metadata(
                    object_=data_sink, annotation=DataSinkCreate, direction="write"
                ),
                "preset_retrieval_parameters": convert_and_respect_annotation_metadata(
                    object_=preset_retrieval_parameters, annotation=PresetRetrievalParams, direction="write"
                ),
                "eval_parameters": convert_and_respect_annotation_metadata(
                    object_=eval_parameters, annotation=EvalExecutionParams, direction="write"
                ),
                "llama_parse_parameters": convert_and_respect_annotation_metadata(
                    object_=llama_parse_parameters, annotation=LlamaParseParameters, direction="write"
                ),
                "name": name,
                "pipeline_type": pipeline_type,
                "managed_pipeline_id": managed_pipeline_id,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Get a pipeline by ID for a given project.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_existing_pipeline(
        self,
        pipeline_id: str,
        *,
        embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineUpdateTransformConfig] = OMIT,
        configured_transformations: typing.Optional[typing.Sequence[ConfiguredTransformationItem]] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        eval_parameters: typing.Optional[EvalExecutionParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        name: typing.Optional[str] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Update an existing pipeline for a project.

        Parameters
        ----------
        pipeline_id : str

        embedding_config : typing.Optional[PipelineUpdateEmbeddingConfig]

        transform_config : typing.Optional[PipelineUpdateTransformConfig]
            Configuration for the transformation.

        configured_transformations : typing.Optional[typing.Sequence[ConfiguredTransformationItem]]
            Deprecated, use embedding_config or transform_config instead. configured transformations for the pipeline.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        eval_parameters : typing.Optional[EvalExecutionParams]
            Eval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]
            Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

        name : typing.Optional[str]

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.update_existing_pipeline(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}",
            method="PUT",
            json={
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=PipelineUpdateEmbeddingConfig, direction="write"
                ),
                "transform_config": convert_and_respect_annotation_metadata(
                    object_=transform_config, annotation=PipelineUpdateTransformConfig, direction="write"
                ),
                "configured_transformations": convert_and_respect_annotation_metadata(
                    object_=configured_transformations,
                    annotation=typing.Sequence[ConfiguredTransformationItem],
                    direction="write",
                ),
                "data_sink_id": data_sink_id,
                "embedding_model_config_id": embedding_model_config_id,
                "data_sink": convert_and_respect_annotation_metadata(
                    object_=data_sink, annotation=DataSinkCreate, direction="write"
                ),
                "preset_retrieval_parameters": convert_and_respect_annotation_metadata(
                    object_=preset_retrieval_parameters, annotation=PresetRetrievalParams, direction="write"
                ),
                "eval_parameters": convert_and_respect_annotation_metadata(
                    object_=eval_parameters, annotation=EvalExecutionParams, direction="write"
                ),
                "llama_parse_parameters": convert_and_respect_annotation_metadata(
                    object_=llama_parse_parameters, annotation=LlamaParseParameters, direction="write"
                ),
                "name": name,
                "managed_pipeline_id": managed_pipeline_id,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_status(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get the status of a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_status(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/status",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ManagedIngestionStatusResponse,
                    parse_obj_as(
                        type_=ManagedIngestionStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def sync_pipeline(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Run ingestion for the pipeline by incrementally updating the data-sink with upstream changes from data-sources & files.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.sync_pipeline(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/sync",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def cancel_pipeline_sync(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> Pipeline:
        """
        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.cancel_pipeline_sync(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/sync/cancel",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def copy_pipeline(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Copy a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.copy_pipeline(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/copy",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_eval_dataset_executions(
        self, eval_dataset_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[EvalDatasetJobRecord]:
        """
        Get the status of an EvalDatasetExecution.

        Parameters
        ----------
        eval_dataset_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[EvalDatasetJobRecord]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_eval_dataset_executions(
            eval_dataset_id="eval_dataset_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/eval-datasets/{jsonable_encoder(eval_dataset_id)}/execute",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[EvalDatasetJobRecord],
                    parse_obj_as(
                        type_=typing.List[EvalDatasetJobRecord],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def execute_eval_dataset(
        self,
        eval_dataset_id: str,
        pipeline_id: str,
        *,
        eval_question_ids: typing.Sequence[str],
        params: typing.Optional[EvalExecutionParamsOverride] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvalDatasetJobRecord:
        """
        Execute a dataset.

        Parameters
        ----------
        eval_dataset_id : str

        pipeline_id : str

        eval_question_ids : typing.Sequence[str]

        params : typing.Optional[EvalExecutionParamsOverride]
            The parameters for the eval execution that will override the ones set in the pipeline.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvalDatasetJobRecord
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.execute_eval_dataset(
            eval_dataset_id="eval_dataset_id",
            pipeline_id="pipeline_id",
            eval_question_ids=["eval_question_ids"],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/eval-datasets/{jsonable_encoder(eval_dataset_id)}/execute",
            method="POST",
            json={
                "eval_question_ids": eval_question_ids,
                "params": convert_and_respect_annotation_metadata(
                    object_=params, annotation=EvalExecutionParamsOverride, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvalDatasetJobRecord,
                    parse_obj_as(
                        type_=EvalDatasetJobRecord,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_eval_dataset_execution_result(
        self, eval_dataset_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[EvalQuestionResult]:
        """
        Get the result of an EvalDatasetExecution.
        If eval_question_ids is specified, only the results for the specified
        questions will be returned.
        If any of the specified questions do not have a result, they will be ignored.

        Parameters
        ----------
        eval_dataset_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[EvalQuestionResult]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_eval_dataset_execution_result(
            eval_dataset_id="eval_dataset_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/eval-datasets/{jsonable_encoder(eval_dataset_id)}/execute/result",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[EvalQuestionResult],
                    parse_obj_as(
                        type_=typing.List[EvalQuestionResult],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_eval_dataset_execution(
        self,
        eval_dataset_id: str,
        eval_dataset_execution_id: str,
        pipeline_id: str,
        *,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvalDatasetJobRecord:
        """
        Get the status of an EvalDatasetExecution.

        Parameters
        ----------
        eval_dataset_id : str

        eval_dataset_execution_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvalDatasetJobRecord
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_eval_dataset_execution(
            eval_dataset_id="eval_dataset_id",
            eval_dataset_execution_id="eval_dataset_execution_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/eval-datasets/{jsonable_encoder(eval_dataset_id)}/execute/{jsonable_encoder(eval_dataset_execution_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvalDatasetJobRecord,
                    parse_obj_as(
                        type_=EvalDatasetJobRecord,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_files(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineFile]:
        """
        Get files for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineFile]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_files(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files",
            method="GET",
            params={
                "data_source_id": data_source_id,
                "only_manually_uploaded": only_manually_uploaded,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineFile],
                    parse_obj_as(
                        type_=typing.List[PipelineFile],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def add_files_to_pipeline(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[PipelineFileCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineFile]:
        """
        Add files to a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[PipelineFileCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineFile]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud, PipelineFileCreate

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.add_files_to_pipeline(
            pipeline_id="pipeline_id",
            request=[
                PipelineFileCreate(
                    file_id="file_id",
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files",
            method="PUT",
            json=convert_and_respect_annotation_metadata(
                object_=request, annotation=typing.Sequence[PipelineFileCreate], direction="write"
            ),
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineFile],
                    parse_obj_as(
                        type_=typing.List[PipelineFile],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_files_2(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        limit: typing.Optional[int] = None,
        offset: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedListPipelineFilesResponse:
        """
        Get files for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        limit : typing.Optional[int]

        offset : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedListPipelineFilesResponse
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_files_2(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files2",
            method="GET",
            params={
                "data_source_id": data_source_id,
                "only_manually_uploaded": only_manually_uploaded,
                "limit": limit,
                "offset": offset,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PaginatedListPipelineFilesResponse,
                    parse_obj_as(
                        type_=PaginatedListPipelineFilesResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_file_status(
        self, file_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get status of a file for a pipeline.

        Parameters
        ----------
        file_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_file_status(
            file_id="file_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files/{jsonable_encoder(file_id)}/status",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ManagedIngestionStatusResponse,
                    parse_obj_as(
                        type_=ManagedIngestionStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_pipeline_file(
        self,
        file_id: str,
        pipeline_id: str,
        *,
        custom_metadata: typing.Optional[
            typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]]
        ] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PipelineFile:
        """
        Update a file for a pipeline.

        Parameters
        ----------
        file_id : str

        pipeline_id : str

        custom_metadata : typing.Optional[typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]]]
            Custom metadata for the file

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PipelineFile
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.update_pipeline_file(
            file_id="file_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files/{jsonable_encoder(file_id)}",
            method="PUT",
            json={
                "custom_metadata": convert_and_respect_annotation_metadata(
                    object_=custom_metadata,
                    annotation=typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]],
                    direction="write",
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PipelineFile,
                    parse_obj_as(
                        type_=PipelineFile,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline_file(
        self, file_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a file from a pipeline.

        Parameters
        ----------
        file_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline_file(
            file_id="file_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files/{jsonable_encoder(file_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def import_pipeline_metadata(
        self, pipeline_id: str, *, upload_file: core.File, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Dict[str, str]:
        """
        Import metadata for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        upload_file : core.File
            See core.File for more documentation

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Dict[str, str]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.import_pipeline_metadata(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/metadata",
            method="PUT",
            data={},
            files={
                "upload_file": upload_file,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Dict[str, str],
                    parse_obj_as(
                        type_=typing.Dict[str, str],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline_files_metadata(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete metadata for all files in a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline_files_metadata(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/metadata",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_data_sources(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[PipelineDataSource]:
        """
        Get data sources for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineDataSource]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_data_sources(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineDataSource],
                    parse_obj_as(
                        type_=typing.List[PipelineDataSource],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def add_data_sources_to_pipeline(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[PipelineDataSourceCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineDataSource]:
        """
        Add data sources to a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[PipelineDataSourceCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineDataSource]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud, PipelineDataSourceCreate

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.add_data_sources_to_pipeline(
            pipeline_id="pipeline_id",
            request=[
                PipelineDataSourceCreate(
                    data_source_id="data_source_id",
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources",
            method="PUT",
            json=convert_and_respect_annotation_metadata(
                object_=request, annotation=typing.Sequence[PipelineDataSourceCreate], direction="write"
            ),
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineDataSource],
                    parse_obj_as(
                        type_=typing.List[PipelineDataSource],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_pipeline_data_source(
        self,
        data_source_id: str,
        pipeline_id: str,
        *,
        sync_interval: typing.Optional[float] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PipelineDataSource:
        """
        Update the configuration of a data source in a pipeline.

        Parameters
        ----------
        data_source_id : str

        pipeline_id : str

        sync_interval : typing.Optional[float]
            The interval at which the data source should be synced.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PipelineDataSource
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.update_pipeline_data_source(
            data_source_id="data_source_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources/{jsonable_encoder(data_source_id)}",
            method="PUT",
            json={
                "sync_interval": sync_interval,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PipelineDataSource,
                    parse_obj_as(
                        type_=PipelineDataSource,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline_data_source(
        self, data_source_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a data source from a pipeline.

        Parameters
        ----------
        data_source_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline_data_source(
            data_source_id="data_source_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources/{jsonable_encoder(data_source_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def sync_pipeline_data_source(
        self, data_source_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> Pipeline:
        """
        Run ingestion for the pipeline data source by incrementally updating the data-sink with upstream changes from data-source.

        Parameters
        ----------
        data_source_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.sync_pipeline_data_source(
            data_source_id="data_source_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources/{jsonable_encoder(data_source_id)}/sync",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_data_source_status(
        self, data_source_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get the status of a data source for a pipeline.

        Parameters
        ----------
        data_source_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_data_source_status(
            data_source_id="data_source_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources/{jsonable_encoder(data_source_id)}/status",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ManagedIngestionStatusResponse,
                    parse_obj_as(
                        type_=ManagedIngestionStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def run_search(
        self,
        pipeline_id: str,
        *,
        query: str,
        dense_similarity_top_k: typing.Optional[int] = OMIT,
        dense_similarity_cutoff: typing.Optional[float] = OMIT,
        sparse_similarity_top_k: typing.Optional[int] = OMIT,
        enable_reranking: typing.Optional[bool] = OMIT,
        rerank_top_n: typing.Optional[int] = OMIT,
        alpha: typing.Optional[float] = OMIT,
        search_filters: typing.Optional[MetadataFilters] = OMIT,
        files_top_k: typing.Optional[int] = OMIT,
        retrieval_mode: typing.Optional[RetrievalMode] = OMIT,
        retrieve_image_nodes: typing.Optional[bool] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RetrieveResults:
        """
        Get retrieval results for a managed pipeline and a query

        Parameters
        ----------
        pipeline_id : str

        query : str
            The query to retrieve against.

        dense_similarity_top_k : typing.Optional[int]
            Number of nodes for dense retrieval.

        dense_similarity_cutoff : typing.Optional[float]
            Minimum similarity score wrt query for retrieval

        sparse_similarity_top_k : typing.Optional[int]
            Number of nodes for sparse retrieval.

        enable_reranking : typing.Optional[bool]
            Enable reranking for retrieval

        rerank_top_n : typing.Optional[int]
            Number of reranked nodes for returning.

        alpha : typing.Optional[float]
            Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval.

        search_filters : typing.Optional[MetadataFilters]
            Search filters for retrieval.

        files_top_k : typing.Optional[int]
            Number of files to retrieve (only for retrieval mode files_via_metadata and files_via_content).

        retrieval_mode : typing.Optional[RetrievalMode]
            The retrieval mode for the query.

        retrieve_image_nodes : typing.Optional[bool]
            Whether to retrieve image nodes.

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RetrieveResults
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.run_search(
            pipeline_id="pipeline_id",
            query="query",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/retrieve",
            method="POST",
            json={
                "dense_similarity_top_k": dense_similarity_top_k,
                "dense_similarity_cutoff": dense_similarity_cutoff,
                "sparse_similarity_top_k": sparse_similarity_top_k,
                "enable_reranking": enable_reranking,
                "rerank_top_n": rerank_top_n,
                "alpha": alpha,
                "search_filters": convert_and_respect_annotation_metadata(
                    object_=search_filters, annotation=MetadataFilters, direction="write"
                ),
                "files_top_k": files_top_k,
                "retrieval_mode": retrieval_mode,
                "retrieve_image_nodes": retrieve_image_nodes,
                "query": query,
                "class_name": class_name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    RetrieveResults,
                    parse_obj_as(
                        type_=RetrieveResults,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_jobs(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[PipelineDeployment]:
        """
        Get jobs for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineDeployment]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_jobs(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/jobs",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineDeployment],
                    parse_obj_as(
                        type_=typing.List[PipelineDeployment],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_job(
        self, job_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> PipelineDeployment:
        """
        Get a job for a pipeline.

        Parameters
        ----------
        job_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PipelineDeployment
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_job(
            job_id="job_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/jobs/{jsonable_encoder(job_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PipelineDeployment,
                    parse_obj_as(
                        type_=PipelineDeployment,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_playground_session(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> PlaygroundSession:
        """
        Get a playground session for a user and pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PlaygroundSession
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_playground_session(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/playground-session",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PlaygroundSession,
                    parse_obj_as(
                        type_=PlaygroundSession,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def chat(
        self,
        pipeline_id: str,
        *,
        messages: typing.Optional[typing.Sequence[InputMessage]] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters
        ----------
        pipeline_id : str

        messages : typing.Optional[typing.Sequence[InputMessage]]

        data : typing.Optional[ChatData]

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.chat(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/chat",
            method="POST",
            json={
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[InputMessage], direction="write"
                ),
                "data": convert_and_respect_annotation_metadata(object_=data, annotation=ChatData, direction="write"),
                "class_name": class_name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Optional[typing.Any],
                    parse_obj_as(
                        type_=typing.Optional[typing.Any],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Return a list of documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        skip : typing.Optional[int]

        limit : typing.Optional[int]

        file_id : typing.Optional[str]

        only_direct_upload : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_documents(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents",
            method="GET",
            params={
                "skip": skip,
                "limit": limit,
                "file_id": file_id,
                "only_direct_upload": only_direct_upload,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[CloudDocument],
                    parse_obj_as(
                        type_=typing.List[CloudDocument],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_batch_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[CloudDocumentCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Batch create documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[CloudDocumentCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        from llama_index import CloudDocumentCreate, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.create_batch_pipeline_documents(
            pipeline_id="pipeline_id",
            request=[
                CloudDocumentCreate(
                    text="text",
                    metadata={"key": "value"},
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents",
            method="POST",
            json=convert_and_respect_annotation_metadata(
                object_=request, annotation=typing.Sequence[CloudDocumentCreate], direction="write"
            ),
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[CloudDocument],
                    parse_obj_as(
                        type_=typing.List[CloudDocument],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert_batch_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[CloudDocumentCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Batch create or update a document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[CloudDocumentCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        from llama_index import CloudDocumentCreate, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.upsert_batch_pipeline_documents(
            pipeline_id="pipeline_id",
            request=[
                CloudDocumentCreate(
                    text="text",
                    metadata={"key": "value"},
                )
            ],
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents",
            method="PUT",
            json=convert_and_respect_annotation_metadata(
                object_=request, annotation=typing.Sequence[CloudDocumentCreate], direction="write"
            ),
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[CloudDocument],
                    parse_obj_as(
                        type_=typing.List[CloudDocument],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_document(
        self, document_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> CloudDocument:
        """
        Return a single document for a pipeline.

        Parameters
        ----------
        document_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CloudDocument
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_document(
            document_id="document_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents/{jsonable_encoder(document_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    CloudDocument,
                    parse_obj_as(
                        type_=CloudDocument,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_pipeline_document(
        self, document_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a document for a pipeline.

        Parameters
        ----------
        document_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.delete_pipeline_document(
            document_id="document_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents/{jsonable_encoder(document_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_pipeline_document_status(
        self, document_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Return a single document for a pipeline.

        Parameters
        ----------
        document_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.get_pipeline_document_status(
            document_id="document_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents/{jsonable_encoder(document_id)}/status",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ManagedIngestionStatusResponse,
                    parse_obj_as(
                        type_=ManagedIngestionStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_pipeline_document_chunks(
        self, document_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[TextNode]:
        """
        Return a list of chunks for a pipeline document.

        Parameters
        ----------
        document_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[TextNode]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.pipelines.list_pipeline_document_chunks(
            document_id="document_id",
            pipeline_id="pipeline_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents/{jsonable_encoder(document_id)}/chunks",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[TextNode],
                    parse_obj_as(
                        type_=typing.List[TextNode],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncPipelinesClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def search_pipelines(
        self,
        *,
        project_id: typing.Optional[str] = None,
        project_name: typing.Optional[str] = None,
        pipeline_name: typing.Optional[str] = None,
        pipeline_type: typing.Optional[PipelineType] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[Pipeline]:
        """
        Search for pipelines by various parameters.

        Parameters
        ----------
        project_id : typing.Optional[str]

        project_name : typing.Optional[str]

        pipeline_name : typing.Optional[str]

        pipeline_type : typing.Optional[PipelineType]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[Pipeline]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.search_pipelines()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/pipelines",
            method="GET",
            params={
                "project_id": project_id,
                "project_name": project_name,
                "pipeline_name": pipeline_name,
                "pipeline_type": pipeline_type,
                "organization_id": organization_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[Pipeline],
                    parse_obj_as(
                        type_=typing.List[Pipeline],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_pipeline(
        self,
        *,
        name: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        embedding_config: typing.Optional[PipelineCreateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineCreateTransformConfig] = OMIT,
        configured_transformations: typing.Optional[typing.Sequence[ConfiguredTransformationItem]] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        eval_parameters: typing.Optional[EvalExecutionParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        pipeline_type: typing.Optional[PipelineType] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Create a new pipeline for a project.

        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        embedding_config : typing.Optional[PipelineCreateEmbeddingConfig]

        transform_config : typing.Optional[PipelineCreateTransformConfig]
            Configuration for the transformation.

        configured_transformations : typing.Optional[typing.Sequence[ConfiguredTransformationItem]]
            Deprecated, use embedding_config or transform_config instead. configured transformations for the pipeline.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        eval_parameters : typing.Optional[EvalExecutionParams]
            Eval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]
            Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

        pipeline_type : typing.Optional[PipelineType]
            Type of pipeline. Either PLAYGROUND or MANAGED.

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.create_pipeline(
                name="name",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/pipelines",
            method="POST",
            params={
                "project_id": project_id,
                "organization_id": organization_id,
            },
            json={
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=PipelineCreateEmbeddingConfig, direction="write"
                ),
                "transform_config": convert_and_respect_annotation_metadata(
                    object_=transform_config, annotation=PipelineCreateTransformConfig, direction="write"
                ),
                "configured_transformations": convert_and_respect_annotation_metadata(
                    object_=configured_transformations,
                    annotation=typing.Sequence[ConfiguredTransformationItem],
                    direction="write",
                ),
                "data_sink_id": data_sink_id,
                "embedding_model_config_id": embedding_model_config_id,
                "data_sink": convert_and_respect_annotation_metadata(
                    object_=data_sink, annotation=DataSinkCreate, direction="write"
                ),
                "preset_retrieval_parameters": convert_and_respect_annotation_metadata(
                    object_=preset_retrieval_parameters, annotation=PresetRetrievalParams, direction="write"
                ),
                "eval_parameters": convert_and_respect_annotation_metadata(
                    object_=eval_parameters, annotation=EvalExecutionParams, direction="write"
                ),
                "llama_parse_parameters": convert_and_respect_annotation_metadata(
                    object_=llama_parse_parameters, annotation=LlamaParseParameters, direction="write"
                ),
                "name": name,
                "pipeline_type": pipeline_type,
                "managed_pipeline_id": managed_pipeline_id,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert_pipeline(
        self,
        *,
        name: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        embedding_config: typing.Optional[PipelineCreateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineCreateTransformConfig] = OMIT,
        configured_transformations: typing.Optional[typing.Sequence[ConfiguredTransformationItem]] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        eval_parameters: typing.Optional[EvalExecutionParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        pipeline_type: typing.Optional[PipelineType] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Upsert a pipeline for a project.
        Updates if a pipeline with the same name and project_id already exists. Otherwise, creates a new pipeline.

        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        embedding_config : typing.Optional[PipelineCreateEmbeddingConfig]

        transform_config : typing.Optional[PipelineCreateTransformConfig]
            Configuration for the transformation.

        configured_transformations : typing.Optional[typing.Sequence[ConfiguredTransformationItem]]
            Deprecated, use embedding_config or transform_config instead. configured transformations for the pipeline.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        eval_parameters : typing.Optional[EvalExecutionParams]
            Eval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]
            Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

        pipeline_type : typing.Optional[PipelineType]
            Type of pipeline. Either PLAYGROUND or MANAGED.

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.upsert_pipeline(
                name="name",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/pipelines",
            method="PUT",
            params={
                "project_id": project_id,
                "organization_id": organization_id,
            },
            json={
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=PipelineCreateEmbeddingConfig, direction="write"
                ),
                "transform_config": convert_and_respect_annotation_metadata(
                    object_=transform_config, annotation=PipelineCreateTransformConfig, direction="write"
                ),
                "configured_transformations": convert_and_respect_annotation_metadata(
                    object_=configured_transformations,
                    annotation=typing.Sequence[ConfiguredTransformationItem],
                    direction="write",
                ),
                "data_sink_id": data_sink_id,
                "embedding_model_config_id": embedding_model_config_id,
                "data_sink": convert_and_respect_annotation_metadata(
                    object_=data_sink, annotation=DataSinkCreate, direction="write"
                ),
                "preset_retrieval_parameters": convert_and_respect_annotation_metadata(
                    object_=preset_retrieval_parameters, annotation=PresetRetrievalParams, direction="write"
                ),
                "eval_parameters": convert_and_respect_annotation_metadata(
                    object_=eval_parameters, annotation=EvalExecutionParams, direction="write"
                ),
                "llama_parse_parameters": convert_and_respect_annotation_metadata(
                    object_=llama_parse_parameters, annotation=LlamaParseParameters, direction="write"
                ),
                "name": name,
                "pipeline_type": pipeline_type,
                "managed_pipeline_id": managed_pipeline_id,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> Pipeline:
        """
        Get a pipeline by ID for a given project.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_pipeline(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_existing_pipeline(
        self,
        pipeline_id: str,
        *,
        embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineUpdateTransformConfig] = OMIT,
        configured_transformations: typing.Optional[typing.Sequence[ConfiguredTransformationItem]] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        eval_parameters: typing.Optional[EvalExecutionParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        name: typing.Optional[str] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Update an existing pipeline for a project.

        Parameters
        ----------
        pipeline_id : str

        embedding_config : typing.Optional[PipelineUpdateEmbeddingConfig]

        transform_config : typing.Optional[PipelineUpdateTransformConfig]
            Configuration for the transformation.

        configured_transformations : typing.Optional[typing.Sequence[ConfiguredTransformationItem]]
            Deprecated, use embedding_config or transform_config instead. configured transformations for the pipeline.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        eval_parameters : typing.Optional[EvalExecutionParams]
            Eval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]
            Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

        name : typing.Optional[str]

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.update_existing_pipeline(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}",
            method="PUT",
            json={
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=PipelineUpdateEmbeddingConfig, direction="write"
                ),
                "transform_config": convert_and_respect_annotation_metadata(
                    object_=transform_config, annotation=PipelineUpdateTransformConfig, direction="write"
                ),
                "configured_transformations": convert_and_respect_annotation_metadata(
                    object_=configured_transformations,
                    annotation=typing.Sequence[ConfiguredTransformationItem],
                    direction="write",
                ),
                "data_sink_id": data_sink_id,
                "embedding_model_config_id": embedding_model_config_id,
                "data_sink": convert_and_respect_annotation_metadata(
                    object_=data_sink, annotation=DataSinkCreate, direction="write"
                ),
                "preset_retrieval_parameters": convert_and_respect_annotation_metadata(
                    object_=preset_retrieval_parameters, annotation=PresetRetrievalParams, direction="write"
                ),
                "eval_parameters": convert_and_respect_annotation_metadata(
                    object_=eval_parameters, annotation=EvalExecutionParams, direction="write"
                ),
                "llama_parse_parameters": convert_and_respect_annotation_metadata(
                    object_=llama_parse_parameters, annotation=LlamaParseParameters, direction="write"
                ),
                "name": name,
                "managed_pipeline_id": managed_pipeline_id,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.delete_pipeline(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_status(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get the status of a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_pipeline_status(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/status",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ManagedIngestionStatusResponse,
                    parse_obj_as(
                        type_=ManagedIngestionStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def sync_pipeline(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> Pipeline:
        """
        Run ingestion for the pipeline by incrementally updating the data-sink with upstream changes from data-sources & files.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.sync_pipeline(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/sync",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def cancel_pipeline_sync(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> Pipeline:
        """
        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.cancel_pipeline_sync(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/sync/cancel",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def copy_pipeline(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> Pipeline:
        """
        Copy a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.copy_pipeline(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/copy",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_eval_dataset_executions(
        self, eval_dataset_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[EvalDatasetJobRecord]:
        """
        Get the status of an EvalDatasetExecution.

        Parameters
        ----------
        eval_dataset_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[EvalDatasetJobRecord]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_eval_dataset_executions(
                eval_dataset_id="eval_dataset_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/eval-datasets/{jsonable_encoder(eval_dataset_id)}/execute",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[EvalDatasetJobRecord],
                    parse_obj_as(
                        type_=typing.List[EvalDatasetJobRecord],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def execute_eval_dataset(
        self,
        eval_dataset_id: str,
        pipeline_id: str,
        *,
        eval_question_ids: typing.Sequence[str],
        params: typing.Optional[EvalExecutionParamsOverride] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvalDatasetJobRecord:
        """
        Execute a dataset.

        Parameters
        ----------
        eval_dataset_id : str

        pipeline_id : str

        eval_question_ids : typing.Sequence[str]

        params : typing.Optional[EvalExecutionParamsOverride]
            The parameters for the eval execution that will override the ones set in the pipeline.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvalDatasetJobRecord
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.execute_eval_dataset(
                eval_dataset_id="eval_dataset_id",
                pipeline_id="pipeline_id",
                eval_question_ids=["eval_question_ids"],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/eval-datasets/{jsonable_encoder(eval_dataset_id)}/execute",
            method="POST",
            json={
                "eval_question_ids": eval_question_ids,
                "params": convert_and_respect_annotation_metadata(
                    object_=params, annotation=EvalExecutionParamsOverride, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvalDatasetJobRecord,
                    parse_obj_as(
                        type_=EvalDatasetJobRecord,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_eval_dataset_execution_result(
        self, eval_dataset_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[EvalQuestionResult]:
        """
        Get the result of an EvalDatasetExecution.
        If eval_question_ids is specified, only the results for the specified
        questions will be returned.
        If any of the specified questions do not have a result, they will be ignored.

        Parameters
        ----------
        eval_dataset_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[EvalQuestionResult]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_eval_dataset_execution_result(
                eval_dataset_id="eval_dataset_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/eval-datasets/{jsonable_encoder(eval_dataset_id)}/execute/result",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[EvalQuestionResult],
                    parse_obj_as(
                        type_=typing.List[EvalQuestionResult],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_eval_dataset_execution(
        self,
        eval_dataset_id: str,
        eval_dataset_execution_id: str,
        pipeline_id: str,
        *,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EvalDatasetJobRecord:
        """
        Get the status of an EvalDatasetExecution.

        Parameters
        ----------
        eval_dataset_id : str

        eval_dataset_execution_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EvalDatasetJobRecord
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_eval_dataset_execution(
                eval_dataset_id="eval_dataset_id",
                eval_dataset_execution_id="eval_dataset_execution_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/eval-datasets/{jsonable_encoder(eval_dataset_id)}/execute/{jsonable_encoder(eval_dataset_execution_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EvalDatasetJobRecord,
                    parse_obj_as(
                        type_=EvalDatasetJobRecord,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_files(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineFile]:
        """
        Get files for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineFile]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.list_pipeline_files(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files",
            method="GET",
            params={
                "data_source_id": data_source_id,
                "only_manually_uploaded": only_manually_uploaded,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineFile],
                    parse_obj_as(
                        type_=typing.List[PipelineFile],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def add_files_to_pipeline(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[PipelineFileCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineFile]:
        """
        Add files to a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[PipelineFileCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineFile]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, PipelineFileCreate

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.add_files_to_pipeline(
                pipeline_id="pipeline_id",
                request=[
                    PipelineFileCreate(
                        file_id="file_id",
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files",
            method="PUT",
            json=convert_and_respect_annotation_metadata(
                object_=request, annotation=typing.Sequence[PipelineFileCreate], direction="write"
            ),
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineFile],
                    parse_obj_as(
                        type_=typing.List[PipelineFile],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_files_2(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        limit: typing.Optional[int] = None,
        offset: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedListPipelineFilesResponse:
        """
        Get files for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        limit : typing.Optional[int]

        offset : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedListPipelineFilesResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.list_pipeline_files_2(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files2",
            method="GET",
            params={
                "data_source_id": data_source_id,
                "only_manually_uploaded": only_manually_uploaded,
                "limit": limit,
                "offset": offset,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PaginatedListPipelineFilesResponse,
                    parse_obj_as(
                        type_=PaginatedListPipelineFilesResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_file_status(
        self, file_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get status of a file for a pipeline.

        Parameters
        ----------
        file_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_pipeline_file_status(
                file_id="file_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files/{jsonable_encoder(file_id)}/status",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ManagedIngestionStatusResponse,
                    parse_obj_as(
                        type_=ManagedIngestionStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_pipeline_file(
        self,
        file_id: str,
        pipeline_id: str,
        *,
        custom_metadata: typing.Optional[
            typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]]
        ] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PipelineFile:
        """
        Update a file for a pipeline.

        Parameters
        ----------
        file_id : str

        pipeline_id : str

        custom_metadata : typing.Optional[typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]]]
            Custom metadata for the file

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PipelineFile
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.update_pipeline_file(
                file_id="file_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files/{jsonable_encoder(file_id)}",
            method="PUT",
            json={
                "custom_metadata": convert_and_respect_annotation_metadata(
                    object_=custom_metadata,
                    annotation=typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]],
                    direction="write",
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PipelineFile,
                    parse_obj_as(
                        type_=PipelineFile,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline_file(
        self, file_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a file from a pipeline.

        Parameters
        ----------
        file_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.delete_pipeline_file(
                file_id="file_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/files/{jsonable_encoder(file_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def import_pipeline_metadata(
        self, pipeline_id: str, *, upload_file: core.File, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Dict[str, str]:
        """
        Import metadata for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        upload_file : core.File
            See core.File for more documentation

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Dict[str, str]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.import_pipeline_metadata(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/metadata",
            method="PUT",
            data={},
            files={
                "upload_file": upload_file,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Dict[str, str],
                    parse_obj_as(
                        type_=typing.Dict[str, str],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline_files_metadata(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete metadata for all files in a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.delete_pipeline_files_metadata(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/metadata",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_data_sources(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[PipelineDataSource]:
        """
        Get data sources for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineDataSource]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.list_pipeline_data_sources(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineDataSource],
                    parse_obj_as(
                        type_=typing.List[PipelineDataSource],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def add_data_sources_to_pipeline(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[PipelineDataSourceCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineDataSource]:
        """
        Add data sources to a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[PipelineDataSourceCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineDataSource]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, PipelineDataSourceCreate

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.add_data_sources_to_pipeline(
                pipeline_id="pipeline_id",
                request=[
                    PipelineDataSourceCreate(
                        data_source_id="data_source_id",
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources",
            method="PUT",
            json=convert_and_respect_annotation_metadata(
                object_=request, annotation=typing.Sequence[PipelineDataSourceCreate], direction="write"
            ),
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineDataSource],
                    parse_obj_as(
                        type_=typing.List[PipelineDataSource],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_pipeline_data_source(
        self,
        data_source_id: str,
        pipeline_id: str,
        *,
        sync_interval: typing.Optional[float] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PipelineDataSource:
        """
        Update the configuration of a data source in a pipeline.

        Parameters
        ----------
        data_source_id : str

        pipeline_id : str

        sync_interval : typing.Optional[float]
            The interval at which the data source should be synced.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PipelineDataSource
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.update_pipeline_data_source(
                data_source_id="data_source_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources/{jsonable_encoder(data_source_id)}",
            method="PUT",
            json={
                "sync_interval": sync_interval,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PipelineDataSource,
                    parse_obj_as(
                        type_=PipelineDataSource,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline_data_source(
        self, data_source_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a data source from a pipeline.

        Parameters
        ----------
        data_source_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.delete_pipeline_data_source(
                data_source_id="data_source_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources/{jsonable_encoder(data_source_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def sync_pipeline_data_source(
        self, data_source_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> Pipeline:
        """
        Run ingestion for the pipeline data source by incrementally updating the data-sink with upstream changes from data-source.

        Parameters
        ----------
        data_source_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.sync_pipeline_data_source(
                data_source_id="data_source_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources/{jsonable_encoder(data_source_id)}/sync",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    Pipeline,
                    parse_obj_as(
                        type_=Pipeline,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_data_source_status(
        self, data_source_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get the status of a data source for a pipeline.

        Parameters
        ----------
        data_source_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_pipeline_data_source_status(
                data_source_id="data_source_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/data-sources/{jsonable_encoder(data_source_id)}/status",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ManagedIngestionStatusResponse,
                    parse_obj_as(
                        type_=ManagedIngestionStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def run_search(
        self,
        pipeline_id: str,
        *,
        query: str,
        dense_similarity_top_k: typing.Optional[int] = OMIT,
        dense_similarity_cutoff: typing.Optional[float] = OMIT,
        sparse_similarity_top_k: typing.Optional[int] = OMIT,
        enable_reranking: typing.Optional[bool] = OMIT,
        rerank_top_n: typing.Optional[int] = OMIT,
        alpha: typing.Optional[float] = OMIT,
        search_filters: typing.Optional[MetadataFilters] = OMIT,
        files_top_k: typing.Optional[int] = OMIT,
        retrieval_mode: typing.Optional[RetrievalMode] = OMIT,
        retrieve_image_nodes: typing.Optional[bool] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RetrieveResults:
        """
        Get retrieval results for a managed pipeline and a query

        Parameters
        ----------
        pipeline_id : str

        query : str
            The query to retrieve against.

        dense_similarity_top_k : typing.Optional[int]
            Number of nodes for dense retrieval.

        dense_similarity_cutoff : typing.Optional[float]
            Minimum similarity score wrt query for retrieval

        sparse_similarity_top_k : typing.Optional[int]
            Number of nodes for sparse retrieval.

        enable_reranking : typing.Optional[bool]
            Enable reranking for retrieval

        rerank_top_n : typing.Optional[int]
            Number of reranked nodes for returning.

        alpha : typing.Optional[float]
            Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval.

        search_filters : typing.Optional[MetadataFilters]
            Search filters for retrieval.

        files_top_k : typing.Optional[int]
            Number of files to retrieve (only for retrieval mode files_via_metadata and files_via_content).

        retrieval_mode : typing.Optional[RetrievalMode]
            The retrieval mode for the query.

        retrieve_image_nodes : typing.Optional[bool]
            Whether to retrieve image nodes.

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RetrieveResults
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.run_search(
                pipeline_id="pipeline_id",
                query="query",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/retrieve",
            method="POST",
            json={
                "dense_similarity_top_k": dense_similarity_top_k,
                "dense_similarity_cutoff": dense_similarity_cutoff,
                "sparse_similarity_top_k": sparse_similarity_top_k,
                "enable_reranking": enable_reranking,
                "rerank_top_n": rerank_top_n,
                "alpha": alpha,
                "search_filters": convert_and_respect_annotation_metadata(
                    object_=search_filters, annotation=MetadataFilters, direction="write"
                ),
                "files_top_k": files_top_k,
                "retrieval_mode": retrieval_mode,
                "retrieve_image_nodes": retrieve_image_nodes,
                "query": query,
                "class_name": class_name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    RetrieveResults,
                    parse_obj_as(
                        type_=RetrieveResults,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_jobs(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[PipelineDeployment]:
        """
        Get jobs for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineDeployment]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.list_pipeline_jobs(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/jobs",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[PipelineDeployment],
                    parse_obj_as(
                        type_=typing.List[PipelineDeployment],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_job(
        self, job_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> PipelineDeployment:
        """
        Get a job for a pipeline.

        Parameters
        ----------
        job_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PipelineDeployment
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_pipeline_job(
                job_id="job_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/jobs/{jsonable_encoder(job_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PipelineDeployment,
                    parse_obj_as(
                        type_=PipelineDeployment,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_playground_session(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> PlaygroundSession:
        """
        Get a playground session for a user and pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PlaygroundSession
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_playground_session(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/playground-session",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    PlaygroundSession,
                    parse_obj_as(
                        type_=PlaygroundSession,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def chat(
        self,
        pipeline_id: str,
        *,
        messages: typing.Optional[typing.Sequence[InputMessage]] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters
        ----------
        pipeline_id : str

        messages : typing.Optional[typing.Sequence[InputMessage]]

        data : typing.Optional[ChatData]

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.chat(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/chat",
            method="POST",
            json={
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[InputMessage], direction="write"
                ),
                "data": convert_and_respect_annotation_metadata(object_=data, annotation=ChatData, direction="write"),
                "class_name": class_name,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.Optional[typing.Any],
                    parse_obj_as(
                        type_=typing.Optional[typing.Any],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Return a list of documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        skip : typing.Optional[int]

        limit : typing.Optional[int]

        file_id : typing.Optional[str]

        only_direct_upload : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.list_pipeline_documents(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents",
            method="GET",
            params={
                "skip": skip,
                "limit": limit,
                "file_id": file_id,
                "only_direct_upload": only_direct_upload,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[CloudDocument],
                    parse_obj_as(
                        type_=typing.List[CloudDocument],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_batch_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[CloudDocumentCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Batch create documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[CloudDocumentCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, CloudDocumentCreate

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.create_batch_pipeline_documents(
                pipeline_id="pipeline_id",
                request=[
                    CloudDocumentCreate(
                        text="text",
                        metadata={"key": "value"},
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents",
            method="POST",
            json=convert_and_respect_annotation_metadata(
                object_=request, annotation=typing.Sequence[CloudDocumentCreate], direction="write"
            ),
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[CloudDocument],
                    parse_obj_as(
                        type_=typing.List[CloudDocument],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert_batch_pipeline_documents(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[CloudDocumentCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Batch create or update a document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[CloudDocumentCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud, CloudDocumentCreate

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.upsert_batch_pipeline_documents(
                pipeline_id="pipeline_id",
                request=[
                    CloudDocumentCreate(
                        text="text",
                        metadata={"key": "value"},
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents",
            method="PUT",
            json=convert_and_respect_annotation_metadata(
                object_=request, annotation=typing.Sequence[CloudDocumentCreate], direction="write"
            ),
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[CloudDocument],
                    parse_obj_as(
                        type_=typing.List[CloudDocument],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_document(
        self, document_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> CloudDocument:
        """
        Return a single document for a pipeline.

        Parameters
        ----------
        document_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CloudDocument
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_pipeline_document(
                document_id="document_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents/{jsonable_encoder(document_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    CloudDocument,
                    parse_obj_as(
                        type_=CloudDocument,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_pipeline_document(
        self, document_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a document for a pipeline.

        Parameters
        ----------
        document_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.delete_pipeline_document(
                document_id="document_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents/{jsonable_encoder(document_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_pipeline_document_status(
        self, document_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Return a single document for a pipeline.

        Parameters
        ----------
        document_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.get_pipeline_document_status(
                document_id="document_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents/{jsonable_encoder(document_id)}/status",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ManagedIngestionStatusResponse,
                    parse_obj_as(
                        type_=ManagedIngestionStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_pipeline_document_chunks(
        self, document_id: str, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[TextNode]:
        """
        Return a list of chunks for a pipeline document.

        Parameters
        ----------
        document_id : str

        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[TextNode]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.pipelines.list_pipeline_document_chunks(
                document_id="document_id",
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/pipelines/{jsonable_encoder(pipeline_id)}/documents/{jsonable_encoder(document_id)}/chunks",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[TextNode],
                    parse_obj_as(
                        type_=typing.List[TextNode],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
