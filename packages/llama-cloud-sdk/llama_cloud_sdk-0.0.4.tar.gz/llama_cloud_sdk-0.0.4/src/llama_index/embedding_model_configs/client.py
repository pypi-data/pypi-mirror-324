# This file was auto-generated by Fern from our API Definition.

import typing
from ..core.client_wrapper import SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.embedding_model_config import EmbeddingModelConfig
from ..core.pydantic_utilities import parse_obj_as
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.http_validation_error import HttpValidationError
from json.decoder import JSONDecodeError
from ..core.api_error import ApiError
from .types.embedding_model_config_create_embedding_config import EmbeddingModelConfigCreateEmbeddingConfig
from ..core.serialization import convert_and_respect_annotation_metadata
from ..types.embedding_model_config_update_embedding_config import EmbeddingModelConfigUpdateEmbeddingConfig
from ..core.jsonable_encoder import jsonable_encoder
from ..core.client_wrapper import AsyncClientWrapper

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class EmbeddingModelConfigsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list_embedding_model_configs(
        self, *, project_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[EmbeddingModelConfig]:
        """
        Parameters
        ----------
        project_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[EmbeddingModelConfig]
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.embedding_model_configs.list_embedding_model_configs(
            project_id="project_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/embedding-model-configs",
            method="GET",
            params={
                "project_id": project_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[EmbeddingModelConfig],
                    parse_obj_as(
                        type_=typing.List[EmbeddingModelConfig],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_embedding_model_config(
        self,
        *,
        project_id: str,
        name: str,
        embedding_config: EmbeddingModelConfigCreateEmbeddingConfig,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EmbeddingModelConfig:
        """
        Create a new embedding model configuration within a specified project.

        Parameters
        ----------
        project_id : str

        name : str
            The name of the embedding model config.

        embedding_config : EmbeddingModelConfigCreateEmbeddingConfig
            The embedding configuration for the embedding model config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EmbeddingModelConfig
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud
        from llama_index.embedding_model_configs import (
            EmbeddingModelConfigCreateEmbeddingConfig_VertexaiEmbedding,
        )

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.embedding_model_configs.create_embedding_model_config(
            project_id="project_id",
            name="name",
            embedding_config=EmbeddingModelConfigCreateEmbeddingConfig_VertexaiEmbedding(),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/embedding-model-configs",
            method="POST",
            params={
                "project_id": project_id,
            },
            json={
                "name": name,
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=EmbeddingModelConfigCreateEmbeddingConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EmbeddingModelConfig,
                    parse_obj_as(
                        type_=EmbeddingModelConfig,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert_embedding_model_config(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        name: typing.Optional[str] = OMIT,
        embedding_config: typing.Optional[EmbeddingModelConfigUpdateEmbeddingConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EmbeddingModelConfig:
        """
        Upserts an embedding model config.
        Updates if an embedding model config with the same name and project_id already exists. Otherwise, creates a new embedding model config.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        name : typing.Optional[str]
            The name of the embedding model config.

        embedding_config : typing.Optional[EmbeddingModelConfigUpdateEmbeddingConfig]
            The embedding configuration for the embedding model config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EmbeddingModelConfig
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.embedding_model_configs.upsert_embedding_model_config()
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/v1/embedding-model-configs",
            method="PUT",
            params={
                "project_id": project_id,
                "organization_id": organization_id,
            },
            json={
                "name": name,
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=EmbeddingModelConfigUpdateEmbeddingConfig, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EmbeddingModelConfig,
                    parse_obj_as(
                        type_=EmbeddingModelConfig,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_embedding_model_config(
        self,
        embedding_model_config_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        embedding_config: typing.Optional[EmbeddingModelConfigUpdateEmbeddingConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EmbeddingModelConfig:
        """
        Update an embedding model config by ID.

        Parameters
        ----------
        embedding_model_config_id : str

        name : typing.Optional[str]
            The name of the embedding model config.

        embedding_config : typing.Optional[EmbeddingModelConfigUpdateEmbeddingConfig]
            The embedding configuration for the embedding model config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EmbeddingModelConfig
            Successful Response

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.embedding_model_configs.update_embedding_model_config(
            embedding_model_config_id="embedding_model_config_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/embedding-model-configs/{jsonable_encoder(embedding_model_config_id)}",
            method="PUT",
            json={
                "name": name,
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=EmbeddingModelConfigUpdateEmbeddingConfig, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EmbeddingModelConfig,
                    parse_obj_as(
                        type_=EmbeddingModelConfig,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_embedding_model_config(
        self, embedding_model_config_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete an embedding model config by ID.

        Parameters
        ----------
        embedding_model_config_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llama_index import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.embedding_model_configs.delete_embedding_model_config(
            embedding_model_config_id="embedding_model_config_id",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/v1/embedding-model-configs/{jsonable_encoder(embedding_model_config_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncEmbeddingModelConfigsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list_embedding_model_configs(
        self, *, project_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[EmbeddingModelConfig]:
        """
        Parameters
        ----------
        project_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[EmbeddingModelConfig]
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.embedding_model_configs.list_embedding_model_configs(
                project_id="project_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/embedding-model-configs",
            method="GET",
            params={
                "project_id": project_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[EmbeddingModelConfig],
                    parse_obj_as(
                        type_=typing.List[EmbeddingModelConfig],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_embedding_model_config(
        self,
        *,
        project_id: str,
        name: str,
        embedding_config: EmbeddingModelConfigCreateEmbeddingConfig,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EmbeddingModelConfig:
        """
        Create a new embedding model configuration within a specified project.

        Parameters
        ----------
        project_id : str

        name : str
            The name of the embedding model config.

        embedding_config : EmbeddingModelConfigCreateEmbeddingConfig
            The embedding configuration for the embedding model config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EmbeddingModelConfig
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud
        from llama_index.embedding_model_configs import (
            EmbeddingModelConfigCreateEmbeddingConfig_VertexaiEmbedding,
        )

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.embedding_model_configs.create_embedding_model_config(
                project_id="project_id",
                name="name",
                embedding_config=EmbeddingModelConfigCreateEmbeddingConfig_VertexaiEmbedding(),
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/embedding-model-configs",
            method="POST",
            params={
                "project_id": project_id,
            },
            json={
                "name": name,
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=EmbeddingModelConfigCreateEmbeddingConfig, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EmbeddingModelConfig,
                    parse_obj_as(
                        type_=EmbeddingModelConfig,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert_embedding_model_config(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        name: typing.Optional[str] = OMIT,
        embedding_config: typing.Optional[EmbeddingModelConfigUpdateEmbeddingConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EmbeddingModelConfig:
        """
        Upserts an embedding model config.
        Updates if an embedding model config with the same name and project_id already exists. Otherwise, creates a new embedding model config.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        name : typing.Optional[str]
            The name of the embedding model config.

        embedding_config : typing.Optional[EmbeddingModelConfigUpdateEmbeddingConfig]
            The embedding configuration for the embedding model config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EmbeddingModelConfig
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.embedding_model_configs.upsert_embedding_model_config()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/v1/embedding-model-configs",
            method="PUT",
            params={
                "project_id": project_id,
                "organization_id": organization_id,
            },
            json={
                "name": name,
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=EmbeddingModelConfigUpdateEmbeddingConfig, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EmbeddingModelConfig,
                    parse_obj_as(
                        type_=EmbeddingModelConfig,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_embedding_model_config(
        self,
        embedding_model_config_id: str,
        *,
        name: typing.Optional[str] = OMIT,
        embedding_config: typing.Optional[EmbeddingModelConfigUpdateEmbeddingConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EmbeddingModelConfig:
        """
        Update an embedding model config by ID.

        Parameters
        ----------
        embedding_model_config_id : str

        name : typing.Optional[str]
            The name of the embedding model config.

        embedding_config : typing.Optional[EmbeddingModelConfigUpdateEmbeddingConfig]
            The embedding configuration for the embedding model config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EmbeddingModelConfig
            Successful Response

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.embedding_model_configs.update_embedding_model_config(
                embedding_model_config_id="embedding_model_config_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/embedding-model-configs/{jsonable_encoder(embedding_model_config_id)}",
            method="PUT",
            json={
                "name": name,
                "embedding_config": convert_and_respect_annotation_metadata(
                    object_=embedding_config, annotation=EmbeddingModelConfigUpdateEmbeddingConfig, direction="write"
                ),
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    EmbeddingModelConfig,
                    parse_obj_as(
                        type_=EmbeddingModelConfig,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_embedding_model_config(
        self, embedding_model_config_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete an embedding model config by ID.

        Parameters
        ----------
        embedding_model_config_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llama_index import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.embedding_model_configs.delete_embedding_model_config(
                embedding_model_config_id="embedding_model_config_id",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/v1/embedding-model-configs/{jsonable_encoder(embedding_model_config_id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    )
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
