from tensorflow.python.ops.gen_resource_variable_ops import *
from tensorflow.core.framework import attr_value_pb2 as attr_value_pb2, variable_pb2 as variable_pb2
from tensorflow.python.client import pywrap_tf_session as pywrap_tf_session
from tensorflow.python.eager import context as context, tape as tape
from tensorflow.python.framework import constant_op as constant_op, cpp_shape_inference_pb2 as cpp_shape_inference_pb2, dtypes as dtypes, errors as errors, meta_graph as meta_graph, ops as ops, tensor_shape as tensor_shape, tensor_spec as tensor_spec
from tensorflow.python.ops import array_ops as array_ops, gen_array_ops as gen_array_ops, gen_resource_variable_ops as gen_resource_variable_ops, gen_state_ops as gen_state_ops, handle_data_util as handle_data_util, math_ops as math_ops, state_ops as state_ops, variables as variables
from tensorflow.python.types import core as core
from tensorflow.python.util import compat as compat
from tensorflow.python.util.deprecation import deprecated as deprecated
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any

get_resource_handle_data: Any

def get_eager_safe_handle_data(handle): ...
def eager_safe_variable_handle(initial_value, shape, shared_name, name, graph_mode): ...

class EagerResourceDeleter:
    def __init__(self, handle, handle_device) -> None: ...
    def __del__(self) -> None: ...

def shape_safe_assign_variable_handle(handle, shape, value, name: Any | None = ...): ...
def variable_accessed(variable) -> None: ...

class BaseResourceVariable(variables.VariableV1, core.Tensor):
    def __init__(self, trainable: Any | None = ..., shape: Any | None = ..., dtype: Any | None = ..., handle: Any | None = ..., constraint: Any | None = ..., synchronization: Any | None = ..., aggregation: Any | None = ..., distribute_strategy: Any | None = ..., name: Any | None = ..., unique_id: Any | None = ..., handle_name: Any | None = ..., graph_element: Any | None = ..., initial_value: Any | None = ..., initializer_op: Any | None = ..., is_initialized_op: Any | None = ..., cached_value: Any | None = ..., save_slice_info: Any | None = ..., handle_deleter: Any | None = ..., caching_device: Any | None = ..., in_graph_mode: Any | None = ..., **unused_kwargs) -> None: ...
    def __tf_function_cache_spec__(self): ...
    def __tf_resource_id__(self): ...
    def __array__(self, dtype: Any | None = ...): ...
    def __nonzero__(self): ...
    def __bool__(self): ...
    def __copy__(self): ...
    def __deepcopy__(self, memo): ...
    @property
    def dtype(self): ...
    @property
    def device(self): ...
    @property
    def graph(self): ...
    @property
    def name(self): ...
    @property
    def shape(self): ...
    def set_shape(self, shape) -> None: ...
    @property
    def create(self): ...
    @property
    def handle(self): ...
    def value(self): ...
    @property
    def initializer(self): ...
    @property
    def initial_value(self): ...
    @property
    def constraint(self): ...
    @property
    def op(self): ...
    @property
    def trainable(self): ...
    @property
    def synchronization(self): ...
    @property
    def aggregation(self): ...
    def eval(self, session: Any | None = ...): ...
    def numpy(self): ...
    def count_up_to(self, limit): ...
    def read_value(self): ...
    def sparse_read(self, indices, name: Any | None = ...): ...
    def gather_nd(self, indices, name: Any | None = ...): ...
    def to_proto(self, export_scope: Any | None = ...): ...
    @staticmethod
    def from_proto(variable_def, import_scope: Any | None = ...): ...
    __array_priority__: int
    def is_initialized(self, name: Any | None = ...): ...
    def assign_sub(self, delta, use_locking: Any | None = ..., name: Any | None = ..., read_value: bool = ...): ...
    def assign_add(self, delta, use_locking: Any | None = ..., name: Any | None = ..., read_value: bool = ...): ...
    def assign(self, value, use_locking: Any | None = ..., name: Any | None = ..., read_value: bool = ...): ...
    def __reduce__(self): ...
    def scatter_sub(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_add(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_max(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_min(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_mul(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_div(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_update(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def batch_scatter_update(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_nd_sub(self, indices, updates, name: Any | None = ...): ...
    def scatter_nd_add(self, indices, updates, name: Any | None = ...): ...
    def scatter_nd_update(self, indices, updates, name: Any | None = ...): ...
    def scatter_nd_max(self, indices, updates, name: Any | None = ...): ...
    def scatter_nd_min(self, indices, updates, name: Any | None = ...): ...
    def __complex__(self): ...
    def __int__(self): ...
    def __long__(self): ...
    def __float__(self): ...
    def __iadd__(self, unused_other) -> None: ...
    def __isub__(self, unused_other) -> None: ...
    def __imul__(self, unused_other) -> None: ...
    def __idiv__(self, unused_other) -> None: ...
    def __itruediv__(self, unused_other) -> None: ...
    def __irealdiv__(self, unused_other) -> None: ...
    def __ipow__(self, unused_other) -> None: ...

class ResourceVariable(BaseResourceVariable):
    def __init__(self, initial_value: Any | None = ..., trainable: Any | None = ..., collections: Any | None = ..., validate_shape: bool = ..., caching_device: Any | None = ..., name: Any | None = ..., dtype: Any | None = ..., variable_def: Any | None = ..., import_scope: Any | None = ..., constraint: Any | None = ..., distribute_strategy: Any | None = ..., synchronization: Any | None = ..., aggregation: Any | None = ..., shape: Any | None = ...) -> None: ...

class UninitializedVariable(BaseResourceVariable):
    def __init__(self, trainable: Any | None = ..., caching_device: Any | None = ..., name: Any | None = ..., shape: Any | None = ..., dtype: Any | None = ..., constraint: Any | None = ..., synchronization: Any | None = ..., aggregation: Any | None = ..., extra_handle_data: Any | None = ..., distribute_strategy: Any | None = ..., **unused_kwargs) -> None: ...

class _UnreadVariable(BaseResourceVariable):
    def __init__(self, handle, dtype, shape, in_graph_mode, deleter, parent_op, unique_id) -> None: ...
    @property
    def name(self): ...
    def value(self): ...
    def read_value(self): ...
    def assign_sub(self, delta, use_locking: Any | None = ..., name: Any | None = ..., read_value: bool = ...): ...
    def assign_add(self, delta, use_locking: Any | None = ..., name: Any | None = ..., read_value: bool = ...): ...
    def assign(self, value, use_locking: Any | None = ..., name: Any | None = ..., read_value: bool = ...): ...
    def scatter_sub(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_add(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_max(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_min(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_mul(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_div(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_update(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def batch_scatter_update(self, sparse_delta, use_locking: bool = ..., name: Any | None = ...): ...
    def scatter_nd_sub(self, indices, updates, name: Any | None = ...): ...
    def scatter_nd_add(self, indices, updates, name: Any | None = ...): ...
    def scatter_nd_update(self, indices, updates, name: Any | None = ...): ...
    def scatter_nd_max(self, indices, updates, name: Any | None = ...): ...
    def scatter_nd_min(self, indices, updates, name: Any | None = ...): ...
    @property
    def op(self): ...

def variable_shape(handle, out_type=...): ...
def is_resource_variable(var): ...
def copy_to_graph_uninitialized(var): ...

class VariableSpec(tensor_spec.DenseSpec):
    value_type: Any
    trainable: Any
    def __init__(self, shape, dtype=..., name: Any | None = ..., trainable: bool = ...) -> None: ...

def write_object_proto_for_resource_variable(resource_variable, proto, options) -> None: ...
