from tensorflow.python.data.ops import dataset_ops as dataset_ops, iterator_ops as iterator_ops
from tensorflow.python.distribute import reduce_util as reduce_util
from tensorflow.python.eager import context as context, def_function as def_function
from tensorflow.python.framework import dtypes as dtypes, ops as ops, sparse_tensor as sparse_tensor, tensor_util as tensor_util
from tensorflow.python.keras import backend as backend, callbacks as callbacks, optimizers as optimizers
from tensorflow.python.keras.engine import training_utils_v1 as training_utils_v1
from tensorflow.python.keras.optimizer_v2 import optimizer_v2 as optimizer_v2
from tensorflow.python.keras.utils import tf_contextlib as tf_contextlib
from tensorflow.python.keras.utils.mode_keys import ModeKeys as ModeKeys
from tensorflow.python.ops import array_ops as array_ops, control_flow_ops as control_flow_ops, math_ops as math_ops, sparse_ops as sparse_ops, variables as variables
from tensorflow.python.ops.ragged import ragged_tensor as ragged_tensor
from tensorflow.python.util import nest as nest
from typing import Any

def set_weights(distribution_strategy, dist_model, weights) -> None: ...
def unwrap_values(distribution_strategy, grouped_inputs, grouped_outputs, grouped_updates: Any | None = ..., grouped_session_args: Any | None = ..., with_loss_tensor: bool = ...): ...
def unwrap_output_dict(strategy, grouped_outputs, mode): ...
def unwrap_outputs(distribution_strategy, grouped_outputs, with_loss_tensor: bool = ...): ...
def flatten_per_replica_values(distribution_strategy, per_replica_values): ...
def validate_callbacks(input_callbacks, optimizer) -> None: ...
def validate_distributed_dataset_inputs(distribution_strategy, x, y, sample_weights: Any | None = ...): ...
def validate_per_replica_inputs(distribution_strategy, x): ...
def validate_all_tensor_types(x, x_values) -> None: ...
def validate_all_tensor_shapes(x, x_values) -> None: ...
def init_restore_or_wait_for_variables() -> None: ...
def validate_inputs(x, y) -> None: ...
def is_dataset_shape_fully_defined(dataset): ...
def process_batch_and_step_size(strategy, inputs, batch_size, steps_per_epoch, mode, validation_split: float = ...): ...
def get_input_params(distribution_strategy, num_samples, steps, batch_size, mode: Any | None = ...): ...
def get_batch_dimension(iterator): ...
def get_iterator(dataset, distribution_strategy): ...
def initialize_iterator(iterator, distribution_strategy) -> None: ...
def is_distributing_by_cloning(model): ...
def clone_model_on_replicas(model, strategy, mode, inputs: Any | None = ..., targets: Any | None = ...) -> None: ...
def get_distributed_model(model, mode): ...
def set_distributed_model(model, mode, distributed_model) -> None: ...
def get_distributed_function(model, mode): ...
def set_distributed_function(model, mode, distributed_function) -> None: ...
def distributed_scope(strategy, learning_phase) -> None: ...
def is_current_worker_chief(): ...
def filter_distributed_callbacks(callbacks_list, model): ...
def concat_along_batch_dimension(outputs): ...
