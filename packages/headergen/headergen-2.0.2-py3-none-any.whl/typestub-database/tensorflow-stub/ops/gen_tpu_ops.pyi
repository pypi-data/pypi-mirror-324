from tensorflow.python.util.deprecation import deprecated_endpoints as deprecated_endpoints
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any, NamedTuple

def all_to_all(input, group_assignment, concat_dimension, split_dimension, split_count, name: Any | None = ...): ...

AllToAll: Any

def all_to_all_eager_fallback(input, group_assignment, concat_dimension, split_dimension, split_count, name, ctx): ...
def assign_variable_xla_concat_nd(resource, inputs, num_concats, paddings=..., name: Any | None = ...): ...

AssignVariableXlaConcatND: Any

def assign_variable_xla_concat_nd_eager_fallback(resource, inputs, num_concats, paddings, name, ctx): ...
def collective_permute(input, source_target_pairs, name: Any | None = ...): ...

CollectivePermute: Any

def collective_permute_eager_fallback(input, source_target_pairs, name, ctx): ...
def configure_distributed_tpu(embedding_config: str = ..., tpu_embedding_config: str = ..., is_global_init: bool = ..., enable_whole_mesh_compilations: bool = ..., compilation_failure_closes_chips: bool = ..., tpu_cancellation_closes_chips: int = ..., name: Any | None = ...): ...

ConfigureDistributedTPU: Any

def configure_distributed_tpu_eager_fallback(embedding_config, tpu_embedding_config, is_global_init, enable_whole_mesh_compilations, compilation_failure_closes_chips, tpu_cancellation_closes_chips, name, ctx): ...
def configure_tpu_embedding(config, name: Any | None = ...): ...

ConfigureTPUEmbedding: Any

def configure_tpu_embedding_eager_fallback(config, name, ctx): ...
def cross_replica_sum(input, group_assignment, name: Any | None = ...): ...

CrossReplicaSum: Any

def cross_replica_sum_eager_fallback(input, group_assignment, name, ctx): ...
def enqueue_tpu_embedding_integer_batch(batch, mode_override, device_ordinal: int = ..., name: Any | None = ...): ...

EnqueueTPUEmbeddingIntegerBatch: Any

def enqueue_tpu_embedding_integer_batch_eager_fallback(batch, mode_override, device_ordinal, name, ctx): ...
def enqueue_tpu_embedding_ragged_tensor_batch(sample_splits, embedding_indices, aggregation_weights, mode_override, table_ids, device_ordinal: int = ..., combiners=..., max_sequence_lengths=..., num_features=..., name: Any | None = ...): ...

EnqueueTPUEmbeddingRaggedTensorBatch: Any

def enqueue_tpu_embedding_ragged_tensor_batch_eager_fallback(sample_splits, embedding_indices, aggregation_weights, mode_override, table_ids, device_ordinal, combiners, max_sequence_lengths, num_features, name, ctx): ...
def enqueue_tpu_embedding_sparse_batch(sample_indices, embedding_indices, aggregation_weights, mode_override, device_ordinal: int = ..., combiners=..., name: Any | None = ...): ...

EnqueueTPUEmbeddingSparseBatch: Any

def enqueue_tpu_embedding_sparse_batch_eager_fallback(sample_indices, embedding_indices, aggregation_weights, mode_override, device_ordinal, combiners, name, ctx): ...
def enqueue_tpu_embedding_sparse_tensor_batch(sample_indices, embedding_indices, aggregation_weights, mode_override, table_ids, device_ordinal: int = ..., combiners=..., max_sequence_lengths=..., num_features=..., name: Any | None = ...): ...

EnqueueTPUEmbeddingSparseTensorBatch: Any

def enqueue_tpu_embedding_sparse_tensor_batch_eager_fallback(sample_indices, embedding_indices, aggregation_weights, mode_override, table_ids, device_ordinal, combiners, max_sequence_lengths, num_features, name, ctx): ...
def infeed_dequeue(dtype, shape, name: Any | None = ...): ...

InfeedDequeue: Any

def infeed_dequeue_eager_fallback(dtype, shape, name, ctx): ...
def infeed_dequeue_tuple(dtypes, shapes, name: Any | None = ...): ...

InfeedDequeueTuple: Any

def infeed_dequeue_tuple_eager_fallback(dtypes, shapes, name, ctx): ...
def infeed_enqueue(input, shape=..., layout=..., device_ordinal: int = ..., name: Any | None = ...): ...

InfeedEnqueue: Any

def infeed_enqueue_eager_fallback(input, shape, layout, device_ordinal, name, ctx): ...
def infeed_enqueue_prelinearized_buffer(input, device_ordinal: int = ..., name: Any | None = ...): ...

InfeedEnqueuePrelinearizedBuffer: Any

def infeed_enqueue_prelinearized_buffer_eager_fallback(input, device_ordinal, name, ctx): ...
def infeed_enqueue_tuple(inputs, shapes, layouts=..., device_ordinal: int = ..., name: Any | None = ...): ...

InfeedEnqueueTuple: Any

def infeed_enqueue_tuple_eager_fallback(inputs, shapes, layouts, device_ordinal, name, ctx): ...
def is_tpu_embedding_initialized(name: Any | None = ...): ...

IsTPUEmbeddingInitialized: Any

def is_tpu_embedding_initialized_eager_fallback(name, ctx): ...
def load_tpu_embedding_adam_parameters(parameters, momenta, velocities, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingADAMParameters: Any

def load_tpu_embedding_adam_parameters_eager_fallback(parameters, momenta, velocities, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_adadelta_parameters(parameters, accumulators, updates, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingAdadeltaParameters: Any

def load_tpu_embedding_adadelta_parameters_eager_fallback(parameters, accumulators, updates, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_adagrad_momentum_parameters(parameters, accumulators, momenta, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingAdagradMomentumParameters: Any

def load_tpu_embedding_adagrad_momentum_parameters_eager_fallback(parameters, accumulators, momenta, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_adagrad_parameters(parameters, accumulators, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingAdagradParameters: Any

def load_tpu_embedding_adagrad_parameters_eager_fallback(parameters, accumulators, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_centered_rms_prop_parameters(parameters, ms, mom, mg, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingCenteredRMSPropParameters: Any

def load_tpu_embedding_centered_rms_prop_parameters_eager_fallback(parameters, ms, mom, mg, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_ftrl_parameters(parameters, accumulators, linears, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingFTRLParameters: Any

def load_tpu_embedding_ftrl_parameters_eager_fallback(parameters, accumulators, linears, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_frequency_estimator_parameters(parameters, last_hit_step, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingFrequencyEstimatorParameters: Any

def load_tpu_embedding_frequency_estimator_parameters_eager_fallback(parameters, last_hit_step, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_mdl_adagrad_light_parameters(parameters, accumulators, weights, benefits, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingMDLAdagradLightParameters: Any

def load_tpu_embedding_mdl_adagrad_light_parameters_eager_fallback(parameters, accumulators, weights, benefits, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_momentum_parameters(parameters, momenta, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingMomentumParameters: Any

def load_tpu_embedding_momentum_parameters_eager_fallback(parameters, momenta, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_proximal_adagrad_parameters(parameters, accumulators, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingProximalAdagradParameters: Any

def load_tpu_embedding_proximal_adagrad_parameters_eager_fallback(parameters, accumulators, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_proximal_yogi_parameters(parameters, v, m, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingProximalYogiParameters: Any

def load_tpu_embedding_proximal_yogi_parameters_eager_fallback(parameters, v, m, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_rms_prop_parameters(parameters, ms, mom, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingRMSPropParameters: Any

def load_tpu_embedding_rms_prop_parameters_eager_fallback(parameters, ms, mom, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def load_tpu_embedding_stochastic_gradient_descent_parameters(parameters, num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

LoadTPUEmbeddingStochasticGradientDescentParameters: Any

def load_tpu_embedding_stochastic_gradient_descent_parameters_eager_fallback(parameters, num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def outfeed_dequeue(dtype, shape, device_ordinal: int = ..., name: Any | None = ...): ...

OutfeedDequeue: Any

def outfeed_dequeue_eager_fallback(dtype, shape, device_ordinal, name, ctx): ...
def outfeed_dequeue_tuple(dtypes, shapes, device_ordinal: int = ..., name: Any | None = ...): ...

OutfeedDequeueTuple: Any

def outfeed_dequeue_tuple_eager_fallback(dtypes, shapes, device_ordinal, name, ctx): ...
def outfeed_dequeue_tuple_v2(device_ordinal, dtypes, shapes, name: Any | None = ...): ...

OutfeedDequeueTupleV2: Any

def outfeed_dequeue_tuple_v2_eager_fallback(device_ordinal, dtypes, shapes, name, ctx): ...
def outfeed_dequeue_v2(device_ordinal, dtype, shape, name: Any | None = ...): ...

OutfeedDequeueV2: Any

def outfeed_dequeue_v2_eager_fallback(device_ordinal, dtype, shape, name, ctx): ...
def outfeed_enqueue(input, name: Any | None = ...): ...

OutfeedEnqueue: Any

def outfeed_enqueue_eager_fallback(input, name, ctx): ...
def outfeed_enqueue_tuple(inputs, name: Any | None = ...): ...

OutfeedEnqueueTuple: Any

def outfeed_enqueue_tuple_eager_fallback(inputs, name, ctx): ...
def prelinearize(input, shape=..., layout=..., name: Any | None = ...): ...

Prelinearize: Any

def prelinearize_eager_fallback(input, shape, layout, name, ctx): ...
def prelinearize_tuple(inputs, shapes, layouts=..., name: Any | None = ...): ...

PrelinearizeTuple: Any

def prelinearize_tuple_eager_fallback(inputs, shapes, layouts, name, ctx): ...
def read_variable_xla_split_nd(resource, T, N, num_splits, paddings=..., name: Any | None = ...): ...

ReadVariableXlaSplitND: Any

def read_variable_xla_split_nd_eager_fallback(resource, T, N, num_splits, paddings, name, ctx): ...
def recv_tpu_embedding_activations(num_outputs, config, name: Any | None = ...): ...

RecvTPUEmbeddingActivations: Any

def recv_tpu_embedding_activations_eager_fallback(num_outputs, config, name, ctx): ...

class _RetrieveTPUEmbeddingADAMParametersOutput(NamedTuple):
    parameters: Any
    momenta: Any
    velocities: Any

def retrieve_tpu_embedding_adam_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingADAMParameters: Any

def retrieve_tpu_embedding_adam_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingAdadeltaParametersOutput(NamedTuple):
    parameters: Any
    accumulators: Any
    updates: Any

def retrieve_tpu_embedding_adadelta_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingAdadeltaParameters: Any

def retrieve_tpu_embedding_adadelta_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingAdagradMomentumParametersOutput(NamedTuple):
    parameters: Any
    accumulators: Any
    momenta: Any

def retrieve_tpu_embedding_adagrad_momentum_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingAdagradMomentumParameters: Any

def retrieve_tpu_embedding_adagrad_momentum_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingAdagradParametersOutput(NamedTuple):
    parameters: Any
    accumulators: Any

def retrieve_tpu_embedding_adagrad_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingAdagradParameters: Any

def retrieve_tpu_embedding_adagrad_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingCenteredRMSPropParametersOutput(NamedTuple):
    parameters: Any
    ms: Any
    mom: Any
    mg: Any

def retrieve_tpu_embedding_centered_rms_prop_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingCenteredRMSPropParameters: Any

def retrieve_tpu_embedding_centered_rms_prop_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingFTRLParametersOutput(NamedTuple):
    parameters: Any
    accumulators: Any
    linears: Any

def retrieve_tpu_embedding_ftrl_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingFTRLParameters: Any

def retrieve_tpu_embedding_ftrl_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingFrequencyEstimatorParametersOutput(NamedTuple):
    parameters: Any
    last_hit_step: Any

def retrieve_tpu_embedding_frequency_estimator_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingFrequencyEstimatorParameters: Any

def retrieve_tpu_embedding_frequency_estimator_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingMDLAdagradLightParametersOutput(NamedTuple):
    parameters: Any
    accumulators: Any
    weights: Any
    benefits: Any

def retrieve_tpu_embedding_mdl_adagrad_light_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingMDLAdagradLightParameters: Any

def retrieve_tpu_embedding_mdl_adagrad_light_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingMomentumParametersOutput(NamedTuple):
    parameters: Any
    momenta: Any

def retrieve_tpu_embedding_momentum_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingMomentumParameters: Any

def retrieve_tpu_embedding_momentum_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingProximalAdagradParametersOutput(NamedTuple):
    parameters: Any
    accumulators: Any

def retrieve_tpu_embedding_proximal_adagrad_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingProximalAdagradParameters: Any

def retrieve_tpu_embedding_proximal_adagrad_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingProximalYogiParametersOutput(NamedTuple):
    parameters: Any
    v: Any
    m: Any

def retrieve_tpu_embedding_proximal_yogi_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingProximalYogiParameters: Any

def retrieve_tpu_embedding_proximal_yogi_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...

class _RetrieveTPUEmbeddingRMSPropParametersOutput(NamedTuple):
    parameters: Any
    ms: Any
    mom: Any

def retrieve_tpu_embedding_rms_prop_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingRMSPropParameters: Any

def retrieve_tpu_embedding_rms_prop_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def retrieve_tpu_embedding_stochastic_gradient_descent_parameters(num_shards, shard_id, table_id: int = ..., table_name: str = ..., config: str = ..., name: Any | None = ...): ...

RetrieveTPUEmbeddingStochasticGradientDescentParameters: Any

def retrieve_tpu_embedding_stochastic_gradient_descent_parameters_eager_fallback(num_shards, shard_id, table_id, table_name, config, name, ctx): ...
def send_tpu_embedding_gradients(inputs, learning_rates, config, name: Any | None = ...): ...

SendTPUEmbeddingGradients: Any

def send_tpu_embedding_gradients_eager_fallback(inputs, learning_rates, config, name, ctx): ...
def shutdown_distributed_tpu(name: Any | None = ...): ...

ShutdownDistributedTPU: Any

def shutdown_distributed_tpu_eager_fallback(name, ctx): ...
def tpu_compilation_result(name: Any | None = ...): ...

TPUCompilationResult: Any

def tpu_compilation_result_eager_fallback(name, ctx): ...
def tpu_embedding_activations(embedding_variable, sliced_activations, table_id, lookup_id, name: Any | None = ...): ...

TPUEmbeddingActivations: Any

def tpu_embedding_activations_eager_fallback(embedding_variable, sliced_activations, table_id, lookup_id, name, ctx): ...
def tpu_ordinal_selector(name: Any | None = ...): ...

TPUOrdinalSelector: Any

def tpu_ordinal_selector_eager_fallback(name, ctx): ...
def tpu_partitioned_call(args, device_ordinal, Tout, f, autotuner_thresh: int = ..., name: Any | None = ...): ...

TPUPartitionedCall: Any

def tpu_partitioned_call_eager_fallback(args, device_ordinal, Tout, f, autotuner_thresh, name, ctx): ...
def tpu_replicate_metadata(num_replicas, num_cores_per_replica: int = ..., topology: str = ..., use_tpu: bool = ..., device_assignment=..., computation_shape=..., host_compute_core=..., padding_map=..., step_marker_location: str = ..., allow_soft_placement: bool = ..., use_spmd_for_xla_partitioning: bool = ..., name: Any | None = ...): ...

TPUReplicateMetadata: Any

def tpu_replicate_metadata_eager_fallback(num_replicas, num_cores_per_replica, topology, use_tpu, device_assignment, computation_shape, host_compute_core, padding_map, step_marker_location, allow_soft_placement, use_spmd_for_xla_partitioning, name, ctx): ...
def tpu_replicated_input(inputs, is_mirrored_variable: bool = ..., index: int = ..., is_packed: bool = ..., name: Any | None = ...): ...

TPUReplicatedInput: Any

def tpu_replicated_input_eager_fallback(inputs, is_mirrored_variable, index, is_packed, name, ctx): ...
def tpu_replicated_output(input, num_replicas, name: Any | None = ...): ...

TPUReplicatedOutput: Any

def tpu_replicated_output_eager_fallback(input, num_replicas, name, ctx): ...
def worker_heartbeat(request, name: Any | None = ...): ...

WorkerHeartbeat: Any

def worker_heartbeat_eager_fallback(request, name, ctx): ...
def xla_concat_nd(inputs, num_concats, paddings=..., name: Any | None = ...): ...

XlaConcatND: Any

def xla_concat_nd_eager_fallback(inputs, num_concats, paddings, name, ctx): ...
def xla_split_nd(input, N, num_splits, paddings=..., name: Any | None = ...): ...

XlaSplitND: Any

def xla_split_nd_eager_fallback(input, N, num_splits, paddings, name, ctx): ...
