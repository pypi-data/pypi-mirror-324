from tensorflow.python import pywrap_tfe as pywrap_tfe
from tensorflow.python.eager import backprop_util as backprop_util, context as context, execute as execute, imperative_grad as imperative_grad, tape as tape
from tensorflow.python.framework import constant_op as constant_op, dtypes as dtypes, ops as ops, tensor_shape as tensor_shape, tensor_util as tensor_util
from tensorflow.python.ops import array_ops as array_ops, check_ops as check_ops, control_flow_util as control_flow_util, default_gradient as default_gradient, gen_array_ops as gen_array_ops, gen_math_ops as gen_math_ops, math_ops as math_ops, resource_variable_ops as resource_variable_ops
from tensorflow.python.ops.unconnected_gradients import UnconnectedGradients as UnconnectedGradients
from tensorflow.python.util import nest as nest, tf_contextlib as tf_contextlib, tf_inspect as tf_inspect
from tensorflow.python.util.lazy_loader import LazyLoader as LazyLoader
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any

pfor_ops: Any
function: Any

def op_attr_type(op_type, attr_name): ...
def make_attr(attr_type, value): ...

class _MockOp:
    attrs: Any
    inputs: Any
    outputs: Any
    type: Any
    skip_input_indices: Any
    def __init__(self, attrs, inputs, outputs, typ, skip_input_indices) -> None: ...
    def get_attr(self, attr): ...

def record_gradient(op_name, inputs, attrs, outputs) -> None: ...
def implicit_val_and_grad(f): ...
def implicit_grad(f): ...
def gradients_function(f, params: Any | None = ...): ...
def val_and_grad_function(f, params: Any | None = ...): ...
def make_vjp(f, params: Any | None = ..., persistent: bool = ...): ...
def flatten_nested_indexed_slices(grad): ...
def aggregate_indexed_slices_gradients(grads): ...

class GradientTape:
    def __init__(self, persistent: bool = ..., watch_accessed_variables: bool = ...) -> None: ...
    def __enter__(self): ...
    def __exit__(self, typ, value, traceback) -> None: ...
    def watch(self, tensor) -> None: ...
    def stop_recording(self) -> None: ...
    def reset(self) -> None: ...
    def watched_variables(self): ...
    def gradient(self, target, sources, output_gradients: Any | None = ..., unconnected_gradients=...): ...
    def jacobian(self, target, sources, unconnected_gradients=..., parallel_iterations: Any | None = ..., experimental_use_pfor: bool = ...): ...
    def batch_jacobian(self, target, source, unconnected_gradients=..., parallel_iterations: Any | None = ..., experimental_use_pfor: bool = ...): ...
