from tensorflow.compiler.xla.experimental.xla_sharding import xla_sharding as xla_sharding
from tensorflow.python.framework import dtypes as dtypes, ops as ops, tensor_shape as tensor_shape
from tensorflow.python.ops import array_ops as array_ops
from tensorflow.python.tpu import tpu_name_util as tpu_name_util, tpu_sharding as tpu_sharding
from tensorflow.python.tpu.ops import tpu_ops as tpu_ops
from tensorflow.python.util import nest as nest
from typing import Any

def partition_or_replicate_on_host(tensor, dims): ...
def tag_sharding_attribute_for_dequeued_tensors(dequeues, dims): ...

class InfeedQueue:
    def __init__(self, number_of_tuple_elements: Any | None = ..., tuple_types: Any | None = ..., tuple_shapes: Any | None = ..., shard_dimensions: Any | None = ..., number_of_partitions: Any | None = ..., name: Any | None = ...) -> None: ...
    @property
    def number_of_tuple_elements(self): ...
    @property
    def tuple_types(self): ...
    def set_tuple_types(self, tuple_types) -> None: ...
    @property
    def tuple_shapes(self): ...
    def set_tuple_shapes(self, tuple_shapes) -> None: ...
    @property
    def sharding_policies(self): ...
    @property
    def shard_dimensions(self): ...
    def set_shard_dimensions(self, shard_dimensions) -> None: ...
    @property
    def number_of_shards(self): ...
    def set_number_of_shards(self, number_of_shards) -> None: ...
    def set_configuration_from_input_tensors(self, input_tensors) -> None: ...
    def set_configuration_from_sharded_input_tensors(self, input_tensors) -> None: ...
    def freeze(self) -> None: ...
    def generate_dequeue_op(self, tpu_device: int = ...): ...
    def generate_enqueue_ops(self, sharded_inputs, tpu_ordinal_function: Any | None = ..., placement_function: Any | None = ...): ...
    def split_inputs_and_generate_enqueue_ops(self, inputs, device_assignment: Any | None = ..., placement_function: Any | None = ..., tpu_ordinal_function: Any | None = ...): ...

class _PartitionedInfeedQueue(InfeedQueue):
    def __init__(self, number_of_tuple_elements, device_assignment, host_id, input_partition_dims: Any | None = ..., tuple_types: Any | None = ..., tuple_shapes: Any | None = ..., name: Any | None = ...) -> None: ...
    def generate_dequeue_op(self, tpu_device: int = ...): ...
    def generate_enqueue_ops(self, sharded_inputs): ...
