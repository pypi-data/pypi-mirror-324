from tensorflow.python.distribute import collective_all_reduce_strategy as collective_all_reduce_strategy, distribution_strategy_context as distribution_strategy_context, mirrored_strategy as mirrored_strategy, one_device_strategy as one_device_strategy, tpu_strategy as tpu_strategy
from tensorflow.python.eager import backprop as backprop, context as context
from tensorflow.python.framework import dtypes as dtypes, ops as ops, smart_cond as smart_cond
from tensorflow.python.keras import backend as backend, optimizers as optimizers
from tensorflow.python.keras.optimizer_v2 import optimizer_v2 as optimizer_v2
from tensorflow.python.ops import control_flow_ops as control_flow_ops, math_ops as math_ops, variable_scope as variable_scope, variables as variables
from tensorflow.python.platform import tf_logging as tf_logging
from tensorflow.python.training.experimental import mixed_precision as mixed_precision
from tensorflow.python.training.tracking import base as trackable, base_delegate as base_delegate
from tensorflow.python.util import nest as nest
from tensorflow.python.util.tf_export import keras_export as keras_export
from typing import Any

class _UnwrapPreventer:
    value: Any
    def __init__(self, value) -> None: ...

class _DynamicLossScaleState(trackable.Trackable):
    def __init__(self, initial_loss_scale, growth_steps, multiplier) -> None: ...
    @property
    def initial_loss_scale(self): ...
    @property
    def growth_steps(self): ...
    @property
    def multiplier(self): ...
    @property
    def current_loss_scale(self): ...
    @property
    def counter(self): ...
    def __call__(self): ...
    def update(self, grads): ...

class LossScaleOptimizer(base_delegate.DelegatingTrackableMixin, optimizer_v2.OptimizerV2):
    def __init__(self, inner_optimizer, dynamic: bool = ..., initial_scale: Any | None = ..., dynamic_growth_steps: Any | None = ...) -> None: ...
    @property
    def dynamic(self): ...
    @property
    def loss_scale(self): ...
    @property
    def dynamic_counter(self): ...
    @property
    def initial_scale(self): ...
    @property
    def dynamic_growth_steps(self): ...
    @property
    def inner_optimizer(self): ...
    def get_scaled_loss(self, loss): ...
    def get_unscaled_gradients(self, grads): ...
    def get_gradients(self, loss, params): ...
    def apply_gradients(self, grads_and_vars, name: Any | None = ..., experimental_aggregate_gradients: bool = ...): ...
    def get_config(self): ...
    @classmethod
    def from_config(cls, config, custom_objects: Any | None = ...): ...
    @property
    def iterations(self): ...
    @iterations.setter
    def iterations(self, variable) -> None: ...
    def get_slot_names(self): ...
    def variables(self): ...
    @property
    def weights(self): ...
    def get_weights(self): ...
    def set_weights(self, weights): ...
    @property
    def clipnorm(self): ...
    @clipnorm.setter
    def clipnorm(self, val) -> None: ...
    @property
    def global_clipnorm(self): ...
    @global_clipnorm.setter
    def global_clipnorm(self, val) -> None: ...
    @property
    def clipvalue(self): ...
    @clipvalue.setter
    def clipvalue(self, val) -> None: ...
    def get_slot(self, var, slot_name): ...
    def add_slot(self, var, slot_name, initializer: str = ...): ...
    def __getattribute__(self, name): ...
    def __dir__(self): ...
    def __setattr__(self, name, value) -> None: ...
    @property
    def learning_rate(self): ...
    @learning_rate.setter
    def learning_rate(self, value) -> None: ...
    @property
    def lr(self): ...
    @lr.setter
    def lr(self, value) -> None: ...

class LossScaleOptimizerV1(LossScaleOptimizer):
    def __init__(self, optimizer, loss_scale) -> None: ...
    @classmethod
    def from_config(cls, config, custom_objects: Any | None = ...): ...

class FakeOptimizerForRestoration(trackable.Trackable):
    def __init__(self, optimizer) -> None: ...
    def get_slot_names(self): ...

def strategy_supports_loss_scaling(): ...
