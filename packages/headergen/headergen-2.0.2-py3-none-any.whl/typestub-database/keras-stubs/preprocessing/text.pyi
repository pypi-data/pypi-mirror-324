from keras.preprocessing.text_dataset import text_dataset_from_directory as text_dataset_from_directory
from typing import Any

hashing_trick: Any
Tokenizer: Any

def text_to_word_sequence(input_text, filters: str = ..., lower: bool = ..., split: str = ...): ...
def one_hot(input_text, n, filters: str = ..., lower: bool = ..., split: str = ...): ...

tokenizer_from_json: Any
