.TH "ramalama 1" 
.nh
.ad l

.SH NAME
.PP
ramalama \- Simple management tool for working with AI Models

.SH SYNOPSIS
.PP
\fBramalama\fP [\fIoptions\fP] \fIcommand\fP

.SH DESCRIPTION
.PP
RamaLama : The goal of RamaLama is to make AI boring.

.PP
RamaLama tool facilitates local management and serving of AI Models.

.PP
On first run RamaLama inspects your system for GPU support, falling back to CPU support if no GPUs are present.

.PP
RamaLama uses container engines like Podman or Docker to pull the appropriate OCI image with all of the software necessary to run an AI Model for your systems setup.

.PP
Running in containers eliminates the need for users to configure the host system for AI. After the initialization, RamaLama runs the AI Models within a container based on the OCI image.

.PP
RamaLama pulls AI Models from model registries. Starting a chatbot or a rest API service from a simple single command. Models are treated similarly to how Podman and Docker treat container images.

.PP
When both Podman and Docker are installed, RamaLama defaults to Podman, The \fB\fCRAMALAMA\_CONTAINER\_ENGINE=docker\fR environment variable can override this behaviour. When neither are installed RamaLama attempts to run the model with software on the local system.

.PP
Note:

.PP
On Macs with Arm support and Podman, the Podman machine must be
configured to use the krunkit VM Type. This allows the Mac's GPU to be
used within the VM.

.PP
Default settings for flags are defined in \fB\fCramalama.conf(5)\fR\&.

.PP
RamaLama supports multiple AI model registries types called transports. Supported transports:

.SH MODEL TRANSPORTS
.TS
allbox;
l l l 
l l l .
\fB\fCTransports\fR	\fB\fCPrefix\fR	\fB\fCWeb Site\fR
URL based	https://, http://, file://	T{
\fB\fChttps://web.site/ai.model\fR, \fB\fCfile://tmp/ai.model\fR
T}
HuggingFace	huggingface://, hf://, hf.co/	\fB\fChuggingface.co\fR
Ollama	ollama://	\fB\fCollama.com\fR
OCI Container Registries	oci://	\fB\fCopencontainers.org\fR
 	 	T{
Examples: \fB\fCquay.io\fR,  \fB\fCDocker Hub\fR,\fB\fCArtifactory\fR
T}
.TE

.PP
RamaLama uses to the Ollama registry transport. This default can be overridden in the \fB\fCramalama.conf\fR file or via the RAMALAMA\_TRANSPORTS
environment. \fB\fCexport RAMALAMA\_TRANSPORT=huggingface\fR Changes RamaLama to use huggingface transport.

.PP
Modify individual model transports by specifying the \fB\fChuggingface://\fR, \fB\fCoci://\fR, \fB\fCollama://\fR, \fB\fChttps://\fR, \fB\fChttp://\fR, \fB\fCfile://\fR prefix to the model.

.PP
URL support means if a model is on a web site or even on your local system, you can run it directly.

.PP
ramalama pull \fB\fChuggingface://\fRafrideva/Tiny\-Vicuna\-1B\-GGUF/tiny\-vicuna\-1b.q2\_k.gguf

.PP
ramalama run \fB\fCfile://\fR$HOME/granite\-7b\-lab\-Q4\_K\_M.gguf

.PP
To make it easier for users, RamaLama uses shortname files, which container
alias names for fully specified AI Models allowing users to specify the shorter
names when referring to models. RamaLama reads shortnames.conf files if they
exist . These files contain a list of name value pairs for specification of
the model. The following table specifies the order which RamaLama reads the files
. Any duplicate names that exist override previously defined shortnames.

.TS
allbox;
l l 
l l .
\fB\fCShortnames type\fR	\fB\fCPath\fR
Distribution	T{
/usr/share/ramalama/shortnames.conf
T}
Local install	T{
/usr/local/share/ramalama/shortnames.conf
T}
Administrators	/etc/ramamala/shortnames.conf
Users	T{
$HOME/.config/ramalama/shortnames.conf
T}
.TE

.PP
.RS

.nf
$ cat /usr/share/ramalama/shortnames.conf
[shortnames]
  "tiny" = "ollama://tinyllama"
  "granite" = "huggingface://instructlab/granite\-7b\-lab\-GGUF/granite\-7b\-lab\-Q4\_K\_M.gguf"
  "granite:7b" = "huggingface://instructlab/granite\-7b\-lab\-GGUF/granite\-7b\-lab\-Q4\_K\_M.gguf"
  "ibm/granite" = "huggingface://instructlab/granite\-7b\-lab\-GGUF/granite\-7b\-lab\-Q4\_K\_M.gguf"
  "merlinite" = "huggingface://instructlab/merlinite\-7b\-lab\-GGUF/merlinite\-7b\-lab\-Q4\_K\_M.gguf"
  "merlinite:7b" = "huggingface://instructlab/merlinite\-7b\-lab\-GGUF/merlinite\-7b\-lab\-Q4\_K\_M.gguf"
...

.fi
.RE

.PP
\fBramalama [GLOBAL OPTIONS]\fP

.SH GLOBAL OPTIONS
.SS \fB\-\-container\fP
.PP
run RamaLama in the default container. Default is \fB\fCtrue\fR unless overridden in the ramalama.conf file.
The environment variable "RAMALAMA\_IN\_CONTAINER=false" can also change the default.

.SS \fB\-\-debug\fP
.PP
print debug messages

.SS \fB\-\-dryrun\fP
.PP
show container runtime command without executing it (default: False)

.SS \fB\-\-engine\fP
.PP
run RamaLama using the specified container engine. Default is \fB\fCpodman\fR if installed otherwise docker.
The default can be overridden in the ramalama.conf file or via the RAMALAMA\_CONTAINER\_ENGINE environment variable.

.SS \fB\-\-gpu\fP
.PP
offload the workload to the GPU (default: False)

.SS \fB\-\-help\fP, \fB\-h\fP
.PP
show this help message and exit

.SS \fB\-\-image\fP=IMAGE
.PP
OCI container image to run with specified AI model. By default RamaLama uses
\fB\fCquay.io/ramalama/ramalama:latest\fR\&. The \-\&\-\&image option allows users to override
the default.

.PP
The default can be overridden in the ramalama.conf file or via the the
RAMALAMA\_IMAGE environment variable. \fB\fCexport RAMALAMA\_TRANSPORT=quay.io/ramalama/aiimage:latest\fR tells
RamaLama to use the \fB\fCquay.io/ramalama/aiimage:latest\fR image.

.SS \fB\-\-nocontainer\fP
.PP
do not run RamaLama in the default container (default: False)
The default can be overridden in the ramalama.conf file.

.SS \fB\-\-runtime\fP=\fIllama.cpp\fP | \fIvllm\fP
.PP
specify the runtime to use, valid options are 'llama.cpp' and 'vllm' (default: llama.cpp)
The default can be overridden in the ramalama.conf file.

.SS \fB\-\-store\fP=STORE
.PP
store AI Models in the specified directory (default rootless: \fB\fC$HOME/.local/share/ramalama\fR, default rootful: \fB\fC/var/lib/ramalama\fR)
The default can be overridden in the ramalama.conf file.

.SS \fB\-\-version\fP, \fB\-v\fP
.PP
show RamaLama version

.SH COMMANDS
.TS
allbox;
l l 
l l .
\fB\fCCommand\fR	\fB\fCDescription\fR
ramalama\-containers(1)	list all RamaLama containers
ramalama\-bench(1)	benchmark specified AI Model
ramalama\-convert(1)	T{
convert AI Models from local storage to OCI Image
T}
ramalama\-info(1)	T{
Display RamaLama configuration information
T}
ramalama\-list(1)	list all downloaded AI Models
ramalama\-login(1)	login to remote registry
ramalama\-logout(1)	logout from remote registry
ramalama\-pull(1)	T{
pull AI Models from Model registries to local storage
T}
ramalama\-push(1)	T{
push AI Models from local storage to remote registries
T}
ramalama\-rm(1)	T{
remove AI Models from local storage
T}
ramalama\-run(1)	T{
run specified AI Model as a chatbot
T}
ramalama\-perplexity(1)	T{
calculate the perplexity value of an AI Model
T}
ramalama\-serve(1)	T{
serve REST API on specified AI Model
T}
ramalama\-stop(1)	T{
stop named container that is running AI Model
T}
ramalama\-version(1)	display version of RamaLama
.TE

.SH CONFIGURATION FILES
.SH SEE ALSO
.PP
\fBpodman(1)\fP, \fBdocker(1)\fP, \fBramalama.conf(5)\fP

.SH HISTORY
.PP
Aug 2024, Originally compiled by Dan Walsh 
\[la]dwalsh@redhat.com\[ra]
