# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['tinysmith',
 'tinysmith.agent',
 'tinysmith.agent.strategies.planner_reviewer_executor',
 'tinysmith.agent.strategies.reflexion',
 'tinysmith.agent.strategies.single_agent',
 'tinysmith.llm',
 'tinysmith.modules',
 'tinysmith.modules.memory',
 'tinysmith.modules.rag',
 'tinysmith.orchestrator']

package_data = \
{'': ['*'],
 'tinysmith.agent.strategies.planner_reviewer_executor': ['templates/*'],
 'tinysmith.agent.strategies.reflexion': ['templates/*'],
 'tinysmith.agent.strategies.single_agent': ['templates/*']}

install_requires = \
['distance>=0.1.3,<0.2.0',
 'groq>=0.16.0,<0.17.0',
 'mistralai>=1.5.0,<2.0.0',
 'openai>=1.60.2,<2.0.0',
 'rich>=13.9.4,<14.0.0']

setup_kwargs = {
    'name': 'tinysmith',
    'version': '0.9.1',
    'description': 'TinyAgentSmith (tinysmith) is a tiny, modular multi-agent LLM framework. It is designed to be lightweight and simple to use, while providing a flexible and extensible platform for quickly iterating on multi-agent algorithms.',
    'long_description': '<p align="center">\n  <img src="https://i.imgur.com/vWDUm4E.png" width=256 height=256 />\n</p>\n<h1 align="center">TinyAgentSmith</h1>\n\nTinyAgentSmith (`tinysmith`) is a tiny, modular multi-agent LLM framework. It is designed to be lightweight and simple to use, while providing a flexible and extensible platform for quickly iterating on multi-agent algorithms.\n\n> Tiny: small, lightweight, and simple.  \n> Agent Smith: a smith that forges agents, also a reference to the movie "The Matrix".\n\n\n## Usage Examples\n### New Custom Strategy\nThe most useful feature of `tinysmith` is the ability to define arbitrary new strategies. The following gives an example of how to define a new toy strategy.\n\n#### 0. Add Logging\nIn order to see what\'s going on, you can set up logging. The following code snippet shows how to set up the logger for the `tinysmith` framework together with the adapted Intercode library in this repository, to show the agent\'s interactions. Use `logger.DEBUG` for a detailed output of what each agent\'s prompts and responses are.\n\n```python\nlogger.setLevel(logging.INFO)\nintercode_logger = logging.getLogger("intercode")\nintercode_logger.setLevel(logging.INFO)\ntinysmith_logger = logging.getLogger("tinysmith")\ntinysmith_logger.setLevel(logging.INFO)\n```\n\n#### 1. Define Agents and Collaboration\nWe want to define an example toy strategy where two agents communicate: An agent that throws a coin, and a poet agent that writes a poem based on the coin\'s outcome.\n\nThe transition function defines which agent is called under what circumstances. The following transition function give a simple example of how to switch between the agents. The transition function can access the full state object and can be made arbitrarily complex.\n\n```python\ndef transition(obs, state):\n    if state.get_agent_name() == \'init\':\n        state.set_agent_name(\'coinflip_agent\')\n    elif state.get_agent_name() == \'coinflip_agent\':\n        state.set_agent_name(\'poet_agent\')\n        state.set_is_envstep(True)  # The poet\'s output is always passed to the environment\n    elif state.get_agent_name() == \'poet_agent\':\n        state.set_agent_name(\'coinflip_agent\')\n```\n\nThe PromptEngineer implementations determine the agent\'s behavior. The following code snippet shows the implementation of the PromptEngineer for the coinflip and poet agents. \n```python\nfrom tinysmith.orchestrator.prompting import PromptEngineer\nfrom tinysmith.orchestrator.state import State\nfrom tinysmith.orchestrator.utils import signal_error\n\nclass CoinflipPE(PromptEngineer):\n    def render_prompt(self, obs, hist, state):\n        # In order to shape the agent\'s behavior, create a prompt as a string and append it to the history.\n        prompt = \'Flip a coin. Answer only with "heads" or "tails".\'\n        hist.append(UserMessage(prompt))\n\n    def process_response(self, obs, res, hist, state):\n        # The LLMs response can be arbitrarily parsed and processed here. It usually makes sense to store some information in the state object to communicate it to other agents.\n        if ((\'heads\' in res.lower() and \'tails\' in res.lower()) \n            or (\'heads\' not in res.lower() and \'tails\' not in res.lower())):\n            # If the response is not valid, the signale_error helper function can be used to reprompt the agent.\n            signal_error(state, \n                         \'Invalid flip.\', \n                         \'Invalid response. Please answer either heads or tails.\')\n            return \'\'\n\n        # Store the coinflip result in the agent\'s namespace in the state object\n        coinflip_agent_state = state.get(\'coinflip\') if state.get(\'coinflip\') else {}\n        if \'heads\' in res.lower(): \n            coinflip_agent_state[\'flip\'] = \'heads\'\n        else:\n            coinflip_agent_state[\'flip\'] = \'tails\'\n        state.set(\'coinflip\', coinflip_agent_state)\n\n    def reset(self):\n        pass\n\nclass PoetPE(PromptEngineer):\n    def render_prompt(self, obs, hist, state):\n        # Other agents\' state can be accessed to use information gathered by other agents previously.\n        coinflip_agent_state = state.get(\'coinflip\')\n\n        prompt = "Your output will be interpretet by a bash shell. " \\\n                + "Use the echo command to print your output. " \\\n                + "Print a happy poem about randomness." \\\n                + f"Include the coinflip result: {coinflip_agent_state[\'flip\']}."\n        hist.append(UserMessage(prompt))\n\n    def process_response(self, obs, res, hist, state):\n        return res\n\n    def reset(self):\n        pass\n```\n\n#### 2. Define the Agents and Orchestrator\nOnce the strategy is defined, the components are just plugged into the framework objects and can be run.\n`tinysmith` provides a selection of LLM adapters (i.e., for https://mistral.ai/, https://openai.com/, and https://groq.com/) in `/llm/adapters.py`. \nUse groq for a free option. Simply replace the `api_key` with the one provided by the respective service.\n\n```python\nfrom tinysmith.agent.agent import Agent\nfrom tinysmith.llm.adapters import UserMessage, GroqAPIAdapter\nfrom tinysmith.orchestrator.orchestrator import Orchestrator\n\ntoken_usage = {} # This can be used to log the token usage by the LLM.\n# Create LLM adapter for the agents\nllm = GroqAPIAdapter(\n        model=\'llama3-8b-8192\', \n        temperature=1,\n        top_p=1,\n        token_usage=token_usage,\n        api_key=\'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\')\n\n# Create the agents with their PromptEngineer and LLM adapter implementations. Their name must match the transition function.\ncoinflip_agent = Agent(\'coinflip_agent\', CoinflipPE(), llm)\npoet_agent = Agent(\'poet_agent\', PoetPE(), llm)\n\n# The state object should be prefilled with all necessary information for the strategy and with meta information.\nstate = State()\nstate.set_max_errors(3) # The orchestrator stops if it encounters more than 3 errors in agent executions.\nstate.set_max_steps(10) # The orchestrator stops after 10 environment steps.\n\n# Assemble all components into the orchestrator\norchestrator = Orchestrator([coinflip_agent, poet_agent], transition, state)\n```\n\n#### 3. Run the Orchestrator\nCreate an environment (e.g. Intercode) and run the orchestrator and the environment in a loop. \n\nTo emulate an environement you could use something like this:\n```python\nwhile True:\n    print(orchestrator.forward(input("> "), 0))\n```\n\n#### Full Example\n```python\nimport logging\n\nfrom tinysmith.agent.agent import Agent\nfrom tinysmith.llm.adapters import UserMessage, GroqAPIAdapter\nfrom tinysmith.orchestrator.orchestrator import Orchestrator\nfrom tinysmith.orchestrator.prompting import PromptEngineer\nfrom tinysmith.orchestrator.state import State\nfrom tinysmith.orchestrator.utils import signal_error\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\ntinysmith_logger = logging.getLogger("tinysmith")\ntinysmith_logger.setLevel(logging.DEBUG)\n\nclass CoinflipPE(PromptEngineer):\n    def render_prompt(self, obs, hist, state):\n        prompt = \'Flip a coin. Answer only with "heads" or "tails".\'\n        hist.append(UserMessage(prompt))\n\n    def process_response(self, obs, res, hist, state):\n        if ((\'heads\' in res.lower() and \'tails\' in res.lower()) \n            or (\'heads\' not in res.lower() and \'tails\' not in res.lower())):\n            signal_error(state, \n                         \'Invalid flip.\', \n                         \'Invalid response. Please answer either heads or tails.\')\n            return \'\'\n\n        coinflip_agent_state = state.get(\'coinflip\') if state.get(\'coinflip\') else {}\n        if \'heads\' in res.lower(): \n            coinflip_agent_state[\'flip\'] = \'heads\'\n        else:\n            coinflip_agent_state[\'flip\'] = \'tails\'\n        state.set(\'coinflip\', coinflip_agent_state)\n\n    def reset(self):\n        pass\n\nclass PoetPE(PromptEngineer):\n    def render_prompt(self, obs, hist, state):\n        coinflip_agent_state = state.get(\'coinflip\')\n\n        prompt = "Your output will be interpretet by a bash shell. " \\\n                + "Use the echo command to print your output. " \\\n                + "Print a happy poem about randomness." \\\n                + f"Include the coinflip result: {coinflip_agent_state[\'flip\']}."\n        hist.append(UserMessage(prompt))\n\n    def process_response(self, obs, res, hist, state):\n        return res\n\n    def reset(self):\n        pass\n\ntoken_usage = {}\nllm = GroqAPIAdapter(\n        model=\'llama3-8b-8192\', \n        temperature=1,\n        top_p=1,\n        token_usage=token_usage,\n        api_key=\'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\')\n\ncoinflip_agent = Agent(\'coinflip_agent\', CoinflipPE(), llm)\npoet_agent = Agent(\'poet_agent\', PoetPE(), llm)\n\ndef transition(obs, state):\n    if state.get_agent_name() == \'init\':\n        state.set_agent_name(\'coinflip_agent\')\n    elif state.get_agent_name() == \'coinflip_agent\':\n        state.set_agent_name(\'poet_agent\')\n        state.set_is_envstep(True)\n    elif state.get_agent_name() == \'poet_agent\':\n        state.set_agent_name(\'coinflip_agent\')\n\nstate = State()\nstate.set_max_errors(3)\nstate.set_max_steps(10)\norchestrator = Orchestrator([coinflip_agent, poet_agent], transition, state)\n\n\nwhile True:\n    print(orchestrator.forward(input("> "), 0))\n\n```\n\n### Built-In: Planner/Reviewer/Executor Strategy\nThis is an example of the built-in planner/reviewer/executor strategy. All built-in strategies can be found in `/agent/strategies/`. These strategies are used mainly for experimentation and as a reference for creating custom strategies.\n\n```python\nimport logging\n\nfrom tinysmith.agent.agent import Agent\nfrom tinysmith.agent.strategies.planner_reviewer_executor.prompting import (\n    ExecutorPromptEngineer,\n    PlannerPromptEngineer,\n    ReviewerPromptEngineer,\n)\nfrom tinysmith.agent.strategies.planner_reviewer_executor.transition import (\n    planner_reviewer_executor,\n)\nfrom tinysmith.llm.adapters import MistralAPIAdapter, OpenAiAPIAdapter\nfrom tinysmith.orchestrator.orchestrator import Orchestrator\nfrom tinysmith.orchestrator.state import State\n\n# Set up logging to see what\'s going on\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\ntinysmith_logger = logging.getLogger("tinysmith")\ntinysmith_logger.setLevel(logging.INFO)\n\n# \'Large LLM\' Adapter\nopenai_llm_adapter = OpenAiAPIAdapter(\n        model = \'gpt-4\',\n        temperature = 0,\n        top_p = 1,\n        token_usage = {},\n        api_key = "<API KEY>")\n\n# \'Small LLM\' Adapter\nmistral_llm_adapter = MistralAPIAdapter(\n        model = \'open-mixtral-8x7b\',\n        temperature = 0,\n        top_p = 1,\n        token_usage = {},\n        api_key = "<API KEY>")\n\n\n# Agents\nplanner = Agent(\n        name = \'planner\',\n        prompt_engineer = PlannerPromptEngineer(),\n        llm_adapter = mistral_llm_adapter\n        )\nreviewer = Agent(\n        name = \'reviewer\',\n        prompt_engineer = ReviewerPromptEngineer(),\n        llm_adapter = mistral_llm_adapter\n        )\nexecutor = Agent(\n        name = \'executor\',\n        prompt_engineer = ExecutorPromptEngineer(),\n        llm_adapter = mistral_llm_adapter\n        )\n\n# Define the orchestrator state and set all necessary information for your strategy\n# For example, the challenge description, and the reviewer\'s maximum number of rejections\nstate = State()\nstate.set_max_errors(3)\nstate.set(\'challenge_description\', \'The flag is in a hidden file. Have fun!\')\nstate.set(\'executor\', {\'max_steps\': 10})\nstate.set(\'reviewer\', {\'max_rejects\': 1})\n\n# Define the orchestrator with the agents, transition function, and state\norchestrator = Orchestrator(\n        agents=[planner, reviewer, executor],\n        transition=planner_reviewer_executor,\n        state=state)\n\n# === Use the agent in some kind of environment loop. ===\n# - BYO environment, e.g InterCode. -\n\nobs = ""\nwhile True:\n    print(f"Agent chose the following action: {orchestrator.forward(obs, 0)}")\n    obs = input("Play environment. Enter observation: ")\n```\n\n### Selfhosted LLM\nThe easiest way to use a self-hosted LLM, is to run it in an inference engine with a OpenAI compatible API. The `CustomAPIAdapter` class can then be used to connect to the self-hosted LLM. \n\n1. Follow the steps to set up vLLM with an API server: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n    - The vLLM server can run on any local or remote machine \n    - The server must be accessible from the host running `tinysmith`.\n2. Connect the `CustomAPIAdapter` to the self-hosted LLM.\n\nThe following code snippet shows how to connect to a self-hosted LLM.\n\n```python\nllm = CustomAPIAdapter(\n        model=\'NousResearch/Meta-Llama-3-8B-Instruct\', \n        temperature=0.5,\n        top_p=1,\n        token_usage={},\n        api_key = "token-abc123",\n        base_url=\'http://localhost:8000/v1\')\n```\n',
    'author': 'J. Florian Kimmes',
    'author_email': 'tinysmith@jfkimmes.eu',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.11,<4.0',
}


setup(**setup_kwargs)
