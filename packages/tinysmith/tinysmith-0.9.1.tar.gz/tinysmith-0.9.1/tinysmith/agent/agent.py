import logging

from tinysmith.llm.adapters import LLMAdapter, MaxContextError, Message, UserMessage
from tinysmith.modules.modules import Module
from tinysmith.orchestrator.prompting import PromptEngineer
from tinysmith.orchestrator.state import State
from tinysmith.orchestrator.utils import signal_error

logger = logging.getLogger(__name__)

class Agent:
    """The agent class represents a single agent in the multi-agent system. After its creation it 
    is passed to the orchestrator and is interacted with only indirectly through the Orchestrator.
    It encapsulates a single agent's behaviour by combining a PromptEngineer, and, optionally,
    a list of Modules. 
    The PromptEngineer renders a prompt that defines the agent's behaviour, and the Modules
    preprocess and postprocess the agent's input and output. 

    Args:
        name (str): The name of the agent. The transition function uses this name to determine
                    which agent to call next.
        prompt_engineer (PromptEngineer): The PromptEngineer that renders the agent's prompt. This
                                          prompt defines the agent's behaviour.
        llm_adapter (LLMAdapter): The LLMAdapter that generates the agent's response. This adapter 
                                  abstracts communication to the underlying language model.
        modules (None|list[Module], optional): A list of Modules that preprocess and postprocess
                                               the agent's input and output. Defaults to None.
    """
    def __init__(self, 
                name: str,
                 prompt_engineer: PromptEngineer,
                 llm_adapter: LLMAdapter,
                 modules: None|list[Module] = None):
        self.name = name

        self.prompt_engineer = prompt_engineer
        self.llm_adapter = llm_adapter

        self.modules = modules if modules is not None else []

        self.history: list[Message] = []
        self.reset()


    def forward(self, obs: None|str, reward: int|None, orchestrator_state: State) -> str:
        """Forward the agent by processing the observation, generating a response, 
        and updating the state.

        This function progresses the policy of the agent. It lets all registered Modules
        preprocess the observation, before letting the agent's PromptEngineer render the prompt.
        The underlying LLMAdapter generates a response, and finally lets all registered Modules 
        postprocess this response.
    
        Args:
            obs (None|str): The last observation provided by the environment. This is None if 
                            another agent has already processed the observation.
            reward (int|None): The reward provided by the environment.
            orchestrator_state (State): The current state of the orchestrator. Is used by all
                                        components to communicate.
        Returns: The postprocessed response generated by the LLM, according to the prompt.
        """
        # This for loop and try block is used to handle context window errors
        # TODO: find an elegant way to handle these errors
        for _ in range(orchestrator_state.get_max_errors()):
            try:
                if orchestrator_state.get_error_status():
                    # If error status is detected, append the improve message to the previous history
                    # We don't run any modules and we don't render the standard prompt again
                    improve_message = UserMessage(orchestrator_state.get_error_improve_message())
                    self.history.append(improve_message)
                    logger.debug(f"Error in: {self.history}") 
                else:
                    # If no error status is detected, we run all modules and render the standard prompt
                    for module in self.modules:
                        module.preprocessing(obs, orchestrator_state)
                    self.prompt_engineer.render_prompt(obs, self.history, orchestrator_state)

                result = self.llm_adapter.generate(self.history)
                orchestrator_state.reset_error()

                if not orchestrator_state.get_error_status():
                    for module in self.modules:
                        module.postprocessing(result, orchestrator_state)

                response = self.prompt_engineer.process_response(obs, result, self.history, orchestrator_state)
                return response 
            except MaxContextError:
                # Setting obs to None should be okay since the prompt engineer 
                # must handle this case for the initial prompt as well
                obs = None
                self.prompt_engineer.render_prompt(obs, self.history, orchestrator_state)
                signal_error(orchestrator_state,
                             "Max context error. Truncating context.",
                             "The previous command generated too much output. " \
                                     + "Please choose a command with less output.")
        raise OverflowError("Too many errors encountered.")


    def reset(self):
        """Resets the agent and all its components.
        """
        for module in self.modules:
            module.reset()
        self.prompt_engineer.reset()
